# utils/add_new_data.py
import h5py
import numpy as np
import argparse
import shutil
import os
from tqdm import tqdm

def add_and_process_data(main_hdf5, new_hdf5, threshold=0.0616):
    """
    1. å°† new_hdf5 ä¸­çš„ demo è¿½åŠ åˆ° main_hdf5
    2. è‡ªåŠ¨é‡å‘½å demo_key é˜²æ­¢å†²çª (ä¾‹å¦‚ demo_0 -> demo_100)
    3. å¯¹æ–°åŠ å…¥çš„ demo è¿›è¡Œå¤¹çˆªäºŒå€¼åŒ–
    """
    print(f"ğŸ“‚ ä¸»æ–‡ä»¶: {main_hdf5}")
    print(f"ğŸ“‚ æ–°æ–‡ä»¶: {new_hdf5}")

    # ä»¥ r+ æ¨¡å¼æ‰“å¼€ä¸»æ–‡ä»¶
    with h5py.File(main_hdf5, 'r+') as f_main, h5py.File(new_hdf5, 'r') as f_new:
        # 1. ç¡®å®šèµ·å§‹ç´¢å¼•
        existing_keys = list(f_main['data'].keys())
        # è·å–ç°æœ‰æœ€å¤§çš„ IDï¼Œä¾‹å¦‚ demo_99 -> max_id = 99
        max_id = -1
        for k in existing_keys:
            try:
                curr_id = int(k.split('_')[1])
                if curr_id > max_id: max_id = curr_id
            except: pass
        
        start_id = max_id + 1
        print(f"ğŸ”¢ ç°æœ‰æœ€å¤§ ID: {max_id}, æ–°æ•°æ®å°†ä» demo_{start_id} å¼€å§‹è¿½åŠ ...")

        new_keys = list(f_new['data'].keys())
        print(f"ğŸ“¦ å‡†å¤‡åˆå¹¶ {len(new_keys)} æ¡æ–°æ•°æ®...")

        count_added = 0
        
        # 2. å¤åˆ¶æ•°æ®
        for i, old_key in enumerate(tqdm(new_keys, desc="Merging")):
            source_grp = f_new['data'][old_key]
            target_key = f"demo_{start_id + i}"
            
            # å¤åˆ¶ Group
            f_new.copy(source_grp, f_main['data'], name=target_key)
            
            # --- 3. ç«‹å³å¯¹æ–°æ•°æ®è¿›è¡Œå¤¹çˆªäºŒå€¼åŒ– ---
            # åªæœ‰æ–°å¤åˆ¶è¿›å»çš„è¿™ä¸ª group éœ€è¦å¤„ç†
            target_grp = f_main['data'][target_key]
            obs_grp = target_grp['obs']
            
            # æŸ¥æ‰¾å¤¹çˆªæ•°æ® Key
            target_dataset_name = None
            is_embedded = False
            
            if 'robot0_joint_pos' in obs_grp:
                joint_shape = obs_grp['robot0_joint_pos'].shape
                if len(joint_shape) == 2 and joint_shape[1] == 8:
                    target_dataset_name = 'robot0_joint_pos'
                    is_embedded = True
            
            if not is_embedded:
                for k in ['robot0_gripper_qpos', 'gripper_states', 'gripper_qpos']:
                    if k in obs_grp:
                        target_dataset_name = k
                        break
            
            # æ‰§è¡ŒäºŒå€¼åŒ–
            if target_dataset_name:
                data = obs_grp[target_dataset_name][:]
                
                if is_embedded:
                    gripper_data = data[:, 7]
                else:
                    gripper_data = data
                
                # äºŒå€¼åŒ–é€»è¾‘
                binary_vals = np.where(gripper_data > threshold, 1.0, -1.0).astype(np.float32)
                
                # å†™å›
                if is_embedded:
                    data[:, 7] = binary_vals
                    del obs_grp[target_dataset_name]
                    obs_grp.create_dataset(target_dataset_name, data=data)
                else:
                    del obs_grp[target_dataset_name]
                    if binary_vals.ndim == 1 and len(obs_grp[target_dataset_name].shape) == 2:
                         binary_vals = binary_vals[:, None]
                    obs_grp.create_dataset(target_dataset_name, data=binary_vals)
            
            count_added += 1

    print(f"\nğŸ‰ åˆå¹¶å®Œæˆï¼")
    print(f"âœ… å·²æ·»åŠ  {count_added} æ¡æ•°æ® (IDèŒƒå›´: {start_id} ~ {start_id + count_added - 1})")
    print(f"âœ… æ–°æ•°æ®å¤¹çˆªå·²äºŒå€¼åŒ– (Threshold={threshold})")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--main', type=str, required=True, help='ä¸» HDF5 æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--new', type=str, required=True, help='åŒ…å«æ–° 40 æ¡æ•°æ®çš„ HDF5 æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    
    add_and_process_data(args.main, args.new)