# # utils/dataset_loader.py
# import torch
# from torch.utils.data import Dataset
# import h5py
# import numpy as np
# import os
# import json
# from transformers import T5Tokenizer

# class RobotDataset(Dataset):
#     def __init__(self, hdf5_path, 
#                  window_size=16, 
#                  pred_horizon=64,
#                  tokenizer_path="/yanghaochuan/models/flan-t5-large",
#                  stats_path="/yanghaochuan/data/16dataset_stats.json"): 
        
#         self.hdf5_path = hdf5_path
#         self.window_size = window_size
#         self.pred_horizon = pred_horizon
        
#         # === 1. åŠ è½½ Tokenizer ===
#         print(f"[Dataset] Loading Tokenizer from {tokenizer_path}...")
#         try:
#             self.tokenizer = T5Tokenizer.from_pretrained(tokenizer_path, local_files_only=True)
#         except:
#             print("[Dataset] Local tokenizer failed, trying default...")
#             self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
#         # === 2. åŠ è½½ç»Ÿè®¡é‡ (ç”¨äº Z-Score å½’ä¸€åŒ–) ===
#         if not os.path.exists(stats_path):
#              raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {stats_path}ã€‚è¯·å…ˆè¿è¡Œ utils/compute_stats.pyï¼")
        
#         with open(stats_path, 'r') as f:
#             stats = json.load(f)
        
#         self.action_mean = torch.tensor(stats['action_mean']).float()
#         self.action_std = torch.tensor(stats['action_std']).float()
#         self.action_std = torch.maximum(self.action_std, torch.tensor(1e-2))
        
#         # === 3. æ‰«ææ•°æ®å¹¶å»ºç«‹ Anchor ç¼“å­˜ (Index-Based Asymmetric Context) ===
#         self.indices = []
#         self.anchor_bank = {}  # {demo_key: first_frame_tensor}
        
#         print(f"[Dataset] Scanning HDF5 for valid samples and Anchors...")
        
#         with h5py.File(hdf5_path, 'r') as f:
#             if 'data' not in f:
#                  raise ValueError(f"HDF5ç»“æ„é”™è¯¯: {hdf5_path} ä¸­æ²¡æœ‰ 'data' ç»„")

#             self.demos = list(f['data'].keys())
            
#             # --- ç¬¬ä¸€éæ‰«æï¼šæ”¶é›†æ‰€æœ‰ Type B (Anchors) ---
#             for demo_key in self.demos:
#                 demo_grp = f['data'][demo_key]
                
#                 # ä¼˜å…ˆè¯»å– HDF5 ä¸­çš„å±æ€§æ ‡è®°
#                 # å¦‚æœæ˜¯æ—§æ•°æ®æ²¡æœ‰æ ‡è®°ï¼Œå›é€€åˆ° demo_idx % 5 == 0 çš„é€»è¾‘
#                 data_type = demo_grp.attrs.get("data_type", None)
#                 if data_type is None:
#                     idx = int(demo_key.split('_')[1])
#                     if idx % 5 == 0:
#                         data_type = "type_b"
                
#                 # å¦‚æœè¢«æ ‡è®°ä¸º Type Bï¼Œåˆ™å­˜å…¥ Anchor é“¶è¡Œ
#                 if data_type == "type_b":
#                     main_key = 'agentview_image' if 'agentview_image' in demo_grp['obs'] else 'agentview_rgb'
#                     wrist_key = 'robot0_eye_in_hand_image'
                    
#                     if main_key in demo_grp['obs'] and wrist_key in demo_grp['obs']:
#                         m0 = torch.tensor(demo_grp['obs'][main_key][0]).float().permute(2, 0, 1) / 255.0
#                         w0 = torch.tensor(demo_grp['obs'][wrist_key][0]).float().permute(2, 0, 1) / 255.0
                        
#                         # [2, 3, H, W]
#                         anchor_frame = torch.stack([m0, w0], dim=0)
#                         self.anchor_bank[demo_key] = anchor_frame
            
#             print(f"[Dataset] Identified {len(self.anchor_bank)} anchors (Type B episodes).")

#             # --- ç¬¬äºŒéæ‰«æï¼šæ„å»ºè®­ç»ƒæ ·æœ¬ç´¢å¼• ---
#             for demo_key in self.demos:
#                 try:
#                     demo_grp = f['data'][demo_key]
#                     if 'actions' not in demo_grp: continue
                    
#                     total_len = demo_grp['actions'].shape[0]
#                     has_teacher = 'teacher_siglip' in demo_grp
#                     min_len = window_size + pred_horizon
                    
#                     if total_len > min_len:
#                         instruction = demo_grp.attrs.get('language_instruction', 'do nothing')
#                         if isinstance(instruction, bytes): instruction = instruction.decode('utf-8')

#                         for i in range(total_len - min_len): 
#                             self.indices.append({
#                                 'demo_key': demo_key,
#                                 'start_idx': i,
#                                 'instruction': instruction,
#                                 'has_teacher': has_teacher
#                             })
#                 except Exception as e:
#                     print(f"Skipping {demo_key}: {e}")
        
#         print(f"[Dataset] Loaded {len(self.indices)} samples.")

#     def __len__(self):
#         return len(self.indices)

#     def __getitem__(self, idx):
#         meta = self.indices[idx]
#         demo_key = meta['demo_key']
#         start = meta['start_idx']
#         instruction = meta['instruction']
        
#         read_len = self.window_size + self.pred_horizon
        
#         with h5py.File(self.hdf5_path, 'r') as f:
#             demo_grp = f['data'][demo_key]
            
#             # --- 1. Video (è¯»å– Type A çš„çœŸå®â€œç³Ÿç³•â€è§†é‡) ---
#             main_key = 'agentview_image' if 'agentview_image' in demo_grp['obs'] else 'agentview_rgb'
#             wrist_key = 'robot0_eye_in_hand_image'
            
#             main_seq = demo_grp['obs'][main_key][start : start + self.window_size]
#             wrist_seq = demo_grp['obs'][wrist_key][start : start + self.window_size]
            
#             main_t = torch.tensor(main_seq).float().permute(0, 3, 1, 2) / 255.0
#             wrist_t = torch.tensor(wrist_seq).float().permute(0, 3, 1, 2) / 255.0
            
#             # [2, 16, 3, H, W] -> [2, 3, 16, H, W]
#             video = torch.stack([main_t, wrist_t], dim=0).permute(0, 2, 1, 3, 4)
            
#             # --- 2. State & Action ---
#             state_seq_raw = demo_grp['obs']['robot0_joint_pos'][start : start + read_len]
#             if state_seq_raw.shape[0] < read_len:
#                 pad_len = read_len - state_seq_raw.shape[0]
#                 state_seq_raw = np.concatenate([state_seq_raw, np.tile(state_seq_raw[-1:], (pad_len, 1))], axis=0)

#             state_seq_tensor = torch.tensor(state_seq_raw).float()
#             state_seq_norm = (state_seq_tensor - self.action_mean) / self.action_std
            
#             state_input = state_seq_norm[:self.window_size]
#             action_target = state_seq_norm[self.window_size : self.window_size + self.pred_horizon]

#             # --- 3. First Frame (Context Injection) ---
#             # å…³é”®ä¿®æ”¹ï¼šæŒ‰ç´¢å¼•åˆ†ç»„æŸ¥æ‰¾ Anchor
#             # è§£æå½“å‰ç´¢å¼•: "demo_12" -> 12
#             current_idx = int(demo_key.split('_')[1])
            
#             # è®¡ç®—å½’å±çš„ Anchor ç´¢å¼• (å‘ä¸‹å–æ•´åˆ°æœ€è¿‘çš„ 5 çš„å€æ•°)
#             # ä¾‹å¦‚: 12 -> 10,  14 -> 10,  15 -> 15
#             anchor_idx = (current_idx // 5) * 5
#             anchor_key = f"demo_{anchor_idx}"
            
#             if anchor_key in self.anchor_bank:
#                 # å‘½ä¸­ç¼“å­˜ï¼šä½¿ç”¨å¯¹åº”çš„ Type B é¦–å¸§
#                 first_frame = self.anchor_bank[anchor_key]
#             else:
#                 # Fallback: ç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œé™¤é Type B è¢«è¿‡æ»¤äº†
#                 # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°±ç”¨è‡ªå·±çš„é¦–å¸§
#                 m0 = torch.tensor(demo_grp['obs'][main_key][0]).float().permute(2, 0, 1) / 255.0
#                 w0 = torch.tensor(demo_grp['obs'][wrist_key][0]).float().permute(2, 0, 1) / 255.0
#                 first_frame = torch.stack([m0, w0], dim=0)

#             # --- 4. Teachers ---
#             if meta['has_teacher']:
#                 teacher_siglip = torch.tensor(demo_grp['teacher_siglip'][start : start + self.window_size]).float()
#                 teacher_exo = torch.tensor(demo_grp['teacher_exo'][start : start + self.window_size]).float()
#             else:
#                 teacher_siglip = torch.zeros(self.window_size, 1152)
#                 teacher_exo = torch.zeros(self.window_size, 1152)

#         # Tokenize
#         text_tokens = self.tokenizer(
#             instruction, return_tensors="pt", padding="max_length", max_length=16, truncation=True
#         ).input_ids.squeeze(0)

#         return {
#             "video": video,
#             "state": state_input,
#             "action_target": action_target,
#             "text_tokens": text_tokens,
#             "first_frame": first_frame, # <--- Swapped Context (Type B)
#             "teacher_siglip": teacher_siglip,
#             "teacher_exo": teacher_exo
#         }
import torch
from torch.utils.data import Dataset
import h5py
import numpy as np
import os
import json
from transformers import T5Tokenizer

class RobotDataset(Dataset):
    def __init__(self, hdf5_path, 
                 window_size=6,         # ğŸŸ¢ ä¿®æ”¹ï¼šå®é™…è¾“å…¥ç»™æ¨¡å‹çš„å¸§æ•° (ä»16æ”¹ä¸º6)
                 history_len=500,        # ğŸŸ¢ æ–°å¢ï¼šæ¨¡æ‹Ÿçš„å†å²è§†é‡é•¿åº¦ (ä»ä¸­é‡‡æ ·6å¸§)
                 pred_horizon=64,
                 tokenizer_path="/yanghaochuan/models/flan-t5-large",
                 stats_path="/yanghaochuan/data/111dataset_stats.json"): 
        
        self.hdf5_path = hdf5_path
        self.window_size = window_size   # è¾“å‡ºç»™æ¨¡å‹çš„å¸§æ•° (6)
        self.history_len = history_len   # å†å²é‡‡æ ·çª—å£ (48)
        self.pred_horizon = pred_horizon
        
        # ğŸŸ¢ å®šä¹‰ç¨€ç–é¢„æµ‹æ­¥é•¿ (World Model Anchors)
        self.future_offsets = [0, 2, 4, 8, 16, 32]
        
        # === 1. åŠ è½½ Tokenizer ===
        print(f"[Dataset] Loading Tokenizer from {tokenizer_path}...")
        try:
            self.tokenizer = T5Tokenizer.from_pretrained(tokenizer_path, local_files_only=True)
        except:
            print("[Dataset] Local tokenizer failed, trying default...")
            self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
        # === 2. åŠ è½½ç»Ÿè®¡é‡ ===
        if not os.path.exists(stats_path):
             raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {stats_path}")
        
        with open(stats_path, 'r') as f:
            stats = json.load(f)
        
        self.action_mean = torch.tensor(stats['action_mean']).float()
        self.action_std = torch.tensor(stats['action_std']).float()
        self.action_std = torch.maximum(self.action_std, torch.tensor(1e-2))
        
        # === 3. æ‰«ææ•°æ® ===
        self.indices = []
        self.anchor_bank = {}
        
        print(f"[Dataset] Scanning HDF5...")
        with h5py.File(hdf5_path, 'r') as f:
            if 'data' not in f: raise ValueError(f"HDF5ç»“æ„é”™è¯¯")
            self.demos = list(f['data'].keys())
            
            # --- æ”¶é›† Anchors (Type B) ---
            for demo_key in self.demos:
                demo_grp = f['data'][demo_key]
                # å…¼å®¹æ—§æ•°æ®çš„ Type B åˆ¤å®š
                data_type = demo_grp.attrs.get("data_type", None)
                if data_type is None: 
                    idx = int(demo_key.split('_')[1])
                    if idx % 5 == 0: data_type = "type_b"
                
                if data_type == "type_b":
                    main_key = 'agentview_image' if 'agentview_image' in demo_grp['obs'] else 'agentview_rgb'
                    wrist_key = 'robot0_eye_in_hand_image'
                    if main_key in demo_grp['obs']:
                        m0 = torch.tensor(demo_grp['obs'][main_key][0]).float().permute(2, 0, 1) / 255.0
                        w0 = torch.tensor(demo_grp['obs'][wrist_key][0]).float().permute(2, 0, 1) / 255.0
                        self.anchor_bank[demo_key] = torch.stack([m0, w0], dim=0)

            # --- æ„å»ºæ ·æœ¬ç´¢å¼• ---
            # æ³¨æ„ï¼šè¿™é‡Œçš„ start_idx ä»£è¡¨çš„æ˜¯â€œå½“å‰æ—¶åˆ» tâ€çš„åŸºå‡†ç‚¹
            # å®é™…ä¸Šï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ t + pred_horizon ä¸è¶Šç•Œ
            # å†å²æ•°æ®ä¸å¤Ÿ history_len æ—¶ï¼Œæˆ‘ä»¬ä¼šç”¨é¦–å¸§å¡«å…… (Handling Cold Start)
            for demo_key in self.demos:
                demo_grp = f['data'][demo_key]
                if 'actions' not in demo_grp: continue
                total_len = demo_grp['actions'].shape[0]
                
                # åªè¦å‰©ä½™é•¿åº¦å¤Ÿé¢„æµ‹æœªæ¥å³å¯
                if total_len > self.pred_horizon:
                    instr = demo_grp.attrs.get('language_instruction', 'do nothing')
                    if isinstance(instr, bytes): instr = instr.decode('utf-8')
                    has_teacher = 'teacher_siglip' in demo_grp
                    
                    # æˆ‘ä»¬è®© i ä»£è¡¨ "å½“å‰æ—¶åˆ» t"
                    # éå†èŒƒå›´ï¼šä» 0 åˆ° total_len - pred_horizon
                    for i in range(total_len - self.pred_horizon): 
                        self.indices.append({
                            'demo_key': demo_key, 
                            'current_t': i, 
                            'instruction': instr, 
                            'has_teacher': has_teacher
                        })
        print(f"[Dataset] Loaded {len(self.indices)} samples.")

    def __len__(self): return len(self.indices)

    def __getitem__(self, idx):
        meta = self.indices[idx]
        demo_key = meta['demo_key']
        current_t = meta['current_t'] # å½“å‰æ—¶åˆ» t
        
        with h5py.File(self.hdf5_path, 'r') as f:
            demo_grp = f['data'][demo_key]
            demo_len = demo_grp['actions'].shape[0]

            # === 1. Video: åŠ¨æ€å‡åŒ€é‡‡æ · (Uniform Sampling) ===
            main_key = 'agentview_image' if 'agentview_image' in demo_grp['obs'] else 'agentview_rgb'
            wrist_key = 'robot0_eye_in_hand_image'
            
            # ç¡®å®šå†å²çª—å£: [t - history_len + 1, t]
            # ä¾‹å¦‚ t=10, len=48 -> start=-37 (è¶Šç•Œ) -> å®é™…ä¸Šåªæœ‰ 0~10 å¯ç”¨
            history_start = max(0, current_t - self.history_len + 1)
            history_end = current_t + 1 # åˆ‡ç‰‡æ˜¯ä¸åŒ…å«endçš„ï¼Œæ‰€ä»¥+1ä»¥åŒ…å«t
            
            valid_len = history_end - history_start
            
            # è®¡ç®—å‡åŒ€é‡‡æ ·ç´¢å¼• (åœ¨ valid_len èŒƒå›´å†…é€‰ window_size å¸§)
            # ä¾‹å¦‚ä» 100 å¸§é‡Œé€‰ 6 å¸§ -> [0, 19, 39, ..., 99]
            if valid_len < self.window_size:
                # å†·å¯åŠ¨ç­–ç•¥ï¼šå¦‚æœå†å²ä¸å¤Ÿé•¿ (ä¾‹å¦‚åˆšå¼€å§‹ç¬¬2å¸§)ï¼Œæ€ä¹ˆé€‰6å¸§ï¼Ÿ
                # ç­–ç•¥ï¼šé‡å¤åˆ©ç”¨ç°æœ‰å¸§ï¼Œæˆ–è€…å…¨éƒ¨å–å®Œåç”¨é¦–å¸§å¡«å……ã€‚
                # np.linspace åœ¨ valid_len < num æ—¶ä¼šè‡ªåŠ¨å¤„ç† (äº§ç”Ÿé‡å¤ç´¢å¼•ï¼Œå¦‚ [0,0,1,1,2,2])
                # è¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ "Copy First Frame" çš„æ³›åŒ–ç‰ˆæœ¬
                offsets = np.linspace(0, valid_len - 1, self.window_size).astype(int)
            else:
                offsets = np.linspace(0, valid_len - 1, self.window_size).astype(int)
            
            # æ˜ å°„å›å…¨å±€ç´¢å¼•
            global_indices = history_start + offsets
            # æ’åºç¡®ä¿æ—¶åºæ­£ç¡® (linspace å·²ç»æ˜¯é€’å¢çš„ï¼Œä¿é™©èµ·è§)
            global_indices = np.sort(global_indices)
            
            # è¯»å–è§†é¢‘ (HDF5 æ”¯æŒåˆ—è¡¨ç´¢å¼•)
            # [6, H, W, 3]
            # main_frames = demo_grp['obs'][main_key][global_indices]
            # wrist_frames = demo_grp['obs'][wrist_key][global_indices]

            # ğŸŸ¢ [ä¿®å¤å¼€å§‹]ï¼šh5py ä¸æ”¯æŒé‡å¤ç´¢å¼•ï¼Œå¿…é¡»å…ˆå»é‡å†æ˜ å°„
            # 1. è·å–å”¯ä¸€ç´¢å¼•å’Œé‡å»ºæ˜ å°„è¡¨
            unique_indices, inverse_indices = np.unique(global_indices, return_inverse=True)
            
            # 2. åªè¯»å–å”¯ä¸€çš„å¸§ (h5py è¦æ±‚ä¸¥æ ¼é€’å¢ï¼Œunique è‡ªåŠ¨æ’å¥½åºäº†)
            # è¯»å‡ºæ¥æ˜¯ [U, H, W, 3]ï¼Œå…¶ä¸­ U <= window_size
            unique_main_frames = demo_grp['obs'][main_key][unique_indices]
            unique_wrist_frames = demo_grp['obs'][wrist_key][unique_indices]
            
            # 3. åœ¨å†…å­˜ä¸­é‡å»ºå®Œæ•´åºåˆ— (åŒ…å«é‡å¤å¸§)
            # ä½¿ç”¨ inverse_indices æŠŠ [U, ...] æ˜ å°„å› [6, ...]
            main_frames = unique_main_frames[inverse_indices]
            wrist_frames = unique_wrist_frames[inverse_indices]
            
            # è½¬ Tensor [6, 3, H, W]
            main_seq = torch.tensor(main_frames).float().permute(0, 3, 1, 2) / 255.0
            wrist_seq = torch.tensor(wrist_frames).float().permute(0, 3, 1, 2) / 255.0
            
            # Stack Views: [2, 3, 6, H, W]
            video = torch.stack([main_seq, wrist_seq], dim=0).permute(0, 1, 2, 3, 4) # è¿™é‡Œçš„ dim é¡ºåºæŒ‰ä½ æ¨¡å‹è¦æ±‚æ¥
            # æ³¨æ„ï¼šä¹‹å‰æ˜¯ [2, 3, T, H, W] è¿˜æ˜¯ [B, 2, C, T, H, W]?
            # ä½ çš„æ—§ä»£ç æ˜¯: torch.stack([main_tensor, wrist_tensor], dim=0).permute(0, 2, 1, 3, 4)
            # å³ [2, T, 3, H, W] -> [2, 3, T, H, W]
            # è¿™é‡Œ main_seq æ˜¯ [T, 3, H, W]ï¼Œæ‰€ä»¥ permute åæ˜¯ [2, 3, T, H, W]
            video = torch.stack([main_seq, wrist_seq], dim=0).transpose(1, 2) 

            # === 2. State & Action (RDT ä»ç„¶éœ€è¦æœªæ¥çš„ Action) ===
            # State: å–å½“å‰æ—¶åˆ» t çš„çŠ¶æ€ (ä½œä¸º Condition)
            # Action: å– t åˆ° t + pred_horizon
            state_raw = demo_grp['obs']['robot0_joint_pos'][current_t : current_t + self.pred_horizon + 1]
            
            # è¡¥é½
            target_len = self.pred_horizon + 1 # 1ä¸ªå½“å‰State + Kä¸ªAction
            if state_raw.shape[0] < target_len:
                state_raw = np.concatenate([state_raw, np.tile(state_raw[-1:], (target_len-state_raw.shape[0], 1))], axis=0)
            
            state_norm = (torch.tensor(state_raw).float() - self.action_mean) / self.action_std
            
            state_input = state_norm[:1] # [1, 8] - å½“å‰ State
            # å¦‚æœ RDT éœ€è¦å†å² State åºåˆ—ï¼Œè¿™é‡Œè¦æ”¹ã€‚ä½†æ ¹æ®ä½ çš„ForeSightè®¾è®¡ï¼ŒRDTç”¨å½“å‰State+Latentå³å¯ã€‚
            # ä¸ºäº†å…¼å®¹ä½ ä¹‹å‰çš„ dataset (è¿”å› window_size ä¸ª state)ï¼Œæˆ‘ä»¬å¯ä»¥å¡«å……
            # ä½†æ–°é€»è¾‘ä¸‹ï¼ŒState ä¸»è¦æ˜¯å½“å‰çŠ¶æ€ã€‚
            # è¿™é‡Œä¸ºäº†å…¼å®¹æ€§ï¼Œè¿”å› [16, 8]ï¼Œå‰é¢ç”¨å½“å‰çŠ¶æ€å¡«å……
            state_input_expanded = state_norm[0].unsqueeze(0).repeat(self.window_size, 1) # [6, 8]
            
            action_target = state_norm[1:] # [64, 8]

            # === 3. First Frame (Anchor) ===
            curr_idx = int(demo_key.split('_')[1])
            anchor_key = f"demo_{(curr_idx//5)*5}"
            first_frame = self.anchor_bank.get(anchor_key, video[:, :, 0]) # Fallback to current start

            # === 4. Teachers (World Model Targets: Sparse Future) ===
            future_exo_feats = []
            if meta['has_teacher']:
                # è¯»å– t, t+4, t+8... çš„ç‰¹å¾
                for offset in self.future_offsets:
                    target_idx = min(current_t + offset, demo_len - 1)
                    future_exo_feats.append(torch.from_numpy(demo_grp['teacher_exo'][target_idx]).float())
                future_exo_target = torch.stack(future_exo_feats)
                
                # è¯­ä¹‰è¾…åŠ© (å–å½“å‰çª—å£çš„å¹³å‡ï¼Œæˆ–è€…ç›´æ¥å–å½“å‰å¸§è¯­ä¹‰)
                # ä¸ºäº†ç®€å•ï¼Œå–å½“å‰ t çš„ SigLIP
                teacher_siglip = torch.from_numpy(demo_grp['teacher_siglip'][current_t]).float().unsqueeze(0).repeat(self.window_size, 1)
            else:
                teacher_siglip = torch.zeros(self.window_size, 1152)
                future_exo_target = torch.zeros(len(self.future_offsets), 1152)
                
            # Teacher Exo Legacy (ä¸ºäº†å…¼å®¹æ—§æ¥å£ï¼Œå…¨0å³å¯ï¼Œæˆ–è€…å–å½“å‰çš„)
            teacher_exo_legacy = torch.zeros(self.window_size, 1152)

        text_tokens = self.tokenizer(meta['instruction'], return_tensors="pt", padding="max_length", max_length=16, truncation=True).input_ids.squeeze(0)

        return {
            "video": video,                 # [2, 3, 6, H, W] (Uniform Sampled)
            "state": state_input_expanded,  # [6, 8] (Current State repeated)
            "action_target": action_target, # [64, 8]
            "text_tokens": text_tokens,
            "first_frame": first_frame,
            "teacher_siglip": teacher_siglip,
            "teacher_exo": teacher_exo_legacy,
            "future_exo_target": future_exo_target # [6, 1152] (Sparse Future)
        }