# # utils/dataset_loader.py
# import torch
# from torch.utils.data import Dataset
# import h5py
# import numpy as np
# import os
# import json
# from transformers import T5Tokenizer

# class RobotDataset(Dataset):
#     def __init__(self, hdf5_path, 
#                  window_size=16, 
#                  pred_horizon=64,
#                  tokenizer_path="/yanghaochuan/models/flan-t5-large",
#                  stats_path="/yanghaochuan/data/16dataset_stats.json"): 
        
#         self.hdf5_path = hdf5_path
#         self.window_size = window_size
#         self.pred_horizon = pred_horizon
        
#         # === 1. åŠ è½½ Tokenizer ===
#         print(f"[Dataset] Loading Tokenizer from {tokenizer_path}...")
#         try:
#             self.tokenizer = T5Tokenizer.from_pretrained(tokenizer_path, local_files_only=True)
#         except:
#             print("[Dataset] Local tokenizer failed, trying default...")
#             self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
#         # === 2. åŠ è½½ç»Ÿè®¡é‡ (ç”¨äº Z-Score å½’ä¸€åŒ–) ===
#         if not os.path.exists(stats_path):
#              raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {stats_path}ã€‚è¯·å…ˆè¿è¡Œ utils/compute_stats.pyï¼")
        
#         with open(stats_path, 'r') as f:
#             stats = json.load(f)
        
#         self.action_mean = torch.tensor(stats['action_mean']).float()
#         self.action_std = torch.tensor(stats['action_std']).float()
#         self.action_std = torch.maximum(self.action_std, torch.tensor(1e-2))
        
#         # === 3. æ‰«ææ•°æ®å¹¶å»ºç«‹ Anchor ç¼“å­˜ (Index-Based Asymmetric Context) ===
#         self.indices = []
#         self.anchor_bank = {}  # {demo_key: first_frame_tensor}
        
#         print(f"[Dataset] Scanning HDF5 for valid samples and Anchors...")
        
#         with h5py.File(hdf5_path, 'r') as f:
#             if 'data' not in f:
#                  raise ValueError(f"HDF5ç»“æ„é”™è¯¯: {hdf5_path} ä¸­æ²¡æœ‰ 'data' ç»„")

#             self.demos = list(f['data'].keys())
            
#             # --- ç¬¬ä¸€éæ‰«æï¼šæ”¶é›†æ‰€æœ‰ Type B (Anchors) ---
#             for demo_key in self.demos:
#                 demo_grp = f['data'][demo_key]
                
#                 # ä¼˜å…ˆè¯»å– HDF5 ä¸­çš„å±æ€§æ ‡è®°
#                 # å¦‚æœæ˜¯æ—§æ•°æ®æ²¡æœ‰æ ‡è®°ï¼Œå›é€€åˆ° demo_idx % 5 == 0 çš„é€»è¾‘
#                 data_type = demo_grp.attrs.get("data_type", None)
#                 if data_type is None:
#                     idx = int(demo_key.split('_')[1])
#                     if idx % 5 == 0:
#                         data_type = "type_b"
                
#                 # å¦‚æœè¢«æ ‡è®°ä¸º Type Bï¼Œåˆ™å­˜å…¥ Anchor é“¶è¡Œ
#                 if data_type == "type_b":
#                     main_key = 'agentview_image' if 'agentview_image' in demo_grp['obs'] else 'agentview_rgb'
#                     wrist_key = 'robot0_eye_in_hand_image'
                    
#                     if main_key in demo_grp['obs'] and wrist_key in demo_grp['obs']:
#                         m0 = torch.tensor(demo_grp['obs'][main_key][0]).float().permute(2, 0, 1) / 255.0
#                         w0 = torch.tensor(demo_grp['obs'][wrist_key][0]).float().permute(2, 0, 1) / 255.0
                        
#                         # [2, 3, H, W]
#                         anchor_frame = torch.stack([m0, w0], dim=0)
#                         self.anchor_bank[demo_key] = anchor_frame
            
#             print(f"[Dataset] Identified {len(self.anchor_bank)} anchors (Type B episodes).")

#             # --- ç¬¬äºŒéæ‰«æï¼šæ„å»ºè®­ç»ƒæ ·æœ¬ç´¢å¼• ---
#             for demo_key in self.demos:
#                 try:
#                     demo_grp = f['data'][demo_key]
#                     if 'actions' not in demo_grp: continue
                    
#                     total_len = demo_grp['actions'].shape[0]
#                     has_teacher = 'teacher_siglip' in demo_grp
#                     min_len = window_size + pred_horizon
                    
#                     if total_len > min_len:
#                         instruction = demo_grp.attrs.get('language_instruction', 'do nothing')
#                         if isinstance(instruction, bytes): instruction = instruction.decode('utf-8')

#                         for i in range(total_len - min_len): 
#                             self.indices.append({
#                                 'demo_key': demo_key,
#                                 'start_idx': i,
#                                 'instruction': instruction,
#                                 'has_teacher': has_teacher
#                             })
#                 except Exception as e:
#                     print(f"Skipping {demo_key}: {e}")
        
#         print(f"[Dataset] Loaded {len(self.indices)} samples.")

#     def __len__(self):
#         return len(self.indices)

#     def __getitem__(self, idx):
#         meta = self.indices[idx]
#         demo_key = meta['demo_key']
#         start = meta['start_idx']
#         instruction = meta['instruction']
        
#         read_len = self.window_size + self.pred_horizon
        
#         with h5py.File(self.hdf5_path, 'r') as f:
#             demo_grp = f['data'][demo_key]
            
#             # --- 1. Video (è¯»å– Type A çš„çœŸå®â€œç³Ÿç³•â€è§†é‡) ---
#             main_key = 'agentview_image' if 'agentview_image' in demo_grp['obs'] else 'agentview_rgb'
#             wrist_key = 'robot0_eye_in_hand_image'
            
#             main_seq = demo_grp['obs'][main_key][start : start + self.window_size]
#             wrist_seq = demo_grp['obs'][wrist_key][start : start + self.window_size]
            
#             main_t = torch.tensor(main_seq).float().permute(0, 3, 1, 2) / 255.0
#             wrist_t = torch.tensor(wrist_seq).float().permute(0, 3, 1, 2) / 255.0
            
#             # [2, 16, 3, H, W] -> [2, 3, 16, H, W]
#             video = torch.stack([main_t, wrist_t], dim=0).permute(0, 2, 1, 3, 4)
            
#             # --- 2. State & Action ---
#             state_seq_raw = demo_grp['obs']['robot0_joint_pos'][start : start + read_len]
#             if state_seq_raw.shape[0] < read_len:
#                 pad_len = read_len - state_seq_raw.shape[0]
#                 state_seq_raw = np.concatenate([state_seq_raw, np.tile(state_seq_raw[-1:], (pad_len, 1))], axis=0)

#             state_seq_tensor = torch.tensor(state_seq_raw).float()
#             state_seq_norm = (state_seq_tensor - self.action_mean) / self.action_std
            
#             state_input = state_seq_norm[:self.window_size]
#             action_target = state_seq_norm[self.window_size : self.window_size + self.pred_horizon]

#             # --- 3. First Frame (Context Injection) ---
#             # å…³é”®ä¿®æ”¹ï¼šæŒ‰ç´¢å¼•åˆ†ç»„æŸ¥æ‰¾ Anchor
#             # è§£æå½“å‰ç´¢å¼•: "demo_12" -> 12
#             current_idx = int(demo_key.split('_')[1])
            
#             # è®¡ç®—å½’å±çš„ Anchor ç´¢å¼• (å‘ä¸‹å–æ•´åˆ°æœ€è¿‘çš„ 5 çš„å€æ•°)
#             # ä¾‹å¦‚: 12 -> 10,  14 -> 10,  15 -> 15
#             anchor_idx = (current_idx // 5) * 5
#             anchor_key = f"demo_{anchor_idx}"
            
#             if anchor_key in self.anchor_bank:
#                 # å‘½ä¸­ç¼“å­˜ï¼šä½¿ç”¨å¯¹åº”çš„ Type B é¦–å¸§
#                 first_frame = self.anchor_bank[anchor_key]
#             else:
#                 # Fallback: ç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œé™¤é Type B è¢«è¿‡æ»¤äº†
#                 # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°±ç”¨è‡ªå·±çš„é¦–å¸§
#                 m0 = torch.tensor(demo_grp['obs'][main_key][0]).float().permute(2, 0, 1) / 255.0
#                 w0 = torch.tensor(demo_grp['obs'][wrist_key][0]).float().permute(2, 0, 1) / 255.0
#                 first_frame = torch.stack([m0, w0], dim=0)

#             # --- 4. Teachers ---
#             if meta['has_teacher']:
#                 teacher_siglip = torch.tensor(demo_grp['teacher_siglip'][start : start + self.window_size]).float()
#                 teacher_exo = torch.tensor(demo_grp['teacher_exo'][start : start + self.window_size]).float()
#             else:
#                 teacher_siglip = torch.zeros(self.window_size, 1152)
#                 teacher_exo = torch.zeros(self.window_size, 1152)

#         # Tokenize
#         text_tokens = self.tokenizer(
#             instruction, return_tensors="pt", padding="max_length", max_length=16, truncation=True
#         ).input_ids.squeeze(0)

#         return {
#             "video": video,
#             "state": state_input,
#             "action_target": action_target,
#             "text_tokens": text_tokens,
#             "first_frame": first_frame, # <--- Swapped Context (Type B)
#             "teacher_siglip": teacher_siglip,
#             "teacher_exo": teacher_exo
#         }



# import torch
# from torch.utils.data import Dataset
# import h5py
# import numpy as np
# import os
# import json
# from transformers import T5Tokenizer
# from torchvision import transforms

# class RobotDataset(Dataset):
#     def __init__(self, hdf5_path, in_memory=True, 
#                  window_size=6,         # ğŸŸ¢ ä¿®æ”¹ï¼šå®é™…è¾“å…¥ç»™æ¨¡å‹çš„å¸§æ•° (ä»16æ”¹ä¸º6)
#                  history_len=500,        # ğŸŸ¢ æ–°å¢ï¼šæ¨¡æ‹Ÿçš„å†å²è§†é‡é•¿åº¦ (ä»ä¸­é‡‡æ ·6å¸§)
#                  pred_horizon=64,
#                  tokenizer_path="/yanghaochuan/models/flan-t5-large",
#                  stats_path="/yanghaochuan/data/111dataset_stats.json"): 
        
#         self.hdf5_path = hdf5_path
#         self.window_size = window_size   # è¾“å‡ºç»™æ¨¡å‹çš„å¸§æ•° (6)
#         self.history_len = history_len   # å†å²é‡‡æ ·çª—å£ (48)
#         self.pred_horizon = pred_horizon
        
#         # ğŸŸ¢ å®šä¹‰ç¨€ç–é¢„æµ‹æ­¥é•¿ (World Model Anchors)
#         self.future_offsets = [0, 2, 4, 8, 16, 32]

#         # === [æ–°å¢] å®šä¹‰å½’ä¸€åŒ– (VideoMAE æ ‡å‡†) ===
#         self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], 
#                                               std=[0.229, 0.224, 0.225])
#         # === 1. åŠ è½½ Tokenizer ===
#         print(f"[Dataset] Loading Tokenizer from {tokenizer_path}...")
#         try:
#             self.tokenizer = T5Tokenizer.from_pretrained(tokenizer_path, local_files_only=True)
#         except:
#             print("[Dataset] Local tokenizer failed, trying default...")
#             self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
#         # === 2. åŠ è½½ç»Ÿè®¡é‡ ===
#         if not os.path.exists(stats_path):
#              raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {stats_path}")
        
#         with open(stats_path, 'r') as f:
#             stats = json.load(f)
        
#         self.action_mean = torch.tensor(stats['action_mean']).float()
#         self.action_std = torch.tensor(stats['action_std']).float()
#         self.action_std = torch.maximum(self.action_std, torch.tensor(1e-2))
        
#         # === 3. æ‰«ææ•°æ® ===
#         self.indices = []
#         self.anchor_bank = {}
        
#         print(f"[Dataset] Scanning HDF5...")
#         with h5py.File(hdf5_path, 'r') as f:
#             if 'data' not in f: raise ValueError(f"HDF5ç»“æ„é”™è¯¯")
#             self.demos = list(f['data'].keys())
            
#             # --- æ”¶é›† Anchors (Type B) ---
#             for demo_key in self.demos:
#                 demo_grp = f['data'][demo_key]
#                 # å…¼å®¹æ—§æ•°æ®çš„ Type B åˆ¤å®š
#                 data_type = demo_grp.attrs.get("data_type", None)
#                 if data_type is None: 
#                     idx = int(demo_key.split('_')[1])
#                     if idx % 5 == 0: data_type = "type_b"
                
#                 if data_type == "type_b":
#                     main_key = 'agentview_image' if 'agentview_image' in demo_grp['obs'] else 'agentview_rgb'
#                     wrist_key = 'robot0_eye_in_hand_image'
#                     if main_key in demo_grp['obs']:
#                         m0 = torch.tensor(demo_grp['obs'][main_key][0]).float().permute(2, 0, 1) / 255.0
#                         w0 = torch.tensor(demo_grp['obs'][wrist_key][0]).float().permute(2, 0, 1) / 255.0
#                         self.anchor_bank[demo_key] = torch.stack([m0, w0], dim=0)

#             # --- æ„å»ºæ ·æœ¬ç´¢å¼• ---
#             # æ³¨æ„ï¼šè¿™é‡Œçš„ start_idx ä»£è¡¨çš„æ˜¯â€œå½“å‰æ—¶åˆ» tâ€çš„åŸºå‡†ç‚¹
#             # å®é™…ä¸Šï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ t + pred_horizon ä¸è¶Šç•Œ
#             # å†å²æ•°æ®ä¸å¤Ÿ history_len æ—¶ï¼Œæˆ‘ä»¬ä¼šç”¨é¦–å¸§å¡«å…… (Handling Cold Start)
#             for demo_key in self.demos:
#                 demo_grp = f['data'][demo_key]
#                 if 'actions' not in demo_grp: continue
#                 total_len = demo_grp['actions'].shape[0]
                
#                 # åªè¦å‰©ä½™é•¿åº¦å¤Ÿé¢„æµ‹æœªæ¥å³å¯
#                 if total_len > self.pred_horizon:
#                     instr = demo_grp.attrs.get('language_instruction', 'do nothing')
#                     if isinstance(instr, bytes): instr = instr.decode('utf-8')
#                     has_teacher = 'teacher_siglip' in demo_grp
                    
#                     # æˆ‘ä»¬è®© i ä»£è¡¨ "å½“å‰æ—¶åˆ» t"
#                     # éå†èŒƒå›´ï¼šä» 0 åˆ° total_len - pred_horizon
#                     for i in range(total_len - self.pred_horizon): 
#                         self.indices.append({
#                             'demo_key': demo_key, 
#                             'current_t': i, 
#                             'instruction': instr, 
#                             'has_teacher': has_teacher
#                         })
#         print(f"[Dataset] Loaded {len(self.indices)} samples.")

#     def __len__(self): return len(self.indices)

#     def __getitem__(self, idx):
#         meta = self.indices[idx]
#         demo_key = meta['demo_key']
#         current_t = meta['current_t'] # å½“å‰æ—¶åˆ» t
        
#         with h5py.File(self.hdf5_path, 'r') as f:
#             demo_grp = f['data'][demo_key]
#             demo_len = demo_grp['actions'].shape[0]

#             # === 1. Video: åŠ¨æ€å‡åŒ€é‡‡æ · (Uniform Sampling) ===
#             main_key = 'agentview_image' if 'agentview_image' in demo_grp['obs'] else 'agentview_rgb'
#             wrist_key = 'robot0_eye_in_hand_image'
            
#             # ç¡®å®šå†å²çª—å£: [t - history_len + 1, t]
#             # ä¾‹å¦‚ t=10, len=48 -> start=-37 (è¶Šç•Œ) -> å®é™…ä¸Šåªæœ‰ 0~10 å¯ç”¨
#             history_start = max(0, current_t - self.history_len + 1)
#             history_end = current_t + 1 # åˆ‡ç‰‡æ˜¯ä¸åŒ…å«endçš„ï¼Œæ‰€ä»¥+1ä»¥åŒ…å«t
            
#             valid_len = history_end - history_start
            
#             # è®¡ç®—å‡åŒ€é‡‡æ ·ç´¢å¼• (åœ¨ valid_len èŒƒå›´å†…é€‰ window_size å¸§)
#             # ä¾‹å¦‚ä» 100 å¸§é‡Œé€‰ 6 å¸§ -> [0, 19, 39, ..., 99]
#             if valid_len < self.window_size:
#                 # å†·å¯åŠ¨ç­–ç•¥ï¼šå¦‚æœå†å²ä¸å¤Ÿé•¿ (ä¾‹å¦‚åˆšå¼€å§‹ç¬¬2å¸§)ï¼Œæ€ä¹ˆé€‰6å¸§ï¼Ÿ
#                 # ç­–ç•¥ï¼šé‡å¤åˆ©ç”¨ç°æœ‰å¸§ï¼Œæˆ–è€…å…¨éƒ¨å–å®Œåç”¨é¦–å¸§å¡«å……ã€‚
#                 # np.linspace åœ¨ valid_len < num æ—¶ä¼šè‡ªåŠ¨å¤„ç† (äº§ç”Ÿé‡å¤ç´¢å¼•ï¼Œå¦‚ [0,0,1,1,2,2])
#                 # è¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ "Copy First Frame" çš„æ³›åŒ–ç‰ˆæœ¬
#                 offsets = np.linspace(0, valid_len - 1, self.window_size).astype(int)
#             else:
#                 offsets = np.linspace(0, valid_len - 1, self.window_size).astype(int)
            
#             # æ˜ å°„å›å…¨å±€ç´¢å¼•
#             global_indices = history_start + offsets
#             # æ’åºç¡®ä¿æ—¶åºæ­£ç¡® (linspace å·²ç»æ˜¯é€’å¢çš„ï¼Œä¿é™©èµ·è§)
#             global_indices = np.sort(global_indices)
            
#             # è¯»å–è§†é¢‘ (HDF5 æ”¯æŒåˆ—è¡¨ç´¢å¼•)
#             # [6, H, W, 3]
#             # main_frames = demo_grp['obs'][main_key][global_indices]
#             # wrist_frames = demo_grp['obs'][wrist_key][global_indices]

#             # ğŸŸ¢ [ä¿®å¤å¼€å§‹]ï¼šh5py ä¸æ”¯æŒé‡å¤ç´¢å¼•ï¼Œå¿…é¡»å…ˆå»é‡å†æ˜ å°„
#             # 1. è·å–å”¯ä¸€ç´¢å¼•å’Œé‡å»ºæ˜ å°„è¡¨
#             unique_indices, inverse_indices = np.unique(global_indices, return_inverse=True)
            
#             # 2. åªè¯»å–å”¯ä¸€çš„å¸§ (h5py è¦æ±‚ä¸¥æ ¼é€’å¢ï¼Œunique è‡ªåŠ¨æ’å¥½åºäº†)
#             # è¯»å‡ºæ¥æ˜¯ [U, H, W, 3]ï¼Œå…¶ä¸­ U <= window_size
#             unique_main_frames = demo_grp['obs'][main_key][unique_indices]
#             unique_wrist_frames = demo_grp['obs'][wrist_key][unique_indices]
            
#             # 3. åœ¨å†…å­˜ä¸­é‡å»ºå®Œæ•´åºåˆ— (åŒ…å«é‡å¤å¸§)
#             # ä½¿ç”¨ inverse_indices æŠŠ [U, ...] æ˜ å°„å› [6, ...]
#             main_frames = unique_main_frames[inverse_indices]
#             wrist_frames = unique_wrist_frames[inverse_indices]
            
#             # # è½¬ Tensor [6, 3, H, W]
#             # main_seq = torch.tensor(main_frames).float().permute(0, 3, 1, 2) / 255.0
#             # wrist_seq = torch.tensor(wrist_frames).float().permute(0, 3, 1, 2) / 255.0
            
#             # # Stack Views: [2, 3, 6, H, W]
#             # video = torch.stack([main_seq, wrist_seq], dim=0).permute(0, 1, 2, 3, 4) # è¿™é‡Œçš„ dim é¡ºåºæŒ‰ä½ æ¨¡å‹è¦æ±‚æ¥

#             # ä¿®æ”¹å (å…ˆ /255.0ï¼Œå†å½’ä¸€åŒ–):
#             main_t_raw = torch.tensor(main_frames).float().permute(0, 3, 1, 2) / 255.0
#             wrist_t_raw = torch.tensor(wrist_frames).float().permute(0, 3, 1, 2) / 255.0
            
#             # åº”ç”¨å½’ä¸€åŒ– (æ³¨æ„ç»´åº¦åŒ¹é…ï¼ŒNormalizeä½œç”¨äºCç»´åº¦)
#             # main_t_raw shape: [6, 3, H, W]
#             main_seq = self.normalize(main_t_raw)
#             wrist_seq = self.normalize(wrist_t_raw)
            
#             # Stack Views: [2, 3, 6, H, W] (æ ¹æ®ä½ çš„æ¨¡å‹è¦æ±‚è°ƒæ•´)
#             video = torch.stack([main_seq, wrist_seq], dim=0).transpose(1, 2)


#             # æ³¨æ„ï¼šä¹‹å‰æ˜¯ [2, 3, T, H, W] è¿˜æ˜¯ [B, 2, C, T, H, W]?
#             # ä½ çš„æ—§ä»£ç æ˜¯: torch.stack([main_tensor, wrist_tensor], dim=0).permute(0, 2, 1, 3, 4)
#             # å³ [2, T, 3, H, W] -> [2, 3, T, H, W]
#             # è¿™é‡Œ main_seq æ˜¯ [T, 3, H, W]ï¼Œæ‰€ä»¥ permute åæ˜¯ [2, 3, T, H, W]
#             video = torch.stack([main_seq, wrist_seq], dim=0).transpose(1, 2) 

#             # === 2. State & Action (RDT ä»ç„¶éœ€è¦æœªæ¥çš„ Action) ===
#             # State: å–å½“å‰æ—¶åˆ» t çš„çŠ¶æ€ (ä½œä¸º Condition)
#             # Action: å– t åˆ° t + pred_horizon
#             state_raw = demo_grp['obs']['robot0_joint_pos'][current_t : current_t + self.pred_horizon + 1]
            
#             # è¡¥é½
#             target_len = self.pred_horizon + 1 # 1ä¸ªå½“å‰State + Kä¸ªAction
#             if state_raw.shape[0] < target_len:
#                 state_raw = np.concatenate([state_raw, np.tile(state_raw[-1:], (target_len-state_raw.shape[0], 1))], axis=0)
            
#             state_norm = (torch.tensor(state_raw).float() - self.action_mean) / self.action_std
            
#             state_input = state_norm[:1] # [1, 8] - å½“å‰ State
#             # å¦‚æœ RDT éœ€è¦å†å² State åºåˆ—ï¼Œè¿™é‡Œè¦æ”¹ã€‚ä½†æ ¹æ®ä½ çš„ForeSightè®¾è®¡ï¼ŒRDTç”¨å½“å‰State+Latentå³å¯ã€‚
#             # ä¸ºäº†å…¼å®¹ä½ ä¹‹å‰çš„ dataset (è¿”å› window_size ä¸ª state)ï¼Œæˆ‘ä»¬å¯ä»¥å¡«å……
#             # ä½†æ–°é€»è¾‘ä¸‹ï¼ŒState ä¸»è¦æ˜¯å½“å‰çŠ¶æ€ã€‚
#             # è¿™é‡Œä¸ºäº†å…¼å®¹æ€§ï¼Œè¿”å› [16, 8]ï¼Œå‰é¢ç”¨å½“å‰çŠ¶æ€å¡«å……
#             state_input_expanded = state_norm[0].unsqueeze(0).repeat(self.window_size, 1) # [6, 8]
            
#             action_target = state_norm[1:] # [64, 8]

#             # === 3. First Frame (Anchor) ===
#             curr_idx = int(demo_key.split('_')[1])
#             anchor_key = f"demo_{(curr_idx//5)*5}"
#             first_frame = self.anchor_bank.get(anchor_key, video[:, :, 0]) # Fallback to current start

#             # === 4. Teachers (World Model Targets: Sparse Future) ===
#             future_exo_feats = []
#             if meta['has_teacher']:
#                 # è¯»å– t, t+4, t+8... çš„ç‰¹å¾
#                 for offset in self.future_offsets:
#                     target_idx = min(current_t + offset, demo_len - 1)
#                     future_exo_feats.append(torch.from_numpy(demo_grp['teacher_exo'][target_idx]).float())
#                 future_exo_target = torch.stack(future_exo_feats)
                
#                 # è¯­ä¹‰è¾…åŠ© (å–å½“å‰çª—å£çš„å¹³å‡ï¼Œæˆ–è€…ç›´æ¥å–å½“å‰å¸§è¯­ä¹‰)
#                 # ä¸ºäº†ç®€å•ï¼Œå–å½“å‰ t çš„ SigLIP
#                 teacher_siglip = torch.from_numpy(demo_grp['teacher_siglip'][current_t]).float().unsqueeze(0).repeat(self.window_size, 1)
#             else:
#                 teacher_siglip = torch.zeros(self.window_size, 1152)
#                 future_exo_target = torch.zeros(len(self.future_offsets), 1152)
                
#             # Teacher Exo Legacy (ä¸ºäº†å…¼å®¹æ—§æ¥å£ï¼Œå…¨0å³å¯ï¼Œæˆ–è€…å–å½“å‰çš„)
#             teacher_exo_legacy = torch.zeros(self.window_size, 1152)

#         text_tokens = self.tokenizer(meta['instruction'], return_tensors="pt", padding="max_length", max_length=16, truncation=True).input_ids.squeeze(0)

#         if anchor_key in self.anchor_bank:
#             first_frame = self.anchor_bank[anchor_key] 
#             # æ³¨æ„ï¼šå¦‚æœæ˜¯ä» bank é‡Œå–å‡ºçš„ï¼Œç¡®ä¿ bank å­˜çš„æ—¶å€™ä¹Ÿå½’ä¸€åŒ–äº†ï¼Œæˆ–è€…åœ¨è¿™é‡Œå½’ä¸€åŒ–
#             # å¦‚æœ bank é‡Œå­˜çš„æ˜¯ raw tensorï¼Œè¿™é‡Œè¦åŠ :
#             # first_frame = self.normalize(first_frame) 
#         else:
#             # å¦‚æœæ˜¯ç°åœºè¯»å–
#             m0 = torch.tensor(demo_grp['obs'][main_key][0]).float().permute(2, 0, 1) / 255.0
#             w0 = torch.tensor(demo_grp['obs'][wrist_key][0]).float().permute(2, 0, 1) / 255.0
            
#             # ä¹Ÿè¦å½’ä¸€åŒ–
#             m0 = self.normalize(m0)
#             w0 = self.normalize(w0)
#             first_frame = torch.stack([m0, w0], dim=0)


#         return {
#             "video": video,                 # [2, 3, 6, H, W] (Uniform Sampled)
#             "state": state_input_expanded,  # [6, 8] (Current State repeated)
#             "action_target": action_target, # [64, 8]
#             "text_tokens": text_tokens,
#             "first_frame": first_frame,
#             "teacher_siglip": teacher_siglip,
#             "teacher_exo": teacher_exo_legacy,
#             "future_exo_target": future_exo_target # [6, 1152] (Sparse Future)
#         }

# utils/dataset_loader.py
import torch
from torch.utils.data import Dataset
import h5py
import numpy as np
import os
import json
from transformers import T5Tokenizer
from torchvision import transforms
from tqdm import tqdm

class RobotDataset(Dataset):
    def __init__(self, hdf5_path, in_memory=True, 
                 window_size=6,         # å®é™…è¾“å…¥ç»™æ¨¡å‹çš„å¸§æ•°
                 history_len=500,       # æ¨¡æ‹Ÿçš„å†å²è§†é‡é•¿åº¦ (ä»ä¸­é‡‡æ · window_size å¸§)
                 pred_horizon=64,
                 tokenizer_path="/yanghaochuan/models/flan-t5-large",
                 stats_path="/yanghaochuan/data/130dataset_stats.json"): 
        
        self.hdf5_path = hdf5_path
        self.window_size = window_size
        self.history_len = history_len
        self.pred_horizon = pred_horizon
        self.in_memory = in_memory
        
        # ğŸŸ¢ å®šä¹‰ç¨€ç–é¢„æµ‹æ­¥é•¿ (World Model Anchors)
        self.future_offsets = [0, 2, 4, 8, 16, 32]

        # === å®šä¹‰å½’ä¸€åŒ– (VideoMAE æ ‡å‡†) ===
        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                              std=[0.229, 0.224, 0.225])

        # === 1. åŠ è½½ Tokenizer ===
        print(f"[Dataset] Loading Tokenizer from {tokenizer_path}...")
        try:
            self.tokenizer = T5Tokenizer.from_pretrained(tokenizer_path, local_files_only=True)
        except:
            print("[Dataset] Local tokenizer failed, trying default...")
            self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
        # === 2. åŠ è½½ç»Ÿè®¡é‡ ===
        if not os.path.exists(stats_path):
             raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {stats_path}")
        
        with open(stats_path, 'r') as f:
            stats = json.load(f)
        
        self.action_mean = torch.tensor(stats['action_mean']).float()
        self.action_std = torch.tensor(stats['action_std']).float()
        self.action_std = torch.maximum(self.action_std, torch.tensor(1e-2))
        
        # === 3. æ‰«ææ•°æ®ç»“æ„ & Anchor ===
        self.indices = []
        self.anchor_bank = {}
        self.cache = {} # å†…å­˜ç¼“å­˜

        print(f"[Dataset] Scanning HDF5 structure...")
        with h5py.File(hdf5_path, 'r') as f:
            if 'data' not in f: raise ValueError(f"HDF5ç»“æ„é”™è¯¯")
            self.demos = list(f['data'].keys())
            
            # --- 3.1 æ”¶é›† Anchors (Type B) ---
            for demo_key in self.demos:
                demo_grp = f['data'][demo_key]
                # å…¼å®¹æ—§æ•°æ®çš„ Type B åˆ¤å®š
                data_type = demo_grp.attrs.get("data_type", None)
                if data_type is None: 
                    idx = int(demo_key.split('_')[1])
                    if idx % 5 == 0: data_type = "type_b"
                
                if data_type == "type_b":
                    main_key = 'agentview_image' if 'agentview_image' in demo_grp['obs'] else 'agentview_rgb'
                    wrist_key = 'robot0_eye_in_hand_image'
                    if main_key in demo_grp['obs']:
                        m0 = torch.tensor(demo_grp['obs'][main_key][0]).float().permute(2, 0, 1) / 255.0
                        w0 = torch.tensor(demo_grp['obs'][wrist_key][0]).float().permute(2, 0, 1) / 255.0
                        # å­˜å…¥ Anchor Bank (æ³¨æ„ï¼šè¿™é‡Œæš‚æœªå½’ä¸€åŒ–ï¼Œä½¿ç”¨æ—¶å†å¤„ç†ï¼Œæˆ–è€…åœ¨æ­¤å¤„ç»Ÿä¸€)
                        # ä¸ºäº†ç»Ÿä¸€ï¼Œæˆ‘ä»¬åœ¨ __getitem__ å¤„ç»Ÿä¸€åš Normalize
                        self.anchor_bank[demo_key] = torch.stack([m0, w0], dim=0)

            # --- 3.2 æš´åŠ›é¢„åŠ è½½åˆ°å†…å­˜ (IO Boost) ---
            if self.in_memory:
                print(f"ğŸ“¥ [IO Boost] Loading ALL data to RAM (Total: {len(self.demos)} demos)...")
                for demo_key in tqdm(self.demos):
                    grp = f['data'][demo_key]
                    cache_item = {}
                    
                    # è‡ªåŠ¨è¯†åˆ« Main Camera Key
                    main_key = 'agentview_image' if 'agentview_image' in grp['obs'] else 'agentview_rgb'
                    wrist_key = 'robot0_eye_in_hand_image'

                    # è¯»å–å›¾åƒæ•°æ® (è€—æ—¶æ“ä½œ)
                    cache_item['main_img'] = grp['obs'][main_key][:]
                    cache_item['wrist_img'] = grp['obs'][wrist_key][:]
                    
                    # è¯»å–çŠ¶æ€ä¸åŠ¨ä½œ
                    cache_item['qpos'] = grp['obs']['robot0_joint_pos'][:]
                    # å¦‚æœæœ‰æ˜¾å¼çš„ actions dataset å°±è¯»ï¼Œæ²¡æœ‰åˆ™åç»­é€šè¿‡ qpos åˆ‡ç‰‡
                    if 'actions' in grp:
                        cache_item['actions'] = grp['actions'][:]
                    
                    # è¯»å– Teacher ç‰¹å¾
                    if 'teacher_siglip' in grp:
                        cache_item['teacher_siglip'] = grp['teacher_siglip'][:]
                    if 'teacher_exo' in grp:
                        cache_item['teacher_exo'] = grp['teacher_exo'][:]
                    
                    self.cache[demo_key] = cache_item
                print("âœ… Dataset successfully loaded to RAM!")

            # --- 3.3 æ„å»ºæ ·æœ¬ç´¢å¼• ---
            for demo_key in self.demos:
                # å¦‚æœåœ¨å†…å­˜é‡Œï¼Œç›´æ¥æŸ¥ç¼“å­˜ï¼›å¦åˆ™æŸ¥æ–‡ä»¶
                if self.in_memory:
                    # ä½¿ç”¨ç¼“å­˜ä¸­çš„é•¿åº¦ä¿¡æ¯
                    if 'actions' in self.cache[demo_key]:
                        total_len = self.cache[demo_key]['actions'].shape[0]
                    else:
                        total_len = self.cache[demo_key]['qpos'].shape[0] # Fallback
                else:
                    demo_grp = f['data'][demo_key]
                    if 'actions' not in demo_grp: continue
                    total_len = demo_grp['actions'].shape[0]

                # åªè¦å‰©ä½™é•¿åº¦å¤Ÿé¢„æµ‹æœªæ¥å³å¯
                if total_len > self.pred_horizon:
                    # è·å–æŒ‡ä»¤ (æŒ‡ä»¤é€šå¸¸å¾ˆçŸ­ï¼Œæš‚ä¸ç¼“å­˜ï¼Œæ¯æ¬¡è¯»å–å¼€é”€å¯å¿½ç•¥ï¼Œæˆ–è€…å­˜å…¥ meta)
                    # ä¸ºç®€å•èµ·è§ï¼Œè¿™é‡Œè¿˜æ˜¯æ¯æ¬¡è¯» attributesï¼Œæˆ–è€…å¦‚æœä½ æƒ³æè‡´ä¼˜åŒ–ï¼Œä¹Ÿå¯ä»¥ç¼“å­˜æŒ‡ä»¤
                    if self.in_memory:
                        # HDF5 attributes æ— æ³•ç¦»çº¿ç¼“å­˜ï¼Œè¿™é‡Œæˆ‘ä»¬å·ä¸ªæ‡’ï¼Œåªç¼“å­˜æ•°æ®
                        # å®é™…è¿è¡Œæ—¶ Instruction è¯»å–å¾ˆå¿«
                        pass 
                    
                    # ä¸ºäº†è·å– instructionï¼Œå¦‚æœæ˜¯ disk mode å¿…é¡»è¯» file
                    # å¦‚æœæ˜¯ memory modeï¼Œæˆ‘ä»¬è¿™é‡Œä¾ç„¶éœ€è¦ file handle æ¥è¯» attrs
                    # ä½† dataset scan åªè·‘ä¸€æ¬¡ï¼Œæ‰€ä»¥æ²¡å…³ç³»
                    demo_grp = f['data'][demo_key]
                    instr = demo_grp.attrs.get('language_instruction', 'do nothing')
                    if isinstance(instr, bytes): instr = instr.decode('utf-8')
                    
                    has_teacher = False
                    if self.in_memory:
                        has_teacher = 'teacher_siglip' in self.cache[demo_key]
                    else:
                        has_teacher = 'teacher_siglip' in demo_grp
                    
                    # # éå†æ¯ä¸€ä¸ªæ—¶é—´æ­¥
                    # for i in range(total_len - self.pred_horizon): 
                    #     self.indices.append({
                    #         'demo_key': demo_key, 
                    #         'current_t': i, 
                    #         'instruction': instr, 
                    #         'has_teacher': has_teacher
                    #     })
                    # 1. åˆ¤å®šå½“å‰ Demo æ˜¯å¦ä¸º Type B (åˆå§‹ä½ç½®å›ºå®šçš„æ•°æ®)
                    # å‡è®¾ä½ çš„å‘½åè§„åˆ™æ˜¯ demo_0, demo_5, demo_10... æ˜¯ Type B
                    curr_idx = int(demo_key.split('_')[1])
                    is_type_b = (curr_idx % 5 == 0) 

                    # 2. è®¾ç½®é‡å¤æ¬¡æ•°
                    # Type B (20æ¡) é‡å¤ 4 æ¬¡ -> ç­‰æ•ˆ 80 æ¡
                    # Type A (80æ¡) é‡å¤ 1 æ¬¡ -> ç­‰æ•ˆ 80 æ¡
                    # è¿™æ ·æ€»æ¯”ä¾‹æ¥è¿‘ 1:1
                    repeat_times = 4 if is_type_b else 1

                    for _ in range(repeat_times):  # <--- ğŸŸ¢ æ–°å¢å¾ªç¯ï¼šå®ç°è¿‡é‡‡æ ·
                        for i in range(total_len - self.pred_horizon): 
                            self.indices.append({
                                'demo_key': demo_key, 
                                'current_t': i, 
                                'instruction': instr, 
                                'has_teacher': has_teacher
                            })

                    print(f"[Dataset Loader] Demo {demo_key} (Type B={is_type_b}) loaded {repeat_times} times.")
                        
        print(f"[Dataset] Loaded {len(self.indices)} samples.")

    def __len__(self): return len(self.indices)

    def __getitem__(self, idx):
        meta = self.indices[idx]
        demo_key = meta['demo_key']
        current_t = meta['current_t']
        
        # å‡†å¤‡æ•°æ®å®¹å™¨
        main_frames = None
        wrist_frames = None
        state_raw = None
        teacher_siglip_tensor = None
        future_exo_target = None
        
        # ç¡®å®šå†å²çª—å£
        history_start = max(0, current_t - self.history_len + 1)
        history_end = current_t + 1
        valid_len = history_end - history_start
        
        # è®¡ç®—é‡‡æ ·ç´¢å¼•
        offsets = np.linspace(0, valid_len - 1, self.window_size).astype(int)
        global_indices = history_start + offsets
        global_indices = np.sort(global_indices)

        # =========================================================
        # ğŸŸ¢ åˆ†æ”¯ A: å†…å­˜æé€Ÿè¯»å– (In-Memory)
        # =========================================================
        if self.in_memory:
            demo_data = self.cache[demo_key]
            demo_len = demo_data['qpos'].shape[0] # ä½¿ç”¨ qpos é•¿åº¦ä½œä¸ºåŸºå‡†
            
            # Numpy æ”¯æŒé‡å¤ç´¢å¼•ï¼Œç›´æ¥è¯»å–å³å¯
            main_frames = demo_data['main_img'][global_indices]
            wrist_frames = demo_data['wrist_img'][global_indices]
            
            # State (ä» current_t å¼€å§‹)
            # RDT éœ€è¦: State(t) + Action(t...t+H)
            # è¿™é‡Œç»Ÿä¸€ä» qpos è¯»å–
            state_range_end = min(current_t + self.pred_horizon + 1, demo_len)
            state_raw = demo_data['qpos'][current_t : state_range_end]

            # Teacher
            if meta['has_teacher']:
                # SigLIP: å–å½“å‰å¸§
                teacher_siglip_tensor = torch.from_numpy(demo_data['teacher_siglip'][current_t]).float()
                
                # Future Exo: ç¨€ç–é‡‡æ ·
                feats = []
                for offset in self.future_offsets:
                    target_idx = min(current_t + offset, demo_len - 1)
                    feats.append(torch.from_numpy(demo_data['teacher_exo'][target_idx]).float())
                future_exo_target = torch.stack(feats)


        
        # =========================================================
        # ğŸ”µ åˆ†æ”¯ B: ç¡¬ç›˜è¯»å– (Disk - å…¼å®¹æ—§é€»è¾‘)
        # =========================================================
        else:
            with h5py.File(self.hdf5_path, 'r') as f:
                demo_grp = f['data'][demo_key]
                demo_len = demo_grp['actions'].shape[0]
                
                main_key = 'agentview_image' if 'agentview_image' in demo_grp['obs'] else 'agentview_rgb'
                wrist_key = 'robot0_eye_in_hand_image'
                
                # h5py ä¸æ”¯æŒé‡å¤ç´¢å¼•ï¼Œéœ€å»é‡
                unique_indices, inverse_indices = np.unique(global_indices, return_inverse=True)
                unique_main = demo_grp['obs'][main_key][unique_indices]
                unique_wrist = demo_grp['obs'][wrist_key][unique_indices]
                
                main_frames = unique_main[inverse_indices]
                wrist_frames = unique_wrist[inverse_indices]
                
                state_range_end = min(current_t + self.pred_horizon + 1, demo_len)
                state_raw = demo_grp['obs']['robot0_joint_pos'][current_t : state_range_end]
                
                if meta['has_teacher']:
                    teacher_siglip_tensor = torch.from_numpy(demo_grp['teacher_siglip'][current_t]).float()
                    feats = []
                    for offset in self.future_offsets:
                        target_idx = min(current_t + offset, demo_len - 1)
                        feats.append(torch.from_numpy(demo_grp['teacher_exo'][target_idx]).float())
                    future_exo_target = torch.stack(feats)

            print("ä»å†…å­˜è¯»å–å¤±è´¥ï¼Œæ”¹ä¸ºç¡¬ç›˜è¯»å–ã€‚")

        # =========================================================
        # ğŸŸ¡ å…¬å…±å¤„ç†é€»è¾‘ (Tensorè½¬æ¢ä¸å½’ä¸€åŒ–)
        # =========================================================
        
        # 1. è§†é¢‘å½’ä¸€åŒ–
        # [6, H, W, 3] -> [6, 3, H, W] -> Normalize
        main_t = torch.tensor(main_frames).float().permute(0, 3, 1, 2) / 255.0
        wrist_t = torch.tensor(wrist_frames).float().permute(0, 3, 1, 2) / 255.0
        
        main_t = self.normalize(main_t)
        wrist_t = self.normalize(wrist_t)
        
        # [2, 3, 6, H, W]
        video = torch.stack([main_t, wrist_t], dim=0).transpose(1, 2)

        # 2. çŠ¶æ€è¡¥é½ä¸å½’ä¸€åŒ–
        target_len = self.pred_horizon + 1
        if state_raw.shape[0] < target_len:
            pad_len = target_len - state_raw.shape[0]
            # ä½¿ç”¨æœ€åä¸€å¸§å¡«å……
            state_raw = np.concatenate([state_raw, np.tile(state_raw[-1:], (pad_len, 1))], axis=0)
            
        state_norm = (torch.tensor(state_raw).float() - self.action_mean) / self.action_std
        
        # åˆ†ç¦» Current State å’Œ Action Target
        # state_input_expanded: [6, 8] (å¤åˆ¶å½“å‰çŠ¶æ€ä»¥é€‚é…æ—§æ¥å£)
        state_input_expanded = state_norm[0].unsqueeze(0).repeat(self.window_size, 1)
        action_target = state_norm[1:] # [64, 8]

        # # 3. Anchor (First Frame)
        # curr_idx = int(demo_key.split('_')[1])
        # anchor_key = f"demo_{(curr_idx//5)*5}"
        
        # if anchor_key in self.anchor_bank:
        #     first_frame = self.anchor_bank[anchor_key]
        #     # ç¡®ä¿ Anchor ä¹Ÿè¢«å½’ä¸€åŒ– (å¦‚æœ Bank é‡Œå­˜çš„æ˜¯ Raw 0-1)
        #     # è¿™é‡Œçš„ anchor_bank åœ¨ init æ—¶å·²ç» /255.0 äº†ï¼Œä½†è¿˜æ²¡ Normalize
        #     # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨æ—¶åš Normalizeï¼Œæˆ–è€…ç¡®ä¿ init é‡Œä¸åš
        #     # æ ¹æ® init ä»£ç ï¼šself.anchor_bank å­˜çš„æ˜¯ /255.0 åçš„ã€‚
        #     # æ‰€ä»¥è¿™é‡Œåº”ç”¨ Normalize
        #     first_frame = torch.stack([
        #         self.normalize(first_frame[0]), 
        #         self.normalize(first_frame[1])
        #     ], dim=0)
        # else:
        #     # Fallback (ä½¿ç”¨å½“å‰åºåˆ—é¦–å¸§)
        #     if self.in_memory:
        #         # å†æ¬¡ä» Cache å–é¦–å¸§ (indices=0)
        #         m0 = torch.tensor(self.cache[demo_key]['main_img'][0]).float().permute(2, 0, 1) / 255.0
        #         w0 = torch.tensor(self.cache[demo_key]['wrist_img'][0]).float().permute(2, 0, 1) / 255.0
        #     else:
        #         # æç«¯æƒ…å†µçš„ fallbackï¼Œæš‚ä¸å¤„ç† h5py æ‰“å¼€ï¼Œç›´æ¥ç”¨å½“å‰ batch çš„ç¬¬ä¸€å¸§è¿‘ä¼¼
        #         m0 = main_t[0] # å·²ç»æ˜¯ Norm è¿‡çš„äº†
        #         w0 = wrist_t[0]
        #         # æ³¨æ„ï¼šm0 w0 å·²ç»æ˜¯ Normalized çš„äº†ï¼Œä¸éœ€è¦å†åš
        #         first_frame = torch.stack([m0, w0], dim=0)
        #         # è·³è¿‡ä¸‹é¢çš„ Normalize
            
        #     if 'first_frame' not in locals():
        #         first_frame = torch.stack([self.normalize(m0), self.normalize(w0)], dim=0)



        # âœ… æ”¹ä¸ºï¼šæ— è®ºæ˜¯ä¸æ˜¯ Type Aï¼Œéƒ½ç”¨è‡ªå·±çš„é¦–å¸§
        # (ä¿æŒä½ ç°æœ‰çš„ fallback é€»è¾‘ï¼Œå¹¶ç¡®ä¿å½’ä¸€åŒ–)
        if self.in_memory:
            m0_raw = torch.tensor(self.cache[demo_key]['main_img'][0]).float().permute(2, 0, 1) / 255.0
            w0_raw = torch.tensor(self.cache[demo_key]['wrist_img'][0]).float().permute(2, 0, 1) / 255.0
        else:
            # ... ä» h5py è¯»å–ç¬¬ 0 å¸§ ...
            pass

        first_frame = torch.stack([self.normalize(m0_raw), self.normalize(w0_raw)], dim=0)





        # 4. Teacher é»˜è®¤å€¼å¡«å……
        if teacher_siglip_tensor is None:
            teacher_siglip = torch.zeros(self.window_size, 1152)
            future_exo_target = torch.zeros(len(self.future_offsets), 1152)
        else:
            teacher_siglip = teacher_siglip_tensor.unsqueeze(0).repeat(self.window_size, 1)
            
        teacher_exo_legacy = torch.zeros(self.window_size, 1152)

        # 5. Tokenize Instruction
        text_tokens = self.tokenizer(meta['instruction'], return_tensors="pt", padding="max_length", max_length=16, truncation=True).input_ids.squeeze(0)

        return {
            "video": video,
            "state": state_input_expanded,
            "action_target": action_target,
            "text_tokens": text_tokens,
            "first_frame": first_frame,
            "teacher_siglip": teacher_siglip,
            "teacher_exo": teacher_exo_legacy,
            "future_exo_target": future_exo_target
        }