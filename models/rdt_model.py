# # models/rdt_model.py
# import torch
# import torch.nn as nn
# import os
# import sys
# import json
# import yaml
# import importlib.util
# import inspect
# import numbers

# # =========================================================================
# # 1. Âº∫ÂäõÂèÇÊï∞Ê∏ÖÊ¥óÂ∑•ÂÖ∑ & Ë°•‰∏Å
# # =========================================================================
# def force_to_int(val):
#     try:
#         if val is None: return None
#         if isinstance(val, (tuple, list)):
#             val = val[0] if len(val) >= 1 else 0
#         if hasattr(val, 'item'): val = val.item()
#         if hasattr(val, 'dtype'): val = int(val)
#         if isinstance(val, float): val = int(val)
#         if isinstance(val, int): return val
#         try: return int(val)
#         except: return val 
#     except:
#         return val

# OriginalLinearInit = torch.nn.Linear.__init__
# def patched_linear_init(self, in_features, out_features, bias=True, device=None, dtype=None):
#     safe_in = force_to_int(in_features)
#     safe_out = force_to_int(out_features)
#     if not isinstance(safe_out, int):
#         try: safe_out = int(safe_out.out_channels) 
#         except: safe_out = 8 
#     OriginalLinearInit(self, safe_in, safe_out, bias=bias, device=device, dtype=dtype)
# torch.nn.Linear.__init__ = patched_linear_init

# try:
#     import timm.models.layers
#     if hasattr(timm.models.layers, 'Mlp'):
#         OriginalTimmMlpInit = timm.models.layers.Mlp.__init__
#         def patched_timm_init(self, in_features, hidden_features=None, out_features=None, *args, **kwargs):
#             return OriginalTimmMlpInit(self, force_to_int(in_features), force_to_int(hidden_features), force_to_int(out_features), *args, **kwargs)
#         timm.models.layers.Mlp.__init__ = patched_timm_init
# except: pass

# try:
#     import timm.layers
#     if hasattr(timm.layers, 'Mlp'):
#         OriginalLayerMlpInit = timm.layers.Mlp.__init__
#         def patched_layer_init(self, in_features, hidden_features=None, out_features=None, *args, **kwargs):
#             return OriginalLayerMlpInit(self, force_to_int(in_features), force_to_int(hidden_features), force_to_int(out_features), *args, **kwargs)
#         timm.layers.Mlp.__init__ = patched_layer_init
# except: pass

# # =========================================================================
# # 2. Âä†ËΩΩ RDT Ê∫êÁ†Å
# # =========================================================================
# RDT_ROOT = "/yanghaochuan/projects/RoboticsDiffusionTransformer"
# RDT_MODELS_DIR = os.path.join(RDT_ROOT, "models")

# if RDT_ROOT not in sys.path: sys.path.insert(0, RDT_ROOT)
# if RDT_MODELS_DIR not in sys.path: sys.path.insert(0, RDT_MODELS_DIR)
# if "models" in sys.modules and RDT_MODELS_DIR not in sys.modules["models"].__path__:
#     sys.modules["models"].__path__.append(RDT_MODELS_DIR)

# TARGET_FILE_PATH = os.path.join(RDT_ROOT, "models", "rdt", "model.py")
# ModelClass = None
# if os.path.exists(TARGET_FILE_PATH):
#     try:
#         spec = importlib.util.spec_from_file_location("rdt_source_model", TARGET_FILE_PATH)
#         rdt_module = importlib.util.module_from_spec(spec)
#         sys.modules["rdt_source_model"] = rdt_module
#         spec.loader.exec_module(rdt_module)
        
#         candidate_classes = []
#         for name, obj in inspect.getmembers(rdt_module):
#             if inspect.isclass(obj) and issubclass(obj, nn.Module):
#                 if any(k in name for k in ["Transformer", "RDT", "Model"]):
#                     if not any(k in name for k in ["Layer", "Block", "Attention", "Embed", "Head", "MLP", "Timestep"]):
#                         candidate_classes.append(obj)
#         if candidate_classes:
#             candidate_classes.sort(key=lambda x: len(x.__name__), reverse=True)
#             ModelClass = candidate_classes[0]
#             print(f"[RDTWrapper] ‚úÖ ÊàêÂäüÈîÅÂÆöÊ®°ÂûãÁ±ª: {ModelClass.__name__}")
#         else:
#             print(f"[RDTWrapper] ‚ùå Êú™ÊâæÂà∞‰∏ªÊ®°ÂûãÁ±ª")
#     except Exception as e:
#         print(f"[RDTWrapper] ‚ùå ÂØºÂÖ• model.py Â§±Ë¥•: {e}")

# # =========================================================================
# # 3. RDTWrapper Á±ªÂÆö‰πâ
# # =========================================================================
# class RDTWrapper(nn.Module):
#     def __init__(self, 
#                  action_dim=8, 
#                  model_path='/yanghaochuan/models/rdt-1b',
#                  rdt_cond_dim=1152,
#                  pred_horizon=16):
#         super().__init__()
#         if ModelClass is None: raise RuntimeError("Êó†Ê≥ïÂàùÂßãÂåñ RDT")

#         # 1. Config
#         config_path = os.path.join(model_path, "config.json")
#         if not os.path.exists(config_path): config_path = os.path.join(model_path, "config.yaml")
#         print(f"[RDTWrapper] Loading config from: {config_path}")
        
#         self.rdt_hidden_size = 2048 
#         # Ë∞ÉÁî®ÂÜÖÈÉ®ÊñπÊ≥ïÂä†ËΩΩÈÖçÁΩÆ
#         args = self._load_config_and_override(config_path, action_dim)

#         # 2. Instantiate
#         print(f"[RDTWrapper] Instantiating with forced horizon={args.horizon}")
#         try:
#             sig = inspect.signature(ModelClass.__init__)
#             params = list(sig.parameters.keys())
#             if 'output_dim' not in vars(args): args.output_dim = args.action_dim
#             valid_args = {k: v for k, v in vars(args).items() if k in params or 'kwargs' in str(sig)}
#             self.rdt_model = ModelClass(**valid_args)
#             print("[RDTWrapper] Instantiation successful via kwargs unpacking.")
#         except Exception as e:
#             print(f"[RDTWrapper] Kwargs instantiation failed: {e}. Falling back to object pass...")
#             self.rdt_model = ModelClass(args)

#         # 3. Detect ACTUAL Hidden Size
#         actual_dim = self.rdt_hidden_size
#         if hasattr(self.rdt_model, 'hidden_size'): actual_dim = self.rdt_model.hidden_size
#         elif hasattr(self.rdt_model, 'embed_dim'): actual_dim = self.rdt_model.embed_dim
#         else:
#             for m in self.rdt_model.modules():
#                 if isinstance(m, nn.Linear):
#                     actual_dim = m.out_features
#                     break
#         print(f"[RDTWrapper] üîç Detected Actual Hidden Dimension: {actual_dim}")
        
#         # 4. Load Weights (Smart Loading with Adaptation)
#         weights_path = os.path.join(model_path, "pytorch_model.bin")
#         if not os.path.exists(weights_path): weights_path = os.path.join(model_path, "diffusion_pytorch_model.bin")
        
#         if os.path.exists(weights_path):
#             print(f"[RDTWrapper] Loading weights with schema adaptation...")
#             try:
#                 state_dict = torch.load(weights_path, map_location="cpu", weights_only=False)
#             except TypeError:
#                 state_dict = torch.load(weights_path, map_location="cpu")
            
#             new_state_dict = {}
#             current_model_dict = self.rdt_model.state_dict()
            
#             for k, v in state_dict.items():
#                 if k.startswith("module."): k = k[7:]
#                 if k in current_model_dict:
#                     target_shape = current_model_dict[k].shape
                    
#                     # === ÈÄÇÈÖç 1: x_pos_embed (3 tokens vs 4 tokens) ===
#                     # Âú∫ÊôØÔºöÂÆòÊñπÊùÉÈáçÊúâ State Token (len 4)Ôºå‰Ω†ÁöÑÊ®°ÂûãÊ≤°Êúâ (len 3)
#                     if "x_pos_embed" in k:
#                         if v.shape[1] == 4 and target_shape[1] == 3:
#                             print(f"[RDTWrapper] ‚úÇÔ∏è  Slicing x_pos_embed: Removing 'state' token (index 2).")
#                             # ÂÆòÊñπÈ°∫Â∫è: [Time, Freq, State, Action] -> ‰øùÁïô [0, 1, 3]
#                             v = v[:, [0, 1, 3], :]
                    
#                     # === ÈÄÇÈÖç 2: img_cond_pos_embed (4000+ vs 2) ===
#                     # Âú∫ÊôØÔºöÂÆòÊñπÊùÉÈáçÂ∑®Â§ßÔºåÊàë‰ª¨Âè™ÈúÄË¶Å 2 ‰∏™Âç†‰ΩçÁ¨¶
#                     if "img_cond_pos_embed" in k:
#                         if v.shape[1] > target_shape[1]:
#                             # Áõ¥Êé•Êà™ÂèñÂâç N ‰∏™ÔºåÂèçÊ≠£Êàë‰ª¨‰º†ÁöÑÊòØÂÖ® 0 Âç†‰ΩçÁ¨¶
#                             v = v[:, :target_shape[1], :]

#                     if v.shape != target_shape:
#                         # ÂÖúÂ∫ïÔºöÂ¶ÇÊûúÂΩ¢Áä∂Ëøò‰∏çÂåπÈÖçÔºåË∑≥ËøáÔºàÈò≤Ê≠¢Êä•ÈîôÔºâ
#                         print(f"[RDTWrapper] ‚ö†Ô∏è  Skipping {k}: shape mismatch {v.shape} vs {target_shape}")
#                         continue
                        
#                     new_state_dict[k] = v
            
#             self.rdt_model.load_state_dict(new_state_dict, strict=False)

#         # 5. Initialize Projection Layers
#         target_dim = actual_dim 
#         self.action_proj = nn.Linear(int(action_dim), int(target_dim))
#         self.cond_proj = nn.Linear(int(rdt_cond_dim), int(target_dim))
#         self.state_proj = nn.Linear(8, int(target_dim))

#         # === ÈÄÇÈÖç 3: Âº∫Âà∂Ë∞ÉÊï¥Ê®°ÂûãÂÜÖÈÉ® img_pos_embed Â§ßÂ∞è ===
#         DUBBY_IMG_LEN = 2
#         if hasattr(self.rdt_model, 'img_cond_pos_embed'):
#              if self.rdt_model.img_cond_pos_embed.shape[1] > DUBBY_IMG_LEN:
#                  print(f"[RDTWrapper] üìâ Resizing internal img_cond_pos_embed to length {DUBBY_IMG_LEN}")
#                  old_pe = self.rdt_model.img_cond_pos_embed.data
#                  new_pe = nn.Parameter(old_pe[:, :DUBBY_IMG_LEN, :].clone())
#                  self.rdt_model.img_cond_pos_embed = new_pe

#         target_dim = 1152 
#         # ‰∏∫‰∫ÜÊõ¥ÈÄöÁî®ÔºåÂª∫ËÆÆËé∑Âèñ model ÁöÑ hidden_sizeÔºåÊàñËÄÖÁõ¥Êé•Á°¨ÁºñÁ†Å‰Ω†Á°ÆÂÆöÁöÑÂÄº
#         if hasattr(self.rdt_model, 'config') and hasattr(self.rdt_model.config, 'hidden_size'):
#              target_dim = self.rdt_model.config.hidden_size
             
#         # ËøôÈáåËæìÂÖ•Áª¥Â∫¶ÂøÖÈ°ªÂØπÂ∫î FusionEncoder ÁöÑËæìÂá∫ (768)
#         self.visual_proj = nn.Linear(768, target_dim)


#     # =================================================
#     # ËæÖÂä©ÊñπÊ≥ïÔºöÊ≥®ÊÑèÁº©ËøõÔºåÂÆÉÂøÖÈ°ªÂú® class RDTWrapper ÂÜÖÈÉ®
#     # =================================================
#     def _load_config_and_override(self, config_path, target_action_dim):
#         class Args: pass
#         args = Args()
        
#         if config_path and os.path.exists(config_path):
#             with open(config_path, 'r') as f:
#                 cfg = json.load(f)
            
#             if 'rdt' in cfg and 'hidden_size' in cfg['rdt']:
#                 self.rdt_hidden_size = int(cfg['rdt']['hidden_size'])
#             elif 'hidden_size' in cfg:
#                 self.rdt_hidden_size = int(cfg['hidden_size'])
                
#             for k, v in cfg.items():
#                 if k == 'rdt' and isinstance(v, dict):
#                     for sub_k, sub_v in v.items():
#                         setattr(args, sub_k, sub_v)
#                 setattr(args, k, v)
                
#         args.action_dim = int(target_action_dim)
#         args.output_dim = int(target_action_dim)
#         args.out_channels = int(target_action_dim)
#         args.input_size = int(target_action_dim)
#         args.in_channels = int(target_action_dim)
        
#         args.hidden_size = self.rdt_hidden_size
#         args.embed_dim = self.rdt_hidden_size 
#         args.d_model = self.rdt_hidden_size
        
#         args.horizon = int(pred_horizon)      # È¢ÑÊµãÊú™Êù•Â§öÂ∞ëÊ≠•
#         args.pred_horizon = int(pred_horizon)
        
#         defaults = {'patch_size': 1, 'img_size': 1, 'num_frames': 1}
#         for k, v in defaults.items():
#             if not hasattr(args, k): setattr(args, k, v)
            
#         return args

#     # def forward(self, noisy_action, timestep, conditions):
#     #     e_t = conditions['e_t']
#     #     cond_embeds = self.cond_proj(e_t).unsqueeze(1) # [B, 1, D]
        
#     #     if noisy_action.dim() == 2: x_in = noisy_action
#     #     else: x_in = noisy_action.squeeze(1)
            
#     #     x_embed = self.action_proj(x_in).unsqueeze(1) # [B, 1, D]
        
#     #     B = x_embed.shape[0]
#     #     device = x_embed.device
        
#     #     freq = torch.full((B,), 30, device=device, dtype=torch.long)
        
#     #     lang_c = cond_embeds 
        
#     #     # ÂõæÂÉèÊù°‰ª∂ÔºöÂÖ® 0 Âç†‰ΩçÁ¨¶ (ÈïøÂ∫¶‰∏∫ 2)
#     #     img_c = torch.zeros((B, 2, cond_embeds.shape[-1]), device=device, dtype=cond_embeds.dtype)
        
#     #     lang_mask = torch.ones((B, 1), device=device, dtype=torch.bool)
#     #     img_mask = torch.ones((B, 2), device=device, dtype=torch.bool)

#     #     return self.rdt_model(
#     #         x=x_embed, 
#     #         freq=freq, 
#     #         t=timestep, 
#     #         lang_c=lang_c, 
#     #         img_c=img_c,
#     #         lang_mask=lang_mask,
#     #         img_mask=img_mask
#     #     )


#     def forward(self, noisy_action, timestep, conditions):
#         """
#         RDTWrapper ÁöÑÂâçÂêë‰º†Êí≠ (‰øÆÊîπÁâà)
#         ËæìÂÖ•:
#             noisy_action: [B, Horizon, Action_Dim] (‰æãÂ¶Ç [B, 16, 8])
#             timestep: [B]
#             conditions: dict, ÂåÖÂê´ 'e_t' (Â∫èÂàóÁâπÂæÅ)
#         """
#         # 1. Ëé∑ÂèñËßÜËßâÂ∫èÂàóÁâπÂæÅ
#         e_t = conditions['e_t'] # ÊúüÊúõÂΩ¢Áä∂: [B, 64, 768] (Êù•Ëá™ FusionEncoder ÁöÑÂ∫èÂàóËæìÂá∫)
        
#         B = e_t.shape[0]
#         device = e_t.device
#         dtype = e_t.dtype
        
#         # 2. ËßÜËßâÊäïÂΩ± (768 -> RDT Hidden Size, ÈÄöÂ∏∏ÊòØ 1152)
#         # [B, 64, 768] -> [B, 64, 1152]
#         img_c = self.visual_proj(e_t)
        
#         # 3. ÊûÑÈÄ†ÊñáÊú¨Êù°‰ª∂ (Language Condition)
#         # Âõ†‰∏∫Êàë‰ª¨‰∏ªË¶Å‰æùËµñËßÜËßâÔºåËøôÈáå‰º†ÂÖ•Á©∫ÁöÑÊñáÊú¨ÂµåÂÖ• (Zero Padding)
#         # RDT ÊúüÊúõ lang_c ÂΩ¢Áä∂‰∏∫ [B, L, D]ÔºåËøôÈáåËÆæ L=1
#         lang_c = torch.zeros((B, 1, 1152), device=device, dtype=dtype)
        
#         # 4. ÊûÑÈÄ† Masks
#         # img_mask: [B, 64], ÂÖ® 1 (ÊâÄÊúâËßÜËßâ Token ÊúâÊïà)
#         img_mask = torch.ones((B, img_c.shape[1]), device=device, dtype=torch.long)
        
#         # lang_mask: [B, 1], ÂÖ® 1 (Ë°®Á§∫ËøôÊòØ‰∏Ä‰∏™ÊúâÊïàÁöÑ"Á©∫Êåá‰ª§")
#         # Ê≥®ÊÑè: RDT ÂÜÖÈÉ®ÂØπ mask ÁöÑÂ§ÑÁêÜÈÄöÂ∏∏ÊòØ bool Êàñ 0/1, long ÊØîËæÉÁ®≥Â¶•
#         lang_mask = torch.ones((B, 1), device=device, dtype=torch.long)

#         # 5. Â§ÑÁêÜÂä®‰ΩúËæìÂÖ•
#         # Â¶ÇÊûúËæìÂÖ•ÊòØ [B, 8]Ôºåunsqueeze Êàê [B, 1, 8]
#         # Â¶ÇÊûúÊòØ [B, 16, 8]Ôºå‰øùÊåÅ‰∏çÂèò
#         if noisy_action.dim() == 2:
#             x_in = noisy_action.unsqueeze(1)
#         else:
#             x_in = noisy_action
            
#         # ÊäïÂΩ±Âä®‰ΩúÁª¥Â∫¶ [B, H, 8] -> [B, H, Hidden]
#         x_embed = self.action_proj(x_in)
        
#         # 6. ÊéßÂà∂È¢ëÁéá (Control Frequency)
#         # Âõ∫ÂÆö‰∏∫ 30Hz Êàñ‰ªé dataset ÁªüËÆ°‰∏≠Ëé∑Âèñ
#         freq = torch.full((B,), 30, device=device, dtype=torch.long)

#         # 7. Ë∞ÉÁî® RDT Backbone
#         return self.rdt_model(
#             x=x_embed, 
#             freq=freq, 
#             t=timestep, 
#             lang_c=lang_c, 
#             img_c=img_c,
#             lang_mask=lang_mask,
#             img_mask=img_mask
#         )

# # models/rdt_model.py
# import torch
# import torch.nn as nn
# import os
# import sys
# import json
# import importlib.util
# import inspect
# import logging

# # ÈÖçÁΩÆÊó•Âøó
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger("[RDTWrapper]")

# # =========================================================================
# # 1. Âº∫ÂäõÂèÇÊï∞Ê∏ÖÊ¥óÂ∑•ÂÖ∑ & Ë°•‰∏Å
# # =========================================================================
# def force_to_int(val):
#     try:
#         if val is None: return None
#         if isinstance(val, (tuple, list)):
#             val = val[0] if len(val) >= 1 else 0
#         if hasattr(val, 'item'): val = val.item()
#         try: return int(val)
#         except: return val 
#     except:
#         return val

# OriginalLinearInit = torch.nn.Linear.__init__
# def patched_linear_init(self, in_features, out_features, bias=True, device=None, dtype=None):
#     safe_in = force_to_int(in_features)
#     safe_out = force_to_int(out_features)
#     if not isinstance(safe_out, int):
#         try: safe_out = int(safe_out.out_channels) 
#         except: safe_out = 8 
#     OriginalLinearInit(self, safe_in, safe_out, bias=bias, device=device, dtype=dtype)
# torch.nn.Linear.__init__ = patched_linear_init

# # =========================================================================
# # 2. Âä†ËΩΩ RDT Ê∫êÁ†Å
# # =========================================================================
# RDT_ROOT = "/yanghaochuan/projects/RoboticsDiffusionTransformer"
# RDT_MODELS_DIR = os.path.join(RDT_ROOT, "models")

# if RDT_ROOT not in sys.path: sys.path.insert(0, RDT_ROOT)
# if RDT_MODELS_DIR not in sys.path: sys.path.insert(0, RDT_MODELS_DIR)
# if "models" in sys.modules and RDT_MODELS_DIR not in sys.modules["models"].__path__:
#     sys.modules["models"].__path__.append(RDT_MODELS_DIR)

# TARGET_FILE_PATH = os.path.join(RDT_ROOT, "models", "rdt", "model.py")
# ModelClass = None
# if os.path.exists(TARGET_FILE_PATH):
#     try:
#         spec = importlib.util.spec_from_file_location("rdt_source_model", TARGET_FILE_PATH)
#         rdt_module = importlib.util.module_from_spec(spec)
#         sys.modules["rdt_source_model"] = rdt_module
#         spec.loader.exec_module(rdt_module)
        
#         # Â∞ùËØïÊâæÂà∞ RDT Á±ª
#         candidate_classes = []
#         for name, obj in inspect.getmembers(rdt_module):
#             if inspect.isclass(obj) and issubclass(obj, nn.Module):
#                 if any(k in name for k in ["Transformer", "RDT", "Model"]):
#                     if not any(k in name for k in ["Layer", "Block", "Attention", "Embed", "Head", "MLP", "Timestep"]):
#                         candidate_classes.append(obj)
#         if candidate_classes:
#             candidate_classes.sort(key=lambda x: len(x.__name__), reverse=True)
#             ModelClass = candidate_classes[0]
#             logger.info(f"‚úÖ ÊàêÂäüÈîÅÂÆöÊ®°ÂûãÁ±ª: {ModelClass.__name__}")
#         else:
#             logger.error("‚ùå Êú™ÊâæÂà∞‰∏ªÊ®°ÂûãÁ±ª")
#     except Exception as e:
#         logger.error(f"‚ùå ÂØºÂÖ• model.py Â§±Ë¥•: {e}")

# # =========================================================================
# # 3. RDTWrapper Á±ªÂÆö‰πâ (ÊúÄÁªà‰øÆÊ≠£Áâà)
# # =========================================================================
# class RDTWrapper(nn.Module):
#     def __init__(self, 
#                  action_dim=8, 
#                  model_path='/yanghaochuan/models/rdt-1b',
#                  rdt_cond_dim=1152,
#                  pred_horizon=16):
#         super().__init__()
#         if ModelClass is None: raise RuntimeError("Êó†Ê≥ïÂàùÂßãÂåñ RDT")

#         # 1. Config
#         config_path = os.path.join(model_path, "config.json")
#         if not os.path.exists(config_path): config_path = os.path.join(model_path, "config.yaml")
#         logger.info(f"Loading config from: {config_path}")
        
#         self.rdt_hidden_size = 768 
#         args = self._load_config_and_override(config_path, action_dim, pred_horizon)

#         # 2. Instantiate Base Model
#         # Ê≥®ÊÑèÔºöËøôÈáåÂàùÂßãÂåñÁöÑÊ®°Âûã‰ºöÊúâÈªòËÆ§ÁöÑ pos_embed (ÂèØËÉΩÊòØ 34 ÊàñÂÖ∂‰ªñ)
#         try:
#             self.rdt_model = ModelClass(args)
#         except:
#             self.rdt_model = ModelClass(**vars(args))

#         # 3. Detect ACTUAL Hidden Size
#         actual_dim = self.rdt_hidden_size
#         if hasattr(self.rdt_model, 'hidden_size'): actual_dim = self.rdt_model.hidden_size
#         elif hasattr(self.rdt_model, 'embed_dim'): actual_dim = self.rdt_model.embed_dim
#         else:
#             for m in self.rdt_model.modules():
#                 if isinstance(m, nn.Linear):
#                     actual_dim = m.out_features
#                     break
#         logger.info(f"üîç Detected Actual Hidden Dimension: {actual_dim}")
        
#         # 4. Initialize Projection Layers (New Modalities)
#         # ÂøÖÈ°ªÂ∞ΩÊó©ÂÆö‰πâÔºåÁ°Æ‰øùÂú® load_state_dict ‰πãÂâçÂ≠òÂú®ÔºåÊàñËÄÖÊòØ LoRA ‰πãÂêé
#         # ‰ΩÜ‰∏∫‰∫ÜÂä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÔºåÊàë‰ª¨‰∏ªË¶ÅÂÖ≥Ê≥® RDT ÂÜÖÈÉ®
#         fusion_out_dim = 768
#         state_dim = 8 
#         self.state_proj = nn.Linear(int(state_dim), int(actual_dim)) 
#         self.action_proj = nn.Linear(int(action_dim), int(actual_dim))
#         self.visual_proj = nn.Linear(int(fusion_out_dim), int(actual_dim))

#         # 5. Smart Weight Loading & Surgery (ÂÖ≥ÈîÆÊ≠•È™§)
#         weights_path = os.path.join(model_path, "pytorch_model.bin")
#         if not os.path.exists(weights_path): weights_path = os.path.join(model_path, "diffusion_pytorch_model.bin")
        
#         if os.path.exists(weights_path):
#             logger.info(f"Loading weights from {weights_path}...")
#             try: state_dict = torch.load(weights_path, map_location="cpu")
#             except: state_dict = torch.load(weights_path, map_location="cpu", weights_only=False)
            
#             # Ëé∑ÂèñÂΩìÂâçÊ®°ÂûãÁöÑ state_dict ‰ª•‰æøÂØπÊØî
#             current_model_dict = self.rdt_model.state_dict()
#             new_state_dict = {}
            
#             # --- [ÊâãÊúØÂå∫Ôºöx_pos_embed] ---
#             # ÁõÆÊ†á: [Time(1), Freq(1), State(1), Action(16)] ÂÖ± 19
#             # ÂéüÂßã: [Time(1), Freq(1), Action(32)] ÂÖ± 34 (ÂÅáËÆæÊó†State) ÊàñËÄÖÊòØ [Time(1), Freq(1), State(1), Action(32)] ÂÖ± 35
#             target_x_len = 1 + 1 + 1 + pred_horizon # 19
            
#             # ÊâæÂà∞ checkpoint ÈáåÁöÑ x_pos_embed
#             ckpt_x_embed = None
#             for k, v in state_dict.items():
#                 if "x_pos_embed" in k:
#                     ckpt_x_embed = v
#                     break
            
#             if ckpt_x_embed is not None:
#                 logger.info(f"ü©π Performing surgery on x_pos_embed. Ckpt shape: {ckpt_x_embed.shape}, Target: {target_x_len}")
                
#                 # ÊûÑÈÄ†Êñ∞ÁöÑ embedding
#                 # 1. Â§çÂà∂ Time (idx 0) Âíå Freq (idx 1)
#                 new_x_embed = [ckpt_x_embed[:, 0:1, :], ckpt_x_embed[:, 1:2, :]]
                
#                 # 2. Â§ÑÁêÜ State (idx 2)
#                 # ËøôÊòØ‰∏Ä‰∏™ÂÖ®Êñ∞ÁöÑ tokenÔºåÊàë‰ª¨‰∏çËÉΩÁõ¥Êé•Â§çÁî® Action 0 ÁöÑ‰ΩçÁΩÆÁºñÁ†Å
#                 # Á≠ñÁï•ÔºöÂàùÂßãÂåñ‰∏∫ 0 ÊàñÈöèÊú∫ÔºåÂπ∂ËÆæÁΩÆ‰∏∫ÂèØËÆ≠ÁªÉ
#                 # ‰∏∫‰∫Ü‰øùÊåÅÂàÜÂ∏É‰∏ÄËá¥ÔºåÊàë‰ª¨ÂèØ‰ª•Âèñ Time Âíå Freq ÁöÑÂùáÂÄº‰Ωú‰∏∫ÂàùÂßãÂÄºÔºåÊàñËÄÖÁõ¥Êé•Áî® 0 (ÂéüÂßã‰ª£Á†ÅÊòØÁî® 0 ÂàùÂßãÂåñÁöÑ)
#                 state_embed = torch.zeros_like(ckpt_x_embed[:, 0:1, :]) 
#                 new_x_embed.append(state_embed)
                
#                 # 3. Â§çÂà∂ Action (idx 3 ~ 18)
#                 # ÂéüÂßã Action ‰ªé idx 2 ÂºÄÂßã (ÂÅáËÆæÂéüÂßãÊó† State) Êàñ idx 3 (Â¶ÇÊûúÂéüÂßãÊúâ State)
#                 # Èâ¥‰∫é‰πãÂâçÊä•Èîô 34 (1+1+32)ÔºåÂéüÂßãÂ∫îËØ•Êó† StateÔºåAction ‰ªé idx 2 ÂºÄÂßã
#                 if ckpt_x_embed.shape[1] == 34: # ÂéüÂßãÊó† State
#                     # ÂèñÂéüÂßãÁöÑÂâç 16 ‰∏™ Action (idx 2 ~ 2+16)
#                     # ËøôÊ†∑ Action 0 ÂØπÂ∫î Action 0ÔºåÂØπÈΩêÊ≠£Á°ÆÔºÅ
#                     action_embeds = ckpt_x_embed[:, 2 : 2+pred_horizon, :]
#                     new_x_embed.append(action_embeds)
#                 else:
#                     # ÂÖúÂ∫ïÔºöÂ¶ÇÊûúÁª¥Â∫¶Â•áÊÄ™ÔºåÂ∞ùËØïÁõ¥Êé•ÂØπÈΩê
#                     logger.warning("Checkpoint dimensions unexpected, falling back to safe slicing for tail")
#                     start_idx = 2
#                     available_len = ckpt_x_embed.shape[1] - start_idx
#                     copylen = min(available_len, pred_horizon)
#                     action_embeds = ckpt_x_embed[:, start_idx : start_idx+copylen, :]
#                     new_x_embed.append(action_embeds)

#                 # ÊãºÊé•
#                 final_x_embed = torch.cat(new_x_embed, dim=1)
                
#                 # Â∞ÜÂ§ÑÁêÜÂ•ΩÁöÑ embedding Â°ûÂõû new_state_dictÔºåÂØπÂ∫îÁöÑ key Ë¶ÅÂíåÂΩìÂâçÊ®°Âûã‰∏ÄËá¥
#                 # Êàë‰ª¨ÈúÄË¶ÅÂú®Âä†ËΩΩÂÆåÂÖ∂‰ªñÊùÉÈáçÂêéÔºåÊâãÂä®ËµãÂÄºÁªô self.rdt_model.x_pos_embed
#                 # ÊâÄ‰ª•ËøôÈáåÂÖà‰∏çÊîæÂÖ• load_state_dictÔºåÊàñËÄÖÊîæÂÖ•‰ΩÜÁ°Æ‰øù shape ÂåπÈÖç
#                 # ‰∏∫‰∫ÜÁÆÄÂçïÔºåÊàë‰ª¨Âú® load_state_dict ‰πãÂêéÊâãÂä®ËµãÂÄº
            
#             # --- [Â∏∏ËßÑÂä†ËΩΩ] ---
#             for k, v in state_dict.items():
#                 k_clean = k.replace("module.", "")
#                 if k_clean in current_model_dict:
#                     # Ë∑≥Ëøá x_pos_embed (ÂêéÈù¢ÊâãÂä®Â§ÑÁêÜ)
#                     if "x_pos_embed" in k_clean: continue
                    
#                     # ÈÄÇÈÖç img_cond_pos_embed (4096 -> 64)
#                     if "img_cond_pos_embed" in k_clean:
#                          target_len = 64 # ËøôÈáåÁöÑ 64 ÊòØ FusionEncoder Ê±†ÂåñÂêéÁöÑÈïøÂ∫¶
#                          if v.shape[1] > target_len:
#                              # ÂõæÂÉèÊòØÁ©∫Èó¥/ËØçË¢ãÁâπÂæÅÔºåÁõ¥Êé•ÂàáÁâáÂΩ±ÂìçËæÉÂ∞è
#                              v = v[:, :target_len, :]
                    
#                     if v.shape == current_model_dict[k_clean].shape:
#                         new_state_dict[k_clean] = v
            
#             # Âä†ËΩΩÂåπÈÖçÁöÑÊùÉÈáç
#             self.rdt_model.load_state_dict(new_state_dict, strict=False)
            
#             # --- [Â∫îÁî®ÊâãÊúØÁªìÊûú] ---
#             if ckpt_x_embed is not None:
#                 # Á°Æ‰øù Parameter Á±ªÂûãÊ≠£Á°Æ
#                 self.rdt_model.x_pos_embed = nn.Parameter(final_x_embed)
#                 # ‚ö†Ô∏è ÂÖ≥ÈîÆÔºöÁ°Æ‰øùÂÆÉÊòØÂèØËÆ≠ÁªÉÁöÑÔºÅLoRA ÈÄöÂ∏∏Âè™ËÆ≠ÁªÉ LinearÔºå
#                 # ‰ΩÜÊàë‰ª¨ÈúÄË¶ÅËøô‰∏™ Embedding ÈÄÇÂ∫îÊñ∞ÁöÑ State Âíå Horizon
#                 self.rdt_model.x_pos_embed.requires_grad = True
#                 logger.info("‚úÖ x_pos_embed surgery complete & set to trainable.")

#         # 6. Double Check Img Embed
#         DUBBY_IMG_LEN = 64 
#         if hasattr(self.rdt_model, 'img_cond_pos_embed'):
#              if self.rdt_model.img_cond_pos_embed.shape[1] != DUBBY_IMG_LEN:
#                  logger.info(f"üìâ Resizing img_cond_pos_embed to {DUBBY_IMG_LEN}")
#                  old_pe = self.rdt_model.img_cond_pos_embed.data
#                  new_pe = nn.Parameter(old_pe[:, :DUBBY_IMG_LEN, :].clone())
#                  self.rdt_model.img_cond_pos_embed = new_pe

#     def _load_config_and_override(self, config_path, target_action_dim, pred_horizon):
#         class Args: pass
#         args = Args()
#         if config_path and os.path.exists(config_path):
#             with open(config_path, 'r') as f: cfg = json.load(f)
#             if 'rdt' in cfg and 'hidden_size' in cfg['rdt']: self.rdt_hidden_size = int(cfg['rdt']['hidden_size'])
#             elif 'hidden_size' in cfg: self.rdt_hidden_size = int(cfg['hidden_size'])
#             for k, v in cfg.items():
#                 if k == 'rdt' and isinstance(v, dict):
#                     for sk, sv in v.items(): setattr(args, sk, sv)
#                 setattr(args, k, v)
#         args.action_dim = int(target_action_dim)
#         args.output_dim = int(target_action_dim)
#         args.horizon = int(pred_horizon)
#         args.pred_horizon = int(pred_horizon)
#         if not hasattr(args, 'patch_size'): args.patch_size = 1
#         if not hasattr(args, 'img_size'): args.img_size = 1
#         if not hasattr(args, 'num_frames'): args.num_frames = 1
#         return args

#     def forward(self, noisy_action, timestep, conditions):
#         B = noisy_action.shape[0]
#         device = noisy_action.device
#         dtype = self.action_proj.weight.dtype

#         # 1. ËßÜËßâ
#         e_t = conditions['e_t'] 
#         img_c = self.visual_proj(e_t.to(dtype)) 

#         # 2. Áä∂ÊÄÅ
#         current_state = conditions.get('state') 
#         if current_state is None:
#              current_state = torch.zeros((B, 1, 8), device=device, dtype=dtype)
#         if current_state.dim() == 2: current_state = current_state.unsqueeze(1)
#         state_embed = self.state_proj(current_state.to(dtype)) 

#         # 3. Âä®‰Ωú
#         if noisy_action.dim() == 2: x_in = noisy_action.unsqueeze(1)
#         else: x_in = noisy_action
#         action_embed = self.action_proj(x_in.to(dtype))
        
#         # ÊãºÊé•: [Time(Áî±RDTÂÜÖÈÉ®Âä†), Freq(Áî±RDTÂÜÖÈÉ®Âä†), State, Action]
#         # Ê≥®ÊÑèÔºöRDT.forward ÂÜÖÈÉ®‰ºöËá™Â∑±Âú®ÊúÄÂâçÈù¢Âä†‰∏ä Time Âíå Freq token
#         # ÊâÄ‰ª•Êàë‰ª¨ÈúÄË¶Å‰º†ËøõÂéªÁöÑÊòØ [State, Action]
#         # x_pos_embed ÈïøÂ∫¶ÊòØ 19 (1+1+1+16)
#         # ÂÜÖÈÉ®ÈÄªËæëÊòØ: x = cat(t, freq, x) -> shape becomes 2 + (1+16) = 19
#         # ÁÑ∂Âêé x + x_pos_embed
        
#         x_input = torch.cat([state_embed, action_embed], dim=1) # [B, 1+16, D]

#         # 4. ÂÖ∂‰ªñ
#         lang_c = torch.zeros((B, 1, self.rdt_hidden_size), device=device, dtype=dtype)
#         img_mask = torch.ones((B, img_c.shape[1]), device=device, dtype=torch.long)
#         lang_mask = torch.ones((B, 1), device=device, dtype=torch.long)
#         freq = torch.full((B,), 30, device=device, dtype=torch.long)

#         return self.rdt_model(
#             x=x_input, 
#             freq=freq, 
#             t=timestep, 
#             lang_c=lang_c, 
#             img_c=img_c,
#             lang_mask=lang_mask,
#             img_mask=img_mask
#         )

import torch
import torch.nn as nn
import os
import sys
import json
import logging
import inspect
import importlib.util

# ÈÖçÁΩÆÊó•Âøó
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("[RDTWrapper]")

# =========================================================================
# 1. Âü∫Á°ÄË°•‰∏Å (‰øùÊåÅ‰∏çÂèò)
# =========================================================================
def force_to_int(val):
    try:
        if hasattr(val, 'item'): val = val.item()
        return int(val)
    except:
        return 0

OriginalLinearInit = torch.nn.Linear.__init__
def patched_linear_init(self, in_features, out_features, bias=True, device=None, dtype=None):
    OriginalLinearInit(self, force_to_int(in_features), force_to_int(out_features), bias=bias, device=device, dtype=dtype)
torch.nn.Linear.__init__ = patched_linear_init

# =========================================================================
# 2. Âä†ËΩΩ RDT Ê∫êÁ†Å
# =========================================================================
RDT_ROOT = "/yanghaochuan/projects/RoboticsDiffusionTransformer"
RDT_MODELS_DIR = os.path.join(RDT_ROOT, "models")
if RDT_ROOT not in sys.path: sys.path.insert(0, RDT_ROOT)
if RDT_MODELS_DIR not in sys.path: sys.path.insert(0, RDT_MODELS_DIR)
if "models" in sys.modules and RDT_MODELS_DIR not in sys.modules["models"].__path__:
    sys.modules["models"].__path__.append(RDT_MODELS_DIR)

TARGET_FILE_PATH = os.path.join(RDT_ROOT, "models", "rdt", "model.py")
ModelClass = None
if os.path.exists(TARGET_FILE_PATH):
    try:
        spec = importlib.util.spec_from_file_location("rdt_source_model", TARGET_FILE_PATH)
        rdt_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(rdt_module)
        for name, obj in inspect.getmembers(rdt_module):
            if inspect.isclass(obj) and issubclass(obj, nn.Module):
                if any(k in name for k in ["Transformer", "RDT", "Model"]) and "Layer" not in name:
                    ModelClass = obj
                    break
        if ModelClass: logger.info(f"‚úÖ Locked Model Class: {ModelClass.__name__}")
    except Exception as e:
        logger.error(f"‚ùå Load failed: {e}")

# =========================================================================
# 3. RDTWrapper (‰øÆÂ§ç Config ËØªÂèñ + Áª¥Â∫¶ÂØπÈΩê)
# =========================================================================
class RDTWrapper(nn.Module):
    def __init__(self, 
                 action_dim=8, 
                 model_path='/yanghaochuan/models/rdt-1b',
                 rdt_cond_dim=768,  # <--- ‰Ω†ÁöÑ FusionEncoder ËæìÂá∫ÊòØ 768
                 pred_horizon=16):
        super().__init__()
        if ModelClass is None: raise RuntimeError("RDT Class not found")

        # 1. Á≤æÁ°ÆËØªÂèñ Config
        config_path = os.path.join(model_path, "config.json")
        if not os.path.exists(config_path): config_path = os.path.join(model_path, "config.yaml")
        
        # ÊòæÂºèÊûÑÈÄ†ÂèÇÊï∞Â≠óÂÖ∏
        kwargs = self._parse_config_robust(config_path)
        
        # ‚ö†Ô∏è Âº∫Âà∂‰øÆÊ≠£ÔºöConfig Êñá‰ª∂ËôΩÁÑ∂ÊòØÂØπÁöÑÔºå‰ΩÜ‰∏∫‰∫ÜÈò≤Ê≠¢‰ª£Á†ÅËØªÈîôÔºåËøôÈáåÂº∫Âà∂ÂÜôÊ≠ª 2048
        kwargs['hidden_size'] = 2048
        kwargs['action_dim'] = int(action_dim)
        kwargs['output_dim'] = int(action_dim)
        kwargs['horizon'] = int(pred_horizon)
        kwargs['pred_horizon'] = int(pred_horizon)
        
        logger.info(f"üõ†Ô∏è  Forcing RDT Init: hidden_size=2048, action_dim={action_dim}")

        # 2. ÂÆû‰æãÂåñÊ®°Âûã
        try:
            self.rdt_model = ModelClass(**kwargs)
        except:
            # Â§áÁî®ÊñπÊ°àÔºö‰º†ÂØπË±°
            class Args: pass
            args = Args()
            for k, v in kwargs.items(): setattr(args, k, v)
            self.rdt_model = ModelClass(args)

        # 3. Áª¥Â∫¶Ê£ÄÊü•
        actual_dim = 0
        for m in self.rdt_model.modules():
            if isinstance(m, nn.Linear):
                actual_dim = m.out_features
                break
        
        if actual_dim != 2048:
            logger.error(f"‚ùå FATAL: Model initialized as {actual_dim}, expected 2048!")
            # Êö¥Âäõ‰øÆÊ≠£ÔºàËôΩÁÑ∂ÂæàÂ∞ëËßÅÈúÄË¶ÅËøôÊ†∑ÂÅöÔºâ
            self.rdt_model.hidden_size = 2048
        
        self.rdt_hidden_size = 2048

        # 4. Âª∫Á´ãÊäïÂΩ±Â±Ç (Project 768 -> 2048)
        # ËøôÊòØËß£ÂÜ≥‰Ω†ËÆ≠ÁªÉÂØºËá¥ÁöÑ 768 Áª¥ÈóÆÈ¢òÁöÑÂÖ≥ÈîÆ
        self.visual_proj = nn.Linear(int(rdt_cond_dim), 2048)
        self.action_proj = nn.Linear(int(action_dim), 2048)

        logger.info(f"üèóÔ∏è Projections: Visual(768->2048), Action({action_dim}->2048)")

        # 5. Âä†ËΩΩÊùÉÈáç + ÊâãÊúØ
        self._load_and_surgically_fix_weights(model_path, pred_horizon)

        # 6. ‰øÆÊ≠£ÂÜÖÈÉ®ÂèÇÊï∞
        if hasattr(self.rdt_model, 'img_cond_pos_embed'):
            # Áº©Â∞èÂÜÖÈÉ® img pos embed ‰ª•ÈÅøÂÖç‰∏çÂøÖË¶ÅÁöÑËÆ°ÁÆóÊàñÊä•Èîô
            if self.rdt_model.img_cond_pos_embed.shape[1] > 2:
                old = self.rdt_model.img_cond_pos_embed.data
                self.rdt_model.img_cond_pos_embed = nn.Parameter(old[:, :2, :].clone())

    def _parse_config_robust(self, path):
        """‰∏ìÈó®ÈíàÂØπ‰Ω†ÁöÑ config ÁªìÊûÑËøõË°åËß£Êûê"""
        with open(path, 'r') as f: cfg = json.load(f)
        kwargs = {}
        # 1. ÂÖàÊääÂ§ñÂ±ÇÂèÇÊï∞ÊãøËøõÊù•
        for k, v in cfg.items():
            if k != 'rdt': kwargs[k] = v
        
        # 2. ÈáçÁÇπËß£Êûê 'rdt' ÂÜÖÈÉ®ÂèÇÊï∞ÔºåÂπ∂Ë¶ÜÁõñÂ§ñÂ±Ç
        # ‰Ω†ÁöÑ config Èáå hidden_size Âú® rdt ‰∏ãÈù¢ÔºåÊâÄ‰ª•Ëøô‰∏ÄÊ≠•Ëá≥ÂÖ≥ÈáçË¶Å
        if 'rdt' in cfg and isinstance(cfg['rdt'], dict):
            for k, v in cfg['rdt'].items():
                kwargs[k] = v
        
        # 3. Ë°•ÂÖÖÈªòËÆ§ÂÄº
        kwargs.setdefault('patch_size', 14)
        kwargs.setdefault('img_size', 224)
        
        return kwargs

    def _load_and_surgically_fix_weights(self, model_path, pred_horizon):
        weights_path = os.path.join(model_path, "pytorch_model.bin")
        if not os.path.exists(weights_path): 
            weights_path = os.path.join(model_path, "diffusion_pytorch_model.bin")
        
        if not os.path.exists(weights_path): return

        logger.info("Loading weights...")
        state_dict = torch.load(weights_path, map_location="cpu")
        
        # 1. ËøáÊª§‰∏çÂåπÈÖçÁöÑÊùÉÈáç
        current_dict = self.rdt_model.state_dict()
        new_dict = {}
        ckpt_x_embed = None

        for k, v in state_dict.items():
            if "x_pos_embed" in k:
                ckpt_x_embed = v
                continue
            
            k_clean = k.replace("module.", "")
            if k_clean in current_dict:
                # ÂõæÂÉè‰ΩçÁΩÆÁºñÁ†ÅÊà™Êñ≠
                if "img_cond_pos_embed" in k_clean and v.shape[1] > 2:
                    v = v[:, :2, :]
                
                if v.shape == current_dict[k_clean].shape:
                    new_dict[k_clean] = v
        
        self.rdt_model.load_state_dict(new_dict, strict=False)

        # 2. ‰øÆÂ§ç x_pos_embed
        if ckpt_x_embed is not None:
            # Á°Æ‰øù embedding ‰πüÊòØ 2048 Áª¥
            if ckpt_x_embed.shape[-1] != 2048:
                logger.warning("Checkpoint dimensions weird, skipping x_pos_embed fix.")
                return

            parts = []
            parts.append(ckpt_x_embed[:, 0:1, :]) # Time
            parts.append(ckpt_x_embed[:, 1:2, :]) # Freq
            parts.append(torch.zeros(1, 1, 2048))  # State (New)
            
            # Actions
            start = 2
            avail = ckpt_x_embed.shape[1] - start
            take = min(avail, pred_horizon)
            parts.append(ckpt_x_embed[:, start : start+take, :])
            
            if take < pred_horizon:
                parts.append(torch.zeros(1, pred_horizon - take, 2048))
            
            final_embed = torch.cat(parts, dim=1)
            self.rdt_model.x_pos_embed = nn.Parameter(final_embed)
            logger.info("‚úÖ x_pos_embed fixed (Time+Freq+State+Actions).")

    def forward(self, noisy_action, timestep, conditions):
        B = noisy_action.shape[0]
        device = noisy_action.device
        dtype = self.action_proj.weight.dtype

        # 1. Visual (768 -> 2048)
        e_t = conditions['e_t'] 
        img_c = self.visual_proj(e_t.to(dtype))

        # 2. State (Dummy -> 2048)
        state_embed = torch.zeros((B, 1, 2048), device=device, dtype=dtype)

        # 3. Action (8 -> 2048)
        if noisy_action.dim() == 2: noisy_action = noisy_action.unsqueeze(1)
        action_embed = self.action_proj(noisy_action.to(dtype))

        # 4. Concat Input
        x_input = torch.cat([state_embed, action_embed], dim=1)

        # 5. Others
        lang_c = torch.zeros((B, 1, 2048), device=device, dtype=dtype)
        lang_mask = torch.ones((B, 1), device=device, dtype=torch.long)
        
        target_img_len = self.rdt_model.img_cond_pos_embed.shape[1]
        if img_c.shape[1] > target_img_len:
            img_c = img_c[:, :target_img_len, :]
        img_mask = torch.ones((B, img_c.shape[1]), device=device, dtype=torch.long)
        
        freq = torch.full((B,), 30, device=device, dtype=torch.long)

        # 6. Forward
        return self.rdt_model(
            x=x_input, freq=freq, t=timestep, 
            lang_c=lang_c, img_c=img_c, 
            lang_mask=lang_mask, img_mask=img_mask
        )

    def save_pretrained(self, save_directory):
        self.rdt_model.save_pretrained(save_directory)
        torch.save(self.visual_proj.state_dict(), os.path.join(save_directory, "visual_proj.bin"))
        torch.save(self.action_proj.state_dict(), os.path.join(save_directory, "action_proj.bin"))

    def load_pretrained_projections(self, save_directory):
        p_vis = os.path.join(save_directory, "visual_proj.bin")
        p_act = os.path.join(save_directory, "action_proj.bin")
        if os.path.exists(p_vis): self.visual_proj.load_state_dict(torch.load(p_vis))
        if os.path.exists(p_act): self.action_proj.load_state_dict(torch.load(p_act))