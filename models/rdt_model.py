# models/rdt_model.py
import torch
import torch.nn as nn
import os
import sys
import json
import yaml
import importlib.util
import inspect
import numbers

# =========================================================================
# 1. å¼ºåŠ›å‚æ•°æ¸…æ´—å·¥å…· & è¡¥ä¸
# =========================================================================
def force_to_int(val):
    try:
        if val is None: return None
        if isinstance(val, (tuple, list)):
            val = val[0] if len(val) >= 1 else 0
        if hasattr(val, 'item'): val = val.item()
        if hasattr(val, 'dtype'): val = int(val)
        if isinstance(val, float): val = int(val)
        if isinstance(val, int): return val
        try: return int(val)
        except: return val 
    except:
        return val

OriginalLinearInit = torch.nn.Linear.__init__
def patched_linear_init(self, in_features, out_features, bias=True, device=None, dtype=None):
    safe_in = force_to_int(in_features)
    safe_out = force_to_int(out_features)
    if not isinstance(safe_out, int):
        try: safe_out = int(safe_out.out_channels) 
        except: safe_out = 8 
    OriginalLinearInit(self, safe_in, safe_out, bias=bias, device=device, dtype=dtype)
torch.nn.Linear.__init__ = patched_linear_init

try:
    import timm.models.layers
    if hasattr(timm.models.layers, 'Mlp'):
        OriginalTimmMlpInit = timm.models.layers.Mlp.__init__
        def patched_timm_init(self, in_features, hidden_features=None, out_features=None, *args, **kwargs):
            return OriginalTimmMlpInit(self, force_to_int(in_features), force_to_int(hidden_features), force_to_int(out_features), *args, **kwargs)
        timm.models.layers.Mlp.__init__ = patched_timm_init
except: pass

try:
    import timm.layers
    if hasattr(timm.layers, 'Mlp'):
        OriginalLayerMlpInit = timm.layers.Mlp.__init__
        def patched_layer_init(self, in_features, hidden_features=None, out_features=None, *args, **kwargs):
            return OriginalLayerMlpInit(self, force_to_int(in_features), force_to_int(hidden_features), force_to_int(out_features), *args, **kwargs)
        timm.layers.Mlp.__init__ = patched_layer_init
except: pass

# =========================================================================
# 2. åŠ è½½ RDT æºç 
# =========================================================================
RDT_ROOT = "/yanghaochuan/projects/RoboticsDiffusionTransformer"
RDT_MODELS_DIR = os.path.join(RDT_ROOT, "models")

if RDT_ROOT not in sys.path: sys.path.insert(0, RDT_ROOT)
if RDT_MODELS_DIR not in sys.path: sys.path.insert(0, RDT_MODELS_DIR)
if "models" in sys.modules and RDT_MODELS_DIR not in sys.modules["models"].__path__:
    sys.modules["models"].__path__.append(RDT_MODELS_DIR)

TARGET_FILE_PATH = os.path.join(RDT_ROOT, "models", "rdt", "model.py")
ModelClass = None
if os.path.exists(TARGET_FILE_PATH):
    try:
        spec = importlib.util.spec_from_file_location("rdt_source_model", TARGET_FILE_PATH)
        rdt_module = importlib.util.module_from_spec(spec)
        sys.modules["rdt_source_model"] = rdt_module
        spec.loader.exec_module(rdt_module)
        
        candidate_classes = []
        for name, obj in inspect.getmembers(rdt_module):
            if inspect.isclass(obj) and issubclass(obj, nn.Module):
                if any(k in name for k in ["Transformer", "RDT", "Model"]):
                    if not any(k in name for k in ["Layer", "Block", "Attention", "Embed", "Head", "MLP", "Timestep"]):
                        candidate_classes.append(obj)
        if candidate_classes:
            candidate_classes.sort(key=lambda x: len(x.__name__), reverse=True)
            ModelClass = candidate_classes[0]
            print(f"[RDTWrapper] âœ… æˆåŠŸé”å®šæ¨¡å‹ç±»: {ModelClass.__name__}")
        else:
            print(f"[RDTWrapper] âŒ æœªæ‰¾åˆ°ä¸»æ¨¡å‹ç±»")
    except Exception as e:
        print(f"[RDTWrapper] âŒ å¯¼å…¥ model.py å¤±è´¥: {e}")

# =========================================================================
# 3. RDTWrapper ç±»å®šä¹‰
# =========================================================================
class RDTWrapper(nn.Module):
    def __init__(self, 
                 action_dim=8, 
                 model_path='/yanghaochuan/models/rdt-1b',
                 rdt_cond_dim=1152,
                 pred_horizon=16):
        super().__init__()
        if ModelClass is None: raise RuntimeError("æ— æ³•åˆå§‹åŒ– RDT")

        # 1. Config
        config_path = os.path.join(model_path, "config.json")
        if not os.path.exists(config_path): config_path = os.path.join(model_path, "config.yaml")
        print(f"[RDTWrapper] Loading config from: {config_path}")
        
        self.rdt_hidden_size = 2048 
        # è°ƒç”¨å†…éƒ¨æ–¹æ³•åŠ è½½é…ç½®
        args = self._load_config_and_override(config_path, action_dim)

        # 2. Instantiate
        print(f"[RDTWrapper] Instantiating with forced horizon={args.horizon}")
        try:
            sig = inspect.signature(ModelClass.__init__)
            params = list(sig.parameters.keys())
            if 'output_dim' not in vars(args): args.output_dim = args.action_dim
            valid_args = {k: v for k, v in vars(args).items() if k in params or 'kwargs' in str(sig)}
            self.rdt_model = ModelClass(**valid_args)
            print("[RDTWrapper] Instantiation successful via kwargs unpacking.")
        except Exception as e:
            print(f"[RDTWrapper] Kwargs instantiation failed: {e}. Falling back to object pass...")
            self.rdt_model = ModelClass(args)

        # 3. Detect ACTUAL Hidden Size
        actual_dim = self.rdt_hidden_size
        if hasattr(self.rdt_model, 'hidden_size'): actual_dim = self.rdt_model.hidden_size
        elif hasattr(self.rdt_model, 'embed_dim'): actual_dim = self.rdt_model.embed_dim
        else:
            for m in self.rdt_model.modules():
                if isinstance(m, nn.Linear):
                    actual_dim = m.out_features
                    break
        print(f"[RDTWrapper] ğŸ” Detected Actual Hidden Dimension: {actual_dim}")
        
        # 4. Load Weights (Smart Loading with Adaptation)
        weights_path = os.path.join(model_path, "pytorch_model.bin")
        if not os.path.exists(weights_path): weights_path = os.path.join(model_path, "diffusion_pytorch_model.bin")
        
        if os.path.exists(weights_path):
            print(f"[RDTWrapper] Loading weights with schema adaptation...")
            try:
                state_dict = torch.load(weights_path, map_location="cpu", weights_only=False)
            except TypeError:
                state_dict = torch.load(weights_path, map_location="cpu")
            
            new_state_dict = {}
            current_model_dict = self.rdt_model.state_dict()
            
            for k, v in state_dict.items():
                if k.startswith("module."): k = k[7:]
                if k in current_model_dict:
                    target_shape = current_model_dict[k].shape
                    
                    # === é€‚é… 1: x_pos_embed (3 tokens vs 4 tokens) ===
                    # åœºæ™¯ï¼šå®˜æ–¹æƒé‡æœ‰ State Token (len 4)ï¼Œä½ çš„æ¨¡å‹æ²¡æœ‰ (len 3)
                    if "x_pos_embed" in k:
                        if v.shape[1] == 4 and target_shape[1] == 3:
                            print(f"[RDTWrapper] âœ‚ï¸  Slicing x_pos_embed: Removing 'state' token (index 2).")
                            # å®˜æ–¹é¡ºåº: [Time, Freq, State, Action] -> ä¿ç•™ [0, 1, 3]
                            v = v[:, [0, 1, 3], :]
                    
                    # === é€‚é… 2: img_cond_pos_embed (4000+ vs 2) ===
                    # åœºæ™¯ï¼šå®˜æ–¹æƒé‡å·¨å¤§ï¼Œæˆ‘ä»¬åªéœ€è¦ 2 ä¸ªå ä½ç¬¦
                    if "img_cond_pos_embed" in k:
                        if v.shape[1] > target_shape[1]:
                            # ç›´æ¥æˆªå–å‰ N ä¸ªï¼Œåæ­£æˆ‘ä»¬ä¼ çš„æ˜¯å…¨ 0 å ä½ç¬¦
                            v = v[:, :target_shape[1], :]

                    if v.shape != target_shape:
                        # å…œåº•ï¼šå¦‚æœå½¢çŠ¶è¿˜ä¸åŒ¹é…ï¼Œè·³è¿‡ï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰
                        print(f"[RDTWrapper] âš ï¸  Skipping {k}: shape mismatch {v.shape} vs {target_shape}")
                        continue
                        
                    new_state_dict[k] = v
            
            self.rdt_model.load_state_dict(new_state_dict, strict=False)

        # 5. Initialize Projection Layers
        target_dim = actual_dim 
        self.action_proj = nn.Linear(int(action_dim), int(target_dim))
        self.cond_proj = nn.Linear(int(rdt_cond_dim), int(target_dim))

        # === é€‚é… 3: å¼ºåˆ¶è°ƒæ•´æ¨¡å‹å†…éƒ¨ img_pos_embed å¤§å° ===
        DUBBY_IMG_LEN = 2
        if hasattr(self.rdt_model, 'img_cond_pos_embed'):
             if self.rdt_model.img_cond_pos_embed.shape[1] > DUBBY_IMG_LEN:
                 print(f"[RDTWrapper] ğŸ“‰ Resizing internal img_cond_pos_embed to length {DUBBY_IMG_LEN}")
                 old_pe = self.rdt_model.img_cond_pos_embed.data
                 new_pe = nn.Parameter(old_pe[:, :DUBBY_IMG_LEN, :].clone())
                 self.rdt_model.img_cond_pos_embed = new_pe

    # =================================================
    # è¾…åŠ©æ–¹æ³•ï¼šæ³¨æ„ç¼©è¿›ï¼Œå®ƒå¿…é¡»åœ¨ class RDTWrapper å†…éƒ¨
    # =================================================
    def _load_config_and_override(self, config_path, target_action_dim):
        class Args: pass
        args = Args()
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r') as f:
                cfg = json.load(f)
            
            if 'rdt' in cfg and 'hidden_size' in cfg['rdt']:
                self.rdt_hidden_size = int(cfg['rdt']['hidden_size'])
            elif 'hidden_size' in cfg:
                self.rdt_hidden_size = int(cfg['hidden_size'])
                
            for k, v in cfg.items():
                if k == 'rdt' and isinstance(v, dict):
                    for sub_k, sub_v in v.items():
                        setattr(args, sub_k, sub_v)
                setattr(args, k, v)
                
        args.action_dim = int(target_action_dim)
        args.output_dim = int(target_action_dim)
        args.out_channels = int(target_action_dim)
        args.input_size = int(target_action_dim)
        args.in_channels = int(target_action_dim)
        
        args.hidden_size = self.rdt_hidden_size
        args.embed_dim = self.rdt_hidden_size 
        args.d_model = self.rdt_hidden_size
        
        args.horizon = int(pred_horizon)      # é¢„æµ‹æœªæ¥å¤šå°‘æ­¥
        args.pred_horizon = int(pred_horizon)
        
        defaults = {'patch_size': 1, 'img_size': 1, 'num_frames': 1}
        for k, v in defaults.items():
            if not hasattr(args, k): setattr(args, k, v)
            
        return args

    # def forward(self, noisy_action, timestep, conditions):
    #     e_t = conditions['e_t']
    #     cond_embeds = self.cond_proj(e_t).unsqueeze(1) # [B, 1, D]
        
    #     if noisy_action.dim() == 2: x_in = noisy_action
    #     else: x_in = noisy_action.squeeze(1)
            
    #     x_embed = self.action_proj(x_in).unsqueeze(1) # [B, 1, D]
        
    #     B = x_embed.shape[0]
    #     device = x_embed.device
        
    #     freq = torch.full((B,), 30, device=device, dtype=torch.long)
        
    #     lang_c = cond_embeds 
        
    #     # å›¾åƒæ¡ä»¶ï¼šå…¨ 0 å ä½ç¬¦ (é•¿åº¦ä¸º 2)
    #     img_c = torch.zeros((B, 2, cond_embeds.shape[-1]), device=device, dtype=cond_embeds.dtype)
        
    #     lang_mask = torch.ones((B, 1), device=device, dtype=torch.bool)
    #     img_mask = torch.ones((B, 2), device=device, dtype=torch.bool)

    #     return self.rdt_model(
    #         x=x_embed, 
    #         freq=freq, 
    #         t=timestep, 
    #         lang_c=lang_c, 
    #         img_c=img_c,
    #         lang_mask=lang_mask,
    #         img_mask=img_mask
    #     )


    def forward(self, noisy_action, timestep, conditions):
        """
        RDTWrapper çš„å‰å‘ä¼ æ’­ (ä¿®æ”¹ç‰ˆ)
        è¾“å…¥:
            noisy_action: [B, Horizon, Action_Dim] (ä¾‹å¦‚ [B, 16, 8])
            timestep: [B]
            conditions: dict, åŒ…å« 'e_t' (åºåˆ—ç‰¹å¾)
        """
        # 1. è·å–è§†è§‰åºåˆ—ç‰¹å¾
        e_t = conditions['e_t'] # æœŸæœ›å½¢çŠ¶: [B, 64, 768] (æ¥è‡ª FusionEncoder çš„åºåˆ—è¾“å‡º)
        
        B = e_t.shape[0]
        device = e_t.device
        dtype = e_t.dtype
        
        # 2. è§†è§‰æŠ•å½± (768 -> RDT Hidden Size, é€šå¸¸æ˜¯ 1152)
        # åŠ¨æ€æ£€æŸ¥å¹¶åˆå§‹åŒ–æŠ•å½±å±‚ (Lazy Initialization)ï¼Œé˜²æ­¢ __init__ æ²¡æ”¹å¯¼è‡´æŠ¥é”™
        # æ¨èæ‚¨åç»­æœ€å¥½æŠŠå®ƒç§»åˆ° __init__ é‡Œ: self.visual_proj = nn.Linear(768, 1152)
        if not hasattr(self, 'visual_proj'):
            # RDT-1B çš„ hidden_size é€šå¸¸æ˜¯ 2048 (InternViT) æˆ– 1152 (SigLIP/Patch)
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦æ˜ å°„åˆ° model.img_embedder æœŸæœ›çš„ç»´åº¦
            # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬è¯»å– self.rdt_model.config.hidden_size æˆ–ç›´æ¥ç¡¬ç¼–ç  1152 (å¸¸è§é…ç½®)
            # æ›´ç¨³å¦¥çš„æ–¹å¼æ˜¯çœ‹ img_c åº”è¯¥è¿›å“ªé‡Œã€‚RDT å†…éƒ¨é€šå¸¸æœ‰ img_adaptorã€‚
            # è¿™é‡Œå‡è®¾ RDT å†…éƒ¨ img_c çš„æœŸæœ›ç»´åº¦æ˜¯ 1152 (SigLIP-So400m çš„ dim)
            target_dim = 1152 
            print(f"[RDTWrapper] Lazy initializing visual_proj: {e_t.shape[-1]} -> {target_dim}")
            self.visual_proj = nn.Linear(e_t.shape[-1], target_dim).to(device).to(dtype)
        
        # [B, 64, 768] -> [B, 64, 1152]
        img_c = self.visual_proj(e_t)
        
        # 3. æ„é€ æ–‡æœ¬æ¡ä»¶ (Language Condition)
        # å› ä¸ºæˆ‘ä»¬ä¸»è¦ä¾èµ–è§†è§‰ï¼Œè¿™é‡Œä¼ å…¥ç©ºçš„æ–‡æœ¬åµŒå…¥ (Zero Padding)
        # RDT æœŸæœ› lang_c å½¢çŠ¶ä¸º [B, L, D]ï¼Œè¿™é‡Œè®¾ L=1
        lang_c = torch.zeros((B, 1, 1152), device=device, dtype=dtype)
        
        # 4. æ„é€  Masks
        # img_mask: [B, 64], å…¨ 1 (æ‰€æœ‰è§†è§‰ Token æœ‰æ•ˆ)
        img_mask = torch.ones((B, img_c.shape[1]), device=device, dtype=torch.long)
        
        # lang_mask: [B, 1], å…¨ 1 (è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„"ç©ºæŒ‡ä»¤")
        # æ³¨æ„: RDT å†…éƒ¨å¯¹ mask çš„å¤„ç†é€šå¸¸æ˜¯ bool æˆ– 0/1, long æ¯”è¾ƒç¨³å¦¥
        lang_mask = torch.ones((B, 1), device=device, dtype=torch.long)

        # 5. å¤„ç†åŠ¨ä½œè¾“å…¥
        # å¦‚æœè¾“å…¥æ˜¯ [B, 8]ï¼Œunsqueeze æˆ [B, 1, 8]
        # å¦‚æœæ˜¯ [B, 16, 8]ï¼Œä¿æŒä¸å˜
        if noisy_action.dim() == 2:
            x_in = noisy_action.unsqueeze(1)
        else:
            x_in = noisy_action
            
        # æŠ•å½±åŠ¨ä½œç»´åº¦ [B, H, 8] -> [B, H, Hidden]
        x_embed = self.action_proj(x_in)
        
        # 6. æ§åˆ¶é¢‘ç‡ (Control Frequency)
        # å›ºå®šä¸º 30Hz æˆ–ä» dataset ç»Ÿè®¡ä¸­è·å–
        freq = torch.full((B,), 30, device=device, dtype=torch.long)

        # 7. è°ƒç”¨ RDT Backbone
        return self.rdt_model(
            x=x_embed, 
            freq=freq, 
            t=timestep, 
            lang_c=lang_c, 
            img_c=img_c,
            lang_mask=lang_mask,
            img_mask=img_mask
        )