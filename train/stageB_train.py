# # train/stageB_train_fast.py
# import sys
# import os
# import torch
# import torch.optim as optim
# import argparse
# import time
# from torch.utils.data import DataLoader
# from torch.amp import autocast # BF16 ä¸éœ€è¦ GradScaler

# # å¼ºåˆ¶ä½¿ç”¨ Flash Attentionï¼Œç¦ç”¨æ™®é€šæ•°å­¦æ³¨æ„åŠ›
# torch.backends.cuda.enable_flash_sdp(True)
# torch.backends.cuda.enable_math_sdp(False) 
# torch.backends.cuda.enable_mem_efficient_sdp(True)

# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# from model.fusion_encoder import FusionEncoder
# from losses.distillation_loss import DistillationLoss
# from losses.decoupling_regularizer import DecouplingLoss
# from losses.temporal_consistency import TemporalConsistencyLoss
# from utils.dataset_loader import RobotDataset

# VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'

# def train_stage_b(args):
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     print(f"=== Stage B Training: Real Teacher Distillation on {device} ===")
#     print(f"=== Mode: BF16 (Fast & Stable) | Workers: {args.num_workers} | Batch: {args.batch_size} ===")
    
#     # å¯ç”¨ TF32 (åœ¨ A800 ä¸Šèƒ½åŠ é€Ÿ FP32/BF16 è®¡ç®—)
#     torch.backends.cuda.matmul.allow_tf32 = True
#     torch.backends.cudnn.allow_tf32 = True

#     # 1. åˆå§‹åŒ–æ¨¡å‹
#     model = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(device)
    
#     if os.path.exists(args.stage_a_ckpt):
#         print(f"Loading Stage A: {args.stage_a_ckpt}")
#         model.load_state_dict(torch.load(args.stage_a_ckpt), strict=False)

#     # å†»ç»“å‚æ•°
#     for param in model.backbone.parameters(): param.requires_grad = False
    
#     layers_to_train = ["blocks.20", "blocks.21", "blocks.22", "blocks.23"] 
#     count = 0
#     for name, param in model.backbone.named_parameters():
#         if any(x in name for x in layers_to_train) or "encoder.layer.2" in name: 
#             param.requires_grad = True
#             count += 1
#     print(f"Unfrozen {count} parameters in VideoMAE backbone.")
    
#     for p in model.routing_layer.parameters(): p.requires_grad = True
#     for p in model.semantic_align_head.parameters(): p.requires_grad = True
#     for p in model.temporal_align_head.parameters(): p.requires_grad = True
#     for p in model.projection_head.parameters(): p.requires_grad = True
    
#     # === ä¼˜åŒ–ç‚¹ A: ç¼–è¯‘æ¨¡å‹ (PyTorch 2.0+) ===
#     print("Compiling model with torch.compile... (First step will be slow)")
#     # try:
#     #     model = torch.compile(model)
#     # except Exception as e:
#     #     print(f"Compile failed, falling back to eager mode: {e}")

#     # 2. æ•°æ®åŠ è½½
#     print(f"Loading data from: {args.data_root}")
#     dataset = RobotDataset(hdf5_path=args.data_root, window_size=16)
    
#     loader = DataLoader(
#         dataset, 
#         batch_size=args.batch_size, 
#         shuffle=True, 
#         num_workers=args.num_workers,
#         pin_memory=True,
#         persistent_workers=True,
#         drop_last=True
#     )

#     # 3. æŸå¤±ä¸ä¼˜åŒ–
#     distill_fn = DistillationLoss()
#     decouple_fn = DecouplingLoss()
#     temporal_fn = TemporalConsistencyLoss()
    
#     # å­¦ä¹ ç‡å¯ä»¥é€‚å½“å›å‡ï¼Œå› ä¸º BF16 å¾ˆç¨³ï¼Œä¸” Batch Size å˜å¤§äº†
#     optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)
#     loss_weights = {"distill":1.0, "decouple":0.5, "consistency":1.0}

#     # 4. è®­ç»ƒå¾ªç¯
#     model.train()
#     print(">>> Training Started... (BF16 Mode) <<<")
    
#     for epoch in range(args.epochs):
#         start_time = time.time()
        
#         for i, batch in enumerate(loader):
#             video = batch['video'].to(device, non_blocking=True)
#             state = batch['state'].to(device, non_blocking=True)
#             text = batch['text_tokens'].to(device, non_blocking=True)
#             ff = batch['first_frame'].to(device, non_blocking=True)
            
#             real_siglip = batch['teacher_siglip'].to(device, non_blocking=True)
#             real_exo = batch['teacher_exo'].to(device, non_blocking=True)
            
#             # æ—¶åºå¹³å‡
#             siglip_target = torch.mean(real_siglip, dim=1)
#             exo_target = torch.mean(real_exo, dim=1)
            
#             # === æ–°å¢ï¼šç»™ Teacher ç‰¹å¾åŠ å™ªå£° (ä»…åœ¨è®­ç»ƒæ—¶) ===
#             if model.training:
#                 noise_scale = 0.01 # è§†ç‰¹å¾æ•°å€¼èŒƒå›´è€Œå®šï¼Œé€šå¸¸ 0.01-0.05
#                 siglip_target += torch.randn_like(siglip_target) * noise_scale
#                 exo_target += torch.randn_like(exo_target) * noise_scale
#             # ============================================

#             teacher_feats = {
#                 "siglip_features": siglip_target,
#                 "exo_features": exo_target
#             }

#             optimizer.zero_grad()

#             # === ä¼˜åŒ–ç‚¹ B: å¯ç”¨ BFloat16 ===
#             # A800 ä¸“å±ï¼šä¸éœ€è¦ Scalerï¼Œå› ä¸ºèŒƒå›´å¤Ÿå¤§ï¼Œä¸ä¼šæº¢å‡º
#             with autocast('cuda', dtype=torch.bfloat16):
#                 out = model(video, text, state, ff)
                
#                 l_distill, _ = distill_fn(out, teacher_feats)
#                 l_decouple = decouple_fn(out['task_slots'], out['background_context'], out['task_confidence'])
#                 l_time = temporal_fn(out['temporal_head_output'])
                
#                 loss = loss_weights['distill'] * l_distill + \
#                        loss_weights['decouple'] * l_decouple + \
#                        loss_weights['consistency'] * l_time

#             # === æ™®é€šåå‘ä¼ æ’­ (BF16 ä¸éœ€è¦ scaler.step) ===
#             loss.backward()
            
#             # ä¾ç„¶ä¿ç•™æ¢¯åº¦è£å‰ªä»¥é˜²ä¸‡ä¸€
#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
#             optimizer.step()
            
#             # è¿›åº¦æ‰“å°
#             if i % 10 == 0 and i > 0:
#                 elapsed = time.time() - start_time
#                 speed = (i * args.batch_size) / elapsed
#                 print(f"Epoch {epoch} [{i}/{len(loader)}] Loss: {loss.item():.8e} | Distill: {l_distill.item():.8e} | Speed: {speed:.1f} img/s")

#     os.makedirs(args.output_dir, exist_ok=True)
#     save_path = os.path.join(args.output_dir, "1223stageB_papercup.pt")
#     torch.save(model.state_dict(), save_path)
#     print(f"Saved to {save_path}")


# if __name__ == '__main__':
#     parser = argparse.ArgumentParser()
#     parser.add_argument('--data_root', type=str, required=True) 
#     parser.add_argument('--stage_a_ckpt', type=str, default='/yanghaochuan/checkpoints/stageA_final.pt')
#     parser.add_argument('--output_dir', type=str, default='/yanghaochuan/checkpoints')
#     # === ä¼˜åŒ–ç‚¹ C: Batch Size ç¿»å€ ===
#     parser.add_argument('--batch_size', type=int, default=24) # ç›´æ¥ä¸Š 48ï¼
#     parser.add_argument('--num_workers', type=int, default=16)
#     parser.add_argument('--epochs', type=int, default=2)
#     args = parser.parse_args()
    
#     train_stage_b(args)

# train/stageB_train.py
import sys
import os
import torch
import torch.optim as optim
import argparse
import time
import numpy as np
from torch.utils.data import DataLoader
from torch.amp import autocast # BF16 ä¸éœ€è¦ GradScaler

# === [ç¯å¢ƒæ£€æŸ¥] WandB ===
try:
    import wandb
    HAS_WANDB = True
except ImportError:
    HAS_WANDB = False
    print("âš ï¸ WandB not found. Install with `pip install wandb`")

# å¼ºåˆ¶ä½¿ç”¨ Flash Attention
torch.backends.cuda.enable_flash_sdp(True)
torch.backends.cuda.enable_math_sdp(False) 
torch.backends.cuda.enable_mem_efficient_sdp(True)

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model.fusion_encoder import FusionEncoder
from losses.distillation_loss import DistillationLoss
from losses.decoupling_regularizer import DecouplingLoss
from losses.temporal_consistency import TemporalConsistencyLoss
from utils.dataset_loader import RobotDataset

VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'

def train_stage_b(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"=== Stage B Training: Real Teacher Distillation on {device} ===")
    print(f"=== Mode: Step-Based Training | Target: {args.max_train_steps} Steps ===")
    
    # å¯ç”¨ TF32
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # === WandB åˆå§‹åŒ– ===
    if args.use_wandb and HAS_WANDB:
        wandb.init(
            project="RDT-StageB-Pretrain",
            name=f"step{args.max_train_steps}_mask0.8_{int(time.time())}",
            config=vars(args),
            resume="allow"
        )

    # 1. åˆå§‹åŒ–æ¨¡å‹
    model = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(device)
    
    if os.path.exists(args.stage_a_ckpt):
        print(f"Loading Stage A: {args.stage_a_ckpt}")
        model.load_state_dict(torch.load(args.stage_a_ckpt), strict=False)

    # å†»ç»“ VideoMAEï¼Œåªå¾®è°ƒ Adapter å’Œéƒ¨åˆ† Block
    for param in model.backbone.parameters(): param.requires_grad = False
    
    layers_to_train = ["blocks.20", "blocks.21", "blocks.22", "blocks.23"] 
    count = 0
    for name, param in model.backbone.named_parameters():
        if any(x in name for x in layers_to_train) or "encoder.layer.2" in name: 
            param.requires_grad = True
            count += 1
    print(f"Unfrozen {count} parameters in VideoMAE backbone.")
    
    for p in model.routing_layer.parameters(): p.requires_grad = True
    for p in model.semantic_align_head.parameters(): p.requires_grad = True
    for p in model.temporal_align_head.parameters(): p.requires_grad = True
    for p in model.projection_head.parameters(): p.requires_grad = True
    
    print("Compiling model with torch.compile...")
    # try:
    #     model = torch.compile(model)
    # except Exception as e:
    #     print(f"Compile failed: {e}")

    # 2. æ•°æ®åŠ è½½
    print(f"Loading data from: {args.data_root}")
    dataset = RobotDataset(hdf5_path=args.data_root, window_size=16)
    
    loader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=args.num_workers,
        pin_memory=True,
        persistent_workers=True,
        drop_last=True
    )

    # 3. æŸå¤±ä¸ä¼˜åŒ–
    distill_fn = DistillationLoss()
    decouple_fn = DecouplingLoss()
    temporal_fn = TemporalConsistencyLoss()
    
    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)
    loss_weights = {"distill":1.0, "decouple":0.5, "consistency":1.0}

    # ====================================================
    # 4. ç²¾å‡†æ–­ç‚¹ç»­è®­é€»è¾‘ (Step-Based Resume)
    # ====================================================
    global_step = 0
    start_epoch = 0
    resume_batch_idx = 0

    if args.resume_from_checkpoint and os.path.exists(args.resume_from_checkpoint):
        print(f"ğŸ”„ Resuming from: {args.resume_from_checkpoint}")
        checkpoint = torch.load(args.resume_from_checkpoint, map_location=device)
        
        # æ¢å¤æƒé‡
        # å…¼å®¹åªä¿å­˜äº† model state dict çš„æƒ…å†µï¼Œä¹Ÿå…¼å®¹ä¿å­˜äº†å®Œæ•´ info çš„æƒ…å†µ
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint) # æ—§æ ¼å¼

        if 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # æ¢å¤æ­¥æ•°
        if 'global_step' in checkpoint:
            global_step = checkpoint['global_step']
            print(f"   -> Found Global Step: {global_step}")
            
            # è®¡ç®—æˆ‘ä»¬éœ€è¦è·³è¿‡å¤šå°‘ä¸ªç‰©ç† Batch
            total_physical_batches = global_step * args.gradient_accumulation_steps
            start_epoch = total_physical_batches // len(loader)
            resume_batch_idx = total_physical_batches % len(loader)
            
            print(f"   -> Resume Location: Epoch {start_epoch}, Batch {resume_batch_idx}")

    # ====================================================
    # 5. è®­ç»ƒå¾ªç¯ (Step-Based)
    # ====================================================
    model.train()
    print(">>> Training Started... (BF16 Mode) <<<")
    
    total_epochs = 999999 # æ— é™å¾ªç¯ï¼Œç”± max_train_steps ç»ˆæ­¢
    
    for epoch in range(start_epoch, total_epochs):
        start_time = time.time()
        
        for i, batch in enumerate(loader):
            # â© è·³è¿‡å·²è®­ç»ƒçš„æ•°æ® (ç²¾å‡†ç»­è®­)
            if epoch == start_epoch and i < resume_batch_idx:
                if i % 50 == 0: print(f"â© Skipping batch {i}/{len(loader)}...", end='\r')
                continue

            # --- æ•°æ®å‡†å¤‡ ---
            video = batch['video'].to(device, non_blocking=True)
            state = batch['state'].to(device, non_blocking=True)
            text = batch['text_tokens'].to(device, non_blocking=True)
            ff = batch['first_frame'].to(device, non_blocking=True)
            
            real_siglip = batch['teacher_siglip'].to(device, non_blocking=True)
            real_exo = batch['teacher_exo'].to(device, non_blocking=True)
            
            # ==========================================================
            # ğŸš¨ Blind Masking ç­–ç•¥ (ä¿ç•™ 80% Mask é€»è¾‘)
            # ==========================================================
            mask_type_log = "Full_Input"
            mask_prob = 0.8 
            B = video.shape[0]
            should_mask = torch.rand(B, device=device) < mask_prob
            
            if should_mask.any():
                video[should_mask, 0] = 0.0
                ff[should_mask, 0] = 0.0 # ğŸš¨ åŒæ­¥ Mask é¦–å¸§
                mask_type_log = "Masked_Main"
            # ==========================================================

            # Teacher å‡†å¤‡
            siglip_target = torch.mean(real_siglip, dim=1)
            exo_target = torch.mean(real_exo, dim=1)
            
            noise_scale = 0.01 
            siglip_target += torch.randn_like(siglip_target) * noise_scale
            exo_target += torch.randn_like(exo_target) * noise_scale

            teacher_feats = {
                "siglip_features": siglip_target,
                "exo_features": exo_target
            }

            # --- å‰å‘ä¼ æ’­ & Loss ---
            with autocast('cuda', dtype=torch.bfloat16):
                out = model(video, text, state, ff)
                
                l_distill, _ = distill_fn(out, teacher_feats)
                l_decouple = decouple_fn(out['task_slots'], out['background_context'], out['task_confidence'])
                l_time = temporal_fn(out['temporal_head_output'])
                
                loss = loss_weights['distill'] * l_distill + \
                       loss_weights['decouple'] * l_decouple + \
                       loss_weights['consistency'] * l_time
                
                # ğŸŒŸ æ¢¯åº¦ç´¯ç§¯å½’ä¸€åŒ–
                loss = loss / args.gradient_accumulation_steps

            # --- åå‘ä¼ æ’­ ---
            loss.backward()

            # --- å‚æ•°æ›´æ–° (æ¯ accum steps ä¸€æ¬¡) ---
            if (i + 1) % args.gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                optimizer.zero_grad()
                
                global_step += 1
                
                # --- æ—¥å¿—è®°å½• ---
                if global_step % 10 == 0:
                    # è¿˜åŸ loss æ•°å€¼
                    real_loss = loss.item() * args.gradient_accumulation_steps
                    
                    elapsed = time.time() - start_time
                    # ä¼°ç®—æ¯ä¸ª step çš„æ—¶é—´ (åŒ…å« accum)
                    speed = (args.batch_size * args.gradient_accumulation_steps) / (elapsed / (i - resume_batch_idx + 1) * args.gradient_accumulation_steps + 1e-6)
                    
                    print(f"Step [{global_step}/{args.max_train_steps}] Loss: {real_loss:.6f} | Distill: {l_distill.item():.6f} (Ep {epoch})")

                    if args.use_wandb and HAS_WANDB:
                        wandb.log({
                            "total_loss": real_loss,
                            "distill_loss": l_distill.item(),
                            "decouple_loss": l_decouple.item(),
                            "temporal_loss": l_time.item(),
                            "global_step": global_step,
                            "epoch": epoch,
                            "lr": optimizer.param_groups[0]['lr']
                        }, step=global_step)

                # --- è§†é¢‘å¯è§†åŒ– (æ¯ 500 æ­¥) ---
                if global_step % 500 == 0 and args.use_wandb and HAS_WANDB:
                    try:
                        vid_sample = video[0].float().cpu().numpy()
                        main_view = np.transpose(vid_sample[0], (1, 0, 2, 3))
                        wrist_view = np.transpose(vid_sample[1], (1, 0, 2, 3))
                        combined_view = np.concatenate([main_view, wrist_view], axis=3)
                        
                        wandb.log({
                            "input_monitor": wandb.Video((combined_view * 255).astype(np.uint8), fps=4, format="gif", caption=f"Step{global_step}: {mask_type_log}")
                        }, step=global_step)
                    except Exception as e:
                        print(f"WandB upload failed: {e}")

                # --- Checkpoint ä¿å­˜ ---
                if global_step % args.checkpointing_steps == 0:
                    save_path = os.path.join(args.output_dir, f"1226stageB_step_{global_step}.pt")
                    torch.save({
                        'epoch': epoch,
                        'global_step': global_step,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                    }, save_path)
                    print(f"ğŸ’¾ Checkpoint saved: {save_path}")

                # --- ç»“æŸè®­ç»ƒ ---
                if global_step >= args.max_train_steps:
                    print(f"ğŸ‰ Reached target {args.max_train_steps} steps. Training Finished.")
                    final_path = os.path.join(args.output_dir, f"stageB_final.pt")
                    torch.save(model.state_dict(), final_path) # Final åªå­˜æƒé‡æ–¹ä¾¿åŠ è½½
                    
                    if args.use_wandb and HAS_WANDB: wandb.finish()
                    return

    if args.use_wandb and HAS_WANDB: wandb.finish()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_root', type=str, required=True) 
    parser.add_argument('--stage_a_ckpt', type=str, default='/yanghaochuan/checkpoints/stageA_final.pt')
    parser.add_argument('--output_dir', type=str, default='/yanghaochuan/checkpoints')
    
    # è®­ç»ƒè¶…å‚
    parser.add_argument('--batch_size', type=int, default=24) 
    parser.add_argument('--num_workers', type=int, default=16)
    
    # Step-based æ§åˆ¶å‚æ•°
    parser.add_argument('--max_train_steps', type=int, default=20000, help="Total training steps")
    parser.add_argument('--checkpointing_steps', type=int, default=2000, help="Save every X steps")
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help="Simulate larger batch size")
    
    # æ‚é¡¹
    parser.add_argument('--resume_from_checkpoint', type=str, default=None)
    parser.add_argument('--use_wandb', action='store_true', default=False)
    
    args = parser.parse_args()
    
    train_stage_b(args)