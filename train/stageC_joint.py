# import sys
# import os
# import torch
# import torch.optim as optim
# import argparse
# import time
# import numpy as np
# import torch.nn.functional as F
# from torch.utils.data import DataLoader
# from torch.amp import autocast
# from diffusers import DDPMScheduler
# from peft import LoraConfig, get_peft_model
# from torch.utils.tensorboard import SummaryWriter 

# # === [ç¯å¢ƒæ£€æŸ¥] WandB ===
# try:
#     import wandb
#     HAS_WANDB = True
# except ImportError:
#     HAS_WANDB = False
#     print("âš ï¸ WandB not found. Install with `pip install wandb` for better visualization.")

# # === æ€§èƒ½ä¼˜åŒ– ===
# torch.backends.cuda.enable_flash_sdp(True)
# torch.backends.cuda.enable_mem_efficient_sdp(True)
# torch.backends.cuda.enable_math_sdp(False)

# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# from model.fusion_encoder import FusionEncoder
# from model.rdt_model import RDTWrapper
# from utils.dataset_loader import RobotDataset
# from losses.consistency_loss import compute_consistency_loss
# # ğŸš¨ [æ–°å¢] å¼•å…¥è’¸é¦ Loss ä½œä¸ºæ­£åˆ™é¡¹
# from losses.distillation_loss import DistillationLoss

# # === è·¯å¾„é…ç½® ===
# VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
# RDT_PATH = '/yanghaochuan/models/rdt-1b'
# STATS_PATH = '/yanghaochuan/data/1223dataset_stats.json'

# def train_stage_c(args):
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
#     # ====================================================
#     # 0. åˆå§‹åŒ–æ—¥å¿—
#     # ====================================================
#     log_dir = os.path.join(args.output_dir, "logs")
#     os.makedirs(log_dir, exist_ok=True)
#     tb_writer = SummaryWriter(log_dir=log_dir)
    
#     if args.use_wandb and HAS_WANDB:
#         wandb.init(
#             project="RDT-StageC-Joint",
#             name=f"step{args.max_train_steps}_acc{args.gradient_accumulation_steps}_{int(time.time())}",
#             config=vars(args),
#             resume="allow"
#         )
    
#     print(f"=== Stage C Joint Training (Step-Based + Regularization) ===")
#     print(f"ğŸ¯ Target: {args.max_train_steps} Global Steps")
#     print(f"ğŸ“¦ Physical Batch Size: {args.batch_size}")
#     print(f"ğŸ”‹ Gradient Accumulation: {args.gradient_accumulation_steps}")

#     # ====================================================
#     # 1. æ¨¡å‹åŠ è½½
#     # ====================================================
#     print("Loading Models...")
#     fusion_encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152, rdt_dim=768).to(device)
    
#     # åŠ è½½ Stage B
#     if args.stage_b_ckpt and os.path.exists(args.stage_b_ckpt):
#         print(f"Loading Stage B: {args.stage_b_ckpt}")
#         ckpt = torch.load(args.stage_b_ckpt, map_location='cpu')
#         state_dict = ckpt['encoder_state_dict'] if 'encoder_state_dict' in ckpt else ckpt
#         # å…¼å®¹ key
#         new_state_dict = {}
#         for k, v in state_dict.items():
#             if k.startswith("module."): new_state_dict[k[7:]] = v
#             else: new_state_dict[k] = v
#         fusion_encoder.load_state_dict(new_state_dict, strict=False)
    
#     # å†»ç»“ VideoMAEï¼Œåªå¾®è°ƒ Adapter
#     fusion_encoder.eval() 
#     for param in fusion_encoder.parameters(): param.requires_grad = True 
#     for param in fusion_encoder.backbone.parameters(): param.requires_grad = False
#     if fusion_encoder.text_encoder:
#         for p in fusion_encoder.text_encoder.parameters(): p.requires_grad = False

#     # åŠ è½½ RDT
#     rdt_wrapper = RDTWrapper(action_dim=8, model_path=RDT_PATH, pred_horizon=args.pred_horizon).to(device)
    
#     # RDT æƒé‡åˆ‡ç‰‡åŠ è½½é€»è¾‘
#     if os.path.exists(RDT_PATH) or os.path.exists(os.path.join(RDT_PATH, "pytorch_model.bin")):
#         rdt_file = RDT_PATH if os.path.isfile(RDT_PATH) else os.path.join(RDT_PATH, "pytorch_model.bin")
#         if os.path.exists(rdt_file):
#             print("Loading RDT weights with auto-slicing...")
#             state_dict = torch.load(rdt_file, map_location='cpu')
#             if 'x_pos_embed' in state_dict:
#                 ckpt_pos = state_dict['x_pos_embed']
#                 curr_pos = rdt_wrapper.rdt_model.x_pos_embed
#                 if ckpt_pos.shape != curr_pos.shape:
#                     print(f"âœ‚ï¸ Slicing position embed: {ckpt_pos.shape} -> {curr_pos.shape}")
#                     state_dict['x_pos_embed'] = ckpt_pos[:, :curr_pos.shape[1], :]
#             rdt_wrapper.rdt_model.load_state_dict(state_dict, strict=False)

#     # LoRA é…ç½®
#     print("Applying LoRA...")
#     peft_config = LoraConfig(
#         r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], 
#         lora_dropout=0.05, bias="none"
#     )
#     rdt_wrapper.rdt_model = get_peft_model(rdt_wrapper.rdt_model, peft_config)
    
#     # ====================================================
#     # 2. ä¼˜åŒ–å™¨ & Loss
#     # ====================================================
#     params = [
#         {'params': filter(lambda p: p.requires_grad, rdt_wrapper.parameters()), 'lr': 1e-4},
#         {'params': filter(lambda p: p.requires_grad, fusion_encoder.parameters()), 'lr': 1e-5}
#     ]
#     optimizer = optim.AdamW(params, weight_decay=1e-4)
    
#     noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2", prediction_type="sample")

#     # ğŸš¨ [æ–°å¢] è’¸é¦ Loss (ç”¨äºæ­£åˆ™åŒ–)
#     distill_fn = DistillationLoss()

#     # ====================================================
#     # 3. æ•°æ®åŠ è½½
#     # ====================================================
#     print(f"Loading Dataset from {args.data_root}")
#     dataset = RobotDataset(hdf5_path=args.data_root, window_size=16, pred_horizon=args.pred_horizon, stats_path=STATS_PATH)
#     loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)

#     # ====================================================
#     # 4. ç²¾å‡†æ–­ç‚¹ç»­è®­
#     # ====================================================
#     global_step = 0
#     start_epoch = 0
#     resume_batch_idx = 0

#     if args.resume_from_checkpoint and os.path.exists(args.resume_from_checkpoint):
#         print(f"ğŸ”„ Resuming from: {args.resume_from_checkpoint}")
#         checkpoint = torch.load(args.resume_from_checkpoint, map_location=device)
        
#         if 'rdt_state_dict' in checkpoint: rdt_wrapper.load_state_dict(checkpoint['rdt_state_dict'], strict=False)
#         if 'encoder_state_dict' in checkpoint: fusion_encoder.load_state_dict(checkpoint['encoder_state_dict'], strict=False)
#         if 'optimizer_state_dict' in checkpoint: optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
#         if 'global_step' in checkpoint:
#             global_step = checkpoint['global_step']
#             total_physical_batches = global_step * args.gradient_accumulation_steps
#             start_epoch = total_physical_batches // len(loader)
#             resume_batch_idx = total_physical_batches % len(loader)
#             print(f"   -> Resume Location: Epoch {start_epoch}, Batch {resume_batch_idx}")
#         elif 'epoch' in checkpoint:
#             start_epoch = checkpoint['epoch'] + 1
#             global_step = start_epoch * len(loader) // args.gradient_accumulation_steps
#             print(f"   -> Resuming from Epoch {start_epoch} (Approx. Step {global_step})")

#     # ====================================================
#     # 5. è®­ç»ƒå¾ªç¯
#     # ====================================================
#     print(">>> Training Started <<<")
#     total_epochs = 999999 
    
#     for epoch in range(start_epoch, total_epochs):
#         rdt_wrapper.train()
        
#         for i, batch in enumerate(loader):
#             if epoch == start_epoch and i < resume_batch_idx:
#                 if i % 50 == 0: print(f"â© Skipping batch {i}/{len(loader)}...", end='\r')
#                 continue

#             # --- æ•°æ®å‡†å¤‡ ---
#             video = batch['video'].to(device, non_blocking=True) # [B, 2, C, T, H, W]
#             state = batch['state'].to(device, non_blocking=True)
#             text = batch['text_tokens'].to(device, non_blocking=True)
#             ff = batch['first_frame'].to(device, non_blocking=True)
#             actions = batch['action_target'].to(device, non_blocking=True)

#             # --- Teacher Features (ç”¨äºæ­£åˆ™åŒ–) ---
#             real_siglip = batch['teacher_siglip'].to(device, non_blocking=True)
#             real_exo = batch['teacher_exo'].to(device, non_blocking=True)
#             siglip_target = torch.mean(real_siglip, dim=1)
#             exo_target = torch.mean(real_exo, dim=1)
#             teacher_feats = {"siglip_features": siglip_target, "exo_features": exo_target}

#             # ==========================================================
#             # ğŸš¨ [å…³é”®ä¿®å¤] Modality Dropout (æ•°æ®å…‹éš†ä¸åŒæ­¥ Mask)
#             # ==========================================================
#             rand_val = torch.rand(1).item()
#             mask_type = "Teacher_Mode"
            
#             # 1. å¿…é¡» Clone! å¦åˆ™ä¼šæ±¡æŸ“ batch['video']ï¼Œå¯¼è‡´ Consistency Loss é‡Œçš„ Teacher ä¹Ÿæ˜¯é»‘çš„
#             video_input = video.clone()
#             ff_input = ff.clone()
            
#             if rand_val < 0.7: 
#                 # 70% æ¦‚ç‡ï¼šæ¨¡æ‹Ÿæ¨ç† (Student Mode)
#                 video_input[:, 0] = 0.0
#                 ff_input[:, 0] = 0.0     # <--- ğŸš¨ åŒæ­¥ Mask é¦–å¸§ (é˜²ä½œå¼Š)
#                 mask_type = "Main_Masked"
#             elif rand_val < 0.8: 
#                 # 10% æ¦‚ç‡ï¼šæ‰‹è…•é®æŒ¡
#                 video_input[:, 1] = 0.0
#                 ff_input[:, 1] = 0.0     # <--- ğŸš¨ åŒæ­¥ Mask é¦–å¸§
#                 mask_type = "Wrist_Masked"
#             # 20% æ¦‚ç‡ï¼šTeacher Mode (å…¨å¯è§)
            
#             # ==========================================================
            
#             # --- å‰å‘ä¼ æ’­ ---
#             with autocast('cuda', dtype=torch.bfloat16):
#                 # 1. Encode (ä½¿ç”¨ Mask åçš„è¾“å…¥)
#                 # è¿”å›å®Œæ•´ dict ä»¥ä¾¿è®¡ç®— Distill Loss
#                 encoder_out = fusion_encoder(video_input, text, state, ff_input)
#                 e_t = encoder_out['e_t']
                
#                 # 2. RDT Forward (Action Loss)
#                 timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (actions.shape[0],), device=device).long()
#                 noise = torch.randn_like(actions)
#                 noisy_actions = noise_scheduler.add_noise(actions, noise, timesteps)
                
#                 conditions = {"e_t": e_t, "state": state[:, -1, :]}
#                 pred_noise = rdt_wrapper(noisy_actions, timesteps, conditions)
                
#                 # --- Loss Calculation ---
                
#                 # # Loss 1: Action Diffusion Loss
#                 # loss_diff = F.mse_loss(pred_noise, noise)
                
#                 # # Loss 2: Consistency Loss (Brain Completion)
#                 # # ä½¿ç”¨åŸå§‹ batch (åŒ…å«æœª Mask çš„æ•°æ®)ï¼Œå‡½æ•°å†…éƒ¨ä¼šè‡ªå·±å¤„ç† Student/Teacher æ„å»º
#                 # loss_cons = compute_consistency_loss(fusion_encoder, batch, device)
                
#                 # # Loss 3: Distillation Regularization (Don't forget semantics)
#                 # # å¼ºè¿«å½“å‰ Mask çŠ¶æ€ä¸‹çš„ encoder_out ä¾ç„¶èƒ½æ¢å¤å‡ºå…¨å±€è¯­ä¹‰
#                 # # è¿™å®Œå…¨å¤ç”¨äº† Stage B çš„é€»è¾‘
#                 # loss_distill_reg, _ = distill_fn(encoder_out, teacher_feats)


#                 # 1. æ”¹ä¸º reduction='none' ä»¥ä¾¿æ‰‹åŠ¨åŠ æƒ
#                 loss_diff_raw = F.mse_loss(pred_noise, noise, reduction='none') 
                
#                 # 2. åˆ›å»ºæƒé‡çŸ©é˜µ (é»˜è®¤å…¨æ˜¯ 1.0)
#                 # shape: [Batch, Pred_Horizon, Action_Dim] -> [B, 64, 8]
#                 loss_weights = torch.ones_like(loss_diff_raw)
                
#                 # 4. è®¡ç®—åŠ æƒåçš„å‡å€¼
#                 loss_diff = (loss_diff_raw * loss_weights).mean()
#                 # =================================================================

#                 # Loss 2: Consistency Loss
#                 loss_cons = compute_consistency_loss(fusion_encoder, batch, device)
                
#                 # Loss 3: Distillation Regularization
#                 loss_distill_reg, _ = distill_fn(encoder_out, teacher_feats)
                
#                 # ğŸŒŸ ç»„åˆ Loss
#                 # diff: 1.0 (ä¸»ä»»åŠ¡)
#                 # cons: 0.1 (è¾…åŠ©è„‘è¡¥)
#                 # distill: 0.05 (è¾…åŠ©è¯­ä¹‰é”šå®šï¼Œé˜²æ­¢æ¼‚ç§»)
#                 total_loss = loss_diff + 0.1 * loss_cons + 0.05 * loss_distill_reg
                
#                 # æ¢¯åº¦ç´¯ç§¯å½’ä¸€åŒ–
#                 total_loss = total_loss / args.gradient_accumulation_steps

#             # --- åå‘ä¼ æ’­ ---
#             total_loss.backward()

#             # --- å‚æ•°æ›´æ–° ---
#             if (i + 1) % args.gradient_accumulation_steps == 0:
#                 torch.nn.utils.clip_grad_norm_(rdt_wrapper.parameters(), 1.0)
#                 optimizer.step()
#                 optimizer.zero_grad()
                
#                 global_step += 1
                
#                 # --- æ—¥å¿—è®°å½• ---
#                 if global_step % 10 == 0:
#                     real_loss = total_loss.item() * args.gradient_accumulation_steps
                    
#                     print(f"Step [{global_step}/{args.max_train_steps}] Loss: {real_loss:.4f} | Diff: {loss_diff.item():.4f} | Cons: {loss_cons.item():.4f} | Reg: {loss_distill_reg.item():.4f}")
                    
#                     tb_writer.add_scalar('Train/Total_Loss', real_loss, global_step)
#                     if args.use_wandb and HAS_WANDB:
#                         wandb.log({
#                             "total_loss": real_loss,
#                             "diff_loss": loss_diff.item(),
#                             "cons_loss": loss_cons.item(),
#                             "distill_reg_loss": loss_distill_reg.item(),
#                             "global_step": global_step,
#                             "epoch": epoch,
#                             "lr": optimizer.param_groups[0]['lr']
#                         }, step=global_step)

#                 # --- è§†é¢‘å¯è§†åŒ– ---
#                 if global_step % 500 == 0 and args.use_wandb and HAS_WANDB:
#                     try:
#                         # å¯è§†åŒ–çœŸæ­£å–‚ç»™ RDT çš„æ•°æ® (video_input)
#                         vid_sample = video_input[0].float().cpu().numpy() 
#                         main_view = np.transpose(vid_sample[0], (1, 0, 2, 3))
#                         wrist_view = np.transpose(vid_sample[1], (1, 0, 2, 3))
#                         combined_view = np.concatenate([main_view, wrist_view], axis=3) 
#                         wandb.log({
#                             "input_monitor": wandb.Video((combined_view * 255).astype(np.uint8), fps=4, format="gif", caption=f"S{global_step}: {mask_type}")
#                         }, step=global_step)
#                     except: pass

#                 # --- Checkpoint ä¿å­˜ ---
#                 if global_step % args.checkpointing_steps == 0:
#                     save_path = os.path.join(args.output_dir, f"12stageC_step_{global_step}.pt")
#                     torch.save({
#                         'epoch': epoch,
#                         'global_step': global_step, 
#                         'rdt_state_dict': rdt_wrapper.state_dict(),
#                         'encoder_state_dict': fusion_encoder.state_dict(),
#                         'optimizer_state_dict': optimizer.state_dict(),
#                         'pred_horizon': args.pred_horizon
#                     }, save_path)
#                     print(f"ğŸ’¾ Checkpoint saved: {save_path}")

#                 # --- ç»“æŸè®­ç»ƒ ---
#                 if global_step >= args.max_train_steps:
#                     print(f"ğŸ‰ Reached target {args.max_train_steps} steps. Training Finished.")
#                     final_path = os.path.join(args.output_dir, f"stageC_final_{global_step}.pt")
#                     torch.save({
#                         'epoch': epoch,
#                         'global_step': global_step,
#                         'rdt_state_dict': rdt_wrapper.state_dict(),
#                         'encoder_state_dict': fusion_encoder.state_dict()
#                     }, final_path)
#                     tb_writer.close()
#                     if args.use_wandb and HAS_WANDB: wandb.finish()
#                     return 

#     tb_writer.close()
#     if args.use_wandb and HAS_WANDB: wandb.finish()

# if __name__ == '__main__':
#     parser = argparse.ArgumentParser()
#     parser.add_argument('--data_root', type=str, default='/yanghaochuan/data/12pick_up_the_orange_ball.hdf5')
#     parser.add_argument('--output_dir', type=str, default='/yanghaochuan/16checkpoints')
#     parser.add_argument('--stage_b_ckpt', type=str, default='/yanghaochuan/checkpoints/16stageB_step_2000.pt')
    
#     # ç‰©ç† Batch Size (æ˜¾å­˜é™åˆ¶ï¼Œä¿æŒ 16)
#     parser.add_argument('--batch_size', type=int, default=32)
#     parser.add_argument('--pred_horizon', type=int, default=64)
    
#     # === å…³é”®æ§åˆ¶å‚æ•° ===
#     parser.add_argument('--gradient_accumulation_steps', type=int, default=2, 
#                         help="Number of updates steps to accumulate before update pass. (Effective BS = batch_size * this)")
    
#     parser.add_argument('--max_train_steps', type=int, default=10000, 
#                         help="Total number of training steps (parameter updates) to perform.")
    
#     parser.add_argument('--checkpointing_steps', type=int, default=500, 
#                         help="Save checkpoint every X updates.")
    
#     parser.add_argument('--resume_from_checkpoint', type=str, default=None)
#     parser.add_argument('--use_wandb', action='store_true', default=False)
    
#     args = parser.parse_args()
#     train_stage_c(args)

# train/stageC_joint.py
import sys
import os
import torch
import torch.optim as optim
import argparse
import time
import numpy as np
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.amp import autocast
from diffusers import DDPMScheduler
from peft import LoraConfig, get_peft_model
from torch.utils.tensorboard import SummaryWriter 

# === [ç¯å¢ƒæ£€æŸ¥] WandB ===
try:
    import wandb
    HAS_WANDB = True
except ImportError:
    HAS_WANDB = False
    print("âš ï¸ WandB not found. Install with `pip install wandb` for better visualization.")

# === æ€§èƒ½ä¼˜åŒ– ===
torch.backends.cuda.enable_flash_sdp(True)
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_math_sdp(False)

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model.fusion_encoder import FusionEncoder
from model.rdt_model import RDTWrapper
from utils.dataset_loader import RobotDataset
from losses.consistency_loss import compute_consistency_loss
from losses.distillation_loss import DistillationLoss

# === è·¯å¾„é…ç½® ===
VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
RDT_PATH = '/yanghaochuan/models/rdt-1b'
# ğŸŸ¢ è¯·ç¡®ä¿è¿™é‡ŒæŒ‡å‘æ­£ç¡®çš„ç»Ÿè®¡æ–‡ä»¶
STATS_PATH = '/yanghaochuan/data/121dataset_stats.json' 

def train_stage_c(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # 0. åˆå§‹åŒ–æ—¥å¿—
    log_dir = os.path.join(args.output_dir, "logs")
    os.makedirs(log_dir, exist_ok=True)
    tb_writer = SummaryWriter(log_dir=log_dir)
    
    if args.use_wandb and HAS_WANDB:
        wandb.init(
            project="RDT-StageC-Joint",
            name=f"ForeSight_StageC_{int(time.time())}",
            config=vars(args),
            resume="allow"
        )
    
    print(f"=== ForeSight VLA Training (Stage C: Policy Learning) ===")
    
    # 1. æ¨¡å‹åŠ è½½
    print("Loading Models...")
    # ç¡®ä¿ teacher_dim å’Œ rdt_dim ä¸ Stage B ä¸€è‡´
    fusion_encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152, rdt_dim=768).to(device)
    
    # åŠ è½½ Stage B é¢„è®­ç»ƒæƒé‡ (World Model)
    if args.stage_b_ckpt and os.path.exists(args.stage_b_ckpt):
        print(f"Loading Stage B (World Model): {args.stage_b_ckpt}")
        ckpt = torch.load(args.stage_b_ckpt, map_location='cpu')
        
        # å…¼å®¹åªä¿å­˜äº† state_dict æˆ–å®Œæ•´ checkpoint çš„æƒ…å†µ
        if 'model_state_dict' in ckpt:
            state_dict = ckpt['model_state_dict']
        elif 'encoder_state_dict' in ckpt:
            state_dict = ckpt['encoder_state_dict']
        else:
            state_dict = ckpt
            
        # å»é™¤ module. å‰ç¼€ (å¦‚æœæ˜¯ DDP è®­ç»ƒä¿å­˜çš„)
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith("module."): new_state_dict[k[7:]] = v
            else: new_state_dict[k] = v
            
        msg = fusion_encoder.load_state_dict(new_state_dict, strict=False)
        print(f"Stage B Loaded. Missing keys: {len(msg.missing_keys)}")
    else:
        print("âš ï¸ Warning: No Stage B checkpoint loaded! Training from scratch (Not Recommended).")
    
    # å†»ç»“ VideoMAE Backboneï¼Œå¾®è°ƒå…¶ä»–éƒ¨åˆ†
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è®© Encoder å¤„äº eval æ¨¡å¼ (BN ä¸æ›´æ–°)ï¼Œä½†å‚æ•° requires_grad=True (æƒé‡å¾®è°ƒ)
    fusion_encoder.eval() 
    for param in fusion_encoder.parameters(): param.requires_grad = True 
    for param in fusion_encoder.backbone.parameters(): param.requires_grad = False
    if fusion_encoder.text_encoder:
        for p in fusion_encoder.text_encoder.parameters(): p.requires_grad = False

    # åŠ è½½ RDT Policy
    rdt_wrapper = RDTWrapper(action_dim=8, model_path=RDT_PATH, pred_horizon=args.pred_horizon).to(device)
    
    # RDT æƒé‡åŠ è½½
    if os.path.exists(RDT_PATH) or os.path.exists(os.path.join(RDT_PATH, "pytorch_model.bin")):
        rdt_file = RDT_PATH if os.path.isfile(RDT_PATH) else os.path.join(RDT_PATH, "pytorch_model.bin")
        if os.path.exists(rdt_file):
            print("Loading RDT pretrained weights...")
            state_dict = torch.load(rdt_file, map_location='cpu')
            rdt_wrapper.rdt_model.load_state_dict(state_dict, strict=False)

    # LoRA é…ç½®
    print("Applying LoRA to RDT...")
    peft_config = LoraConfig(
        r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], 
        lora_dropout=0.05, bias="none"
    )
    rdt_wrapper.rdt_model = get_peft_model(rdt_wrapper.rdt_model, peft_config)
    
    # ğŸŸ¢ ç¼–è¯‘ Encoder (Backbone å†»ç»“äº†ï¼Œç¼–è¯‘æ•ˆæœå¾ˆå¥½)
    print("ğŸš€ Compiling FusionEncoder...")
    try:
        fusion_encoder = torch.compile(fusion_encoder)
    except Exception as e:
        print(f"âš ï¸ Encoder compilation failed: {e}")

    # # ğŸŸ¢ ç¼–è¯‘ RDT (LoRA éƒ¨åˆ†å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´ç¼–è¯‘)
    # print("ğŸš€ Compiling RDT...")
    # try:
    #     rdt_wrapper.rdt_model = torch.compile(rdt_wrapper.rdt_model)
    # except Exception as e:
    #     print(f"âš ï¸ RDT compilation failed: {e}")


    # ä¼˜åŒ–å™¨é…ç½®ï¼šRDT å­¦ä¹ ç‡ç¨é«˜ï¼ŒEncoder å­¦ä¹ ç‡æä½ (å¾®è°ƒ)
    params = [
        {'params': filter(lambda p: p.requires_grad, rdt_wrapper.parameters()), 'lr': 1e-4},
        {'params': filter(lambda p: p.requires_grad, fusion_encoder.parameters()), 'lr': 1e-5}
    ]
    optimizer = optim.AdamW(params, weight_decay=1e-4)
    noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2", prediction_type="sample")
    distill_fn = DistillationLoss()

    # 3. æ•°æ®åŠ è½½
    print(f"Loading Dataset from {args.data_root}")
    # ğŸŸ¢ [å…³é”®ä¿®æ”¹] window_size å¿…é¡»æ”¹ä¸º 6ï¼Œä¸ Stage B ä¿æŒä¸€è‡´ï¼
    dataset = RobotDataset(
        hdf5_path=args.data_root, 
        window_size=6,             # <--- Modified: Match Stage B
        pred_horizon=args.pred_horizon, 
        stats_path=STATS_PATH
    )
    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)

    # 4. ç»­è®­é€»è¾‘
    global_step = 0
    start_epoch = 0
    resume_batch_idx = 0
    if args.resume_from_checkpoint and os.path.exists(args.resume_from_checkpoint):
        print(f"ğŸ”„ Resuming from: {args.resume_from_checkpoint}")
        checkpoint = torch.load(args.resume_from_checkpoint, map_location=device)
        if 'rdt_state_dict' in checkpoint: rdt_wrapper.load_state_dict(checkpoint['rdt_state_dict'], strict=False)
        if 'encoder_state_dict' in checkpoint: fusion_encoder.load_state_dict(checkpoint['encoder_state_dict'], strict=False)
        if 'optimizer_state_dict' in checkpoint: optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if 'global_step' in checkpoint:
            global_step = checkpoint['global_step']
            total_physical_batches = global_step * args.gradient_accumulation_steps
            start_epoch = total_physical_batches // len(loader)
            resume_batch_idx = total_physical_batches % len(loader)

    # 5. è®­ç»ƒå¾ªç¯
    print(">>> Training Started <<<")
    
    # æ— é™ Epoch å¾ªç¯ï¼Œç”± max_train_steps ç»ˆæ­¢
    total_epochs = 999999 
    
    for epoch in range(start_epoch, total_epochs):
        rdt_wrapper.train()
        
        for i, batch in enumerate(loader):
            if epoch == start_epoch and i < resume_batch_idx: continue

            # æ•°æ®æ¬è¿
            video = batch['video'].to(device, non_blocking=True) # [B, 3, 6, H, W]
            state = batch['state'].to(device, non_blocking=True)
            text = batch['text_tokens'].to(device, non_blocking=True)
            ff = batch['first_frame'].to(device, non_blocking=True)
            actions = batch['action_target'].to(device, non_blocking=True)
            
            # ğŸŸ¢ [ForeSight] æœªæ¥ç›®æ ‡
            future_exo_target = batch['future_exo_target'].to(device, non_blocking=True)

            # Teacher Features (Distillation)
            real_siglip = batch['teacher_siglip'].to(device, non_blocking=True)
            real_exo = batch['teacher_exo'].to(device, non_blocking=True)
            siglip_target = torch.mean(real_siglip, dim=1)
            exo_target = torch.mean(real_exo, dim=1)
            teacher_feats = {"siglip_features": siglip_target, "exo_features": exo_target}

            # Modality Dropout (éšæœº Mask æ¨¡æ‹Ÿæ¨ç†æ—¶çš„ä¸ç¡®å®šæ€§)
            # rand_val = torch.rand(1).item()
            # video_input = video.clone()
            # ff_input = ff.clone()
            
            # if rand_val < 0.7: 
            #     video_input[:, 0] = 0.0 # Mask Main Camera
            #     ff_input[:, 0] = 0.0
            # elif rand_val < 0.8: 
            #     video_input[:, 1] = 0.0 # Mask Wrist Camera
            #     ff_input[:, 1] = 0.0
            rand_val = torch.rand(1).item()
            mask_type = "Wrist_Only" # é»˜è®¤çŠ¶æ€
            
            # video_input = video.clone()
            # ff_input = ff.clone()
            
            # # ç­–ç•¥ï¼š90% çš„æ—¶é—´å®Œå…¨ Mask æ‰ Main View
            # # ç†ç”±ï¼šæ¨ç†æ—¶ä½ åªæœ‰ Wristã€‚å¦‚æœè®­ç»ƒæ—¶è®©å®ƒçœ‹åˆ° Mainï¼Œå®ƒå°±ä¼šä¾èµ– Mainã€‚
            # # å¿…é¡»æŠŠå®ƒé€¼åˆ°â€œåªèƒ½é  Wrist + Latentâ€æ¥å†³ç­–çš„ç»å¢ƒã€‚
            # if rand_val < 1.01:
            #     video_input[:, 0] = 0.0
            #     ff_input[:, 0] = 0.0
            #     mask_type = "Simulate_Inference"
            
            # # å‰©ä¸‹ 10%ï¼šTeacher Guidance (å…¨å¯è§)
            # # ä»…ç”¨äºç»´æŒ Encoder çš„ç‰¹å¾ç¨³å®šæ€§ï¼Œä¸è®©å®ƒå½»åº•é—å¿˜ Stage B å­¦åˆ°çš„å…¨å›¾ç‰¹å¾ã€‚
            # else:
            #     mask_type = "Teacher_Guidance"



            rand_val = torch.rand(1).item()
            
            video_input = video.clone()
            ff_input = ff.clone()
            
            if rand_val < 0.5:
                # [Mode A: Inference Simulation] (50%)
                # æ¨¡æ‹ŸçœŸå®æ¨ç†ï¼šMain Camera ä¸¢å¤±ï¼Œåªæœ‰ Wrist Camera
                # ç›®çš„ï¼šé€‚åº”éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒ
                video_input[:, 0] = 0.0 # Mask Main
                ff_input[:, 0] = 0.0    # Mask First Frame Main
                mask_type = "Inference_Mode (Wrist Only)"
                
            elif rand_val < 0.8:
                # [Mode B: Total Blindness] (30%)
                # æ¨¡æ‹Ÿå…¨ç›²ï¼šMain + Wrist å…¨éƒ¨ä¸¢å¤±
                # ç›®çš„ï¼šå¼ºè¿«æ¨¡å‹å¿…é¡»ä¾èµ– State (Proprioception)
                # æ­¤æ—¶ Encoder è¾“å‡ºçš„ e_t å‡ ä¹æ²¡æœ‰è§†è§‰ä¿¡æ¯ï¼ŒAction ç”Ÿæˆå…¨é  State Injection
                video_input[:] = 0.0 
                ff_input[:] = 0.0
                mask_type = "Blind_Mode (State Only)"
                
            else:
                # [Mode C: Teacher Guidance] (20%)
                # å…¨å¯è§ï¼šMain + Wrist éƒ½æœ‰
                # ç›®çš„ï¼šç»´æŒ VideoMAE çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼Œå¹¶æä¾›è¯­ä¹‰é”šç‚¹
                mask_type = "Teacher_Mode (Full View)"


            CONSISTENCY_FREQ = 5


            with autocast('cuda', dtype=torch.bfloat16):
                # 1. Encoder Forward
                # è¿™é‡Œçš„ out åŒ…å« 'e_t' (70 tokens) å’Œ 'wm_latents' (6 latents)
                encoder_out = fusion_encoder(video_input, text, state, ff_input)
                
                e_t = encoder_out['e_t']         # [B, 70, 768] -> ç»™ RDT
                wm_pred = encoder_out['wm_latents'] # [B, 6, 1152] -> ç»™ WM Loss
                
                # 2. RDT Forward (Action Generation)
                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (actions.shape[0],), device=device).long()
                noise = torch.randn_like(actions)
                noisy_actions = noise_scheduler.add_noise(actions, noise, timesteps)
                
                # Condition ä¼ å…¥ e_t å’Œ å½“å‰ state
                conditions = {"e_t": e_t, "state": state[:, -1, :]}
                pred_noise = rdt_wrapper(noisy_actions, timesteps, conditions)
                
                # --- Loss Calculation ---
                
                # Loss 1: Action Diffusion Loss
                loss_diff = F.mse_loss(pred_noise, noise)
                # # ğŸŸ¢ [ä¿®æ”¹] ç¨€ç–è®¡ç®— Consistency Loss
                # if global_step % CONSISTENCY_FREQ == 0:
                #     loss_cons = compute_consistency_loss(fusion_encoder, batch, device)
                # else:
                #     loss_cons = torch.tensor(0.0, device=device, requires_grad=True)
                # âœ… ä¿®æ”¹åï¼šç›´æ¥ç¦ç”¨ï¼
                # æˆ‘ä»¬ä¸å¸Œæœ›æ¨¡å‹åœ¨â€œçœ‹ä¸è§â€çš„æ—¶å€™å»ççŒœâ€œçœ‹å¾—è§â€çš„ç‰¹å¾ï¼Œè¿™ä¼šå¯¼è‡´å®ƒäº§ç”Ÿå¹»è§‰ã€‚
                loss_cons = torch.tensor(0.0, device=device, requires_grad=True)
                
                # Loss 2: ğŸŸ¢ [ForeSight] World Model Loss (MSE + Cosine)
                # å¿…é¡»ä¸ Stage B ä¿æŒä¸€è‡´ï¼Œé˜²æ­¢å¾®è°ƒæ—¶ç ´å Latent ç»“æ„
                l_wm_mse = F.mse_loss(wm_pred, future_exo_target)
                
                wm_pred_norm = F.normalize(wm_pred, dim=-1)
                target_norm = F.normalize(future_exo_target, dim=-1)
                l_wm_cos = (1.0 - (wm_pred_norm * target_norm).sum(dim=-1)).mean()
                
                loss_wm = l_wm_mse + 0.5 * l_wm_cos
                
                # Loss 3: Regularization (Consistency & Distill)
                loss_cons = compute_consistency_loss(fusion_encoder, batch, device)
                loss_distill_reg, _ = distill_fn(encoder_out, teacher_feats)
                
                # ğŸŒŸ æ€» Loss
                # Diff: 1.0 (ä¸»ä»»åŠ¡)
                # WM: 0.5 (å¼ºçº¦æŸï¼Œä¿æŒé¢„æµ‹èƒ½åŠ›)
                # Cons: 0.1 (è¾…åŠ©)
                # Distill: 0.05 (é˜²æ¼‚ç§»)
                total_loss = loss_diff + 0.5 * loss_wm + 0.1 * loss_cons + 0.05 * loss_distill_reg
                total_loss = total_loss / args.gradient_accumulation_steps

            total_loss.backward()

            if (i + 1) % args.gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(rdt_wrapper.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()
                global_step += 1
                
                if global_step % 10 == 0:
                    real_loss = total_loss.item() * args.gradient_accumulation_steps
                    print(f"Step {global_step} | L: {real_loss:.4f} | Act: {loss_diff.item():.4f} | WM: {loss_wm.item():.4f} (Cos:{l_wm_cos.item():.3f})")
                    
                    tb_writer.add_scalar('Train/Total_Loss', real_loss, global_step)
                    if args.use_wandb and HAS_WANDB:
                        wandb.log({
                            "total_loss": real_loss,
                            "action_loss": loss_diff.item(),
                            "wm_loss": loss_wm.item(),
                            "wm_cos": l_wm_cos.item(),
                            "cons_loss": loss_cons.item(),
                            "global_step": global_step,
                            "epoch": epoch
                        }, step=global_step)

                # Checkpoint
                if global_step % args.checkpointing_steps == 0:
                    save_path = os.path.join(args.output_dir, f"StageC_ForeSight_step_{global_step}.pt")
                    torch.save({
                        'epoch': epoch,
                        'global_step': global_step, 
                        'rdt_state_dict': rdt_wrapper.state_dict(),
                        'encoder_state_dict': fusion_encoder.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'pred_horizon': args.pred_horizon
                    }, save_path)
                    print(f"ğŸ’¾ Checkpoint saved: {save_path}")

                if global_step >= args.max_train_steps:
                    print(f"ğŸ‰ Training Finished.")
                    final_path = os.path.join(args.output_dir, f"StageC_ForeSight_final.pt")
                    torch.save({
                        'epoch': epoch,
                        'global_step': global_step,
                        'rdt_state_dict': rdt_wrapper.state_dict(),
                        'encoder_state_dict': fusion_encoder.state_dict()
                    }, final_path)
                    tb_writer.close()
                    if args.use_wandb and HAS_WANDB: wandb.finish()
                    return 

    tb_writer.close()
    if args.use_wandb and HAS_WANDB: wandb.finish()

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    # é»˜è®¤å‚æ•°ä»…ä¾›å‚è€ƒï¼Œå»ºè®®é€šè¿‡ shell è„šæœ¬ä¼ å…¥
    parser.add_argument('--data_root', type=str, default='/yanghaochuan/data/hdf5/pick_up_the_orange_ball_and_put_it_on_the_plank.hdf5')
    parser.add_argument('--output_dir', type=str, default='/yanghaochuan/121checkpoints_finetune')
    # é»˜è®¤åŠ è½½ Stage B (ForeSight Pretrained)
    parser.add_argument('--stage_b_ckpt', type=str, default='/yanghaochuan/checkpoints/120StageB_ForeSight_step_2500.pt')
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--pred_horizon', type=int, default=64)
    parser.add_argument('--gradient_accumulation_steps', type=int, default=2)
    parser.add_argument('--max_train_steps', type=int, default=10000)
    parser.add_argument('--checkpointing_steps', type=int, default=500)
    parser.add_argument('--resume_from_checkpoint', type=str, default=None)
    parser.add_argument('--use_wandb', action='store_true', default=False)
    args = parser.parse_args()
    train_stage_c(args)