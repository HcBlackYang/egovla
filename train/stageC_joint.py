import sys
import os
import torch
import torch.optim as optim
import argparse
import time
import numpy as np
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.amp import autocast
from diffusers import DDPMScheduler
from peft import LoraConfig, get_peft_model
from torch.utils.tensorboard import SummaryWriter 

# === [ç¯å¢ƒæ£€æŸ¥] WandB ===
try:
    import wandb
    HAS_WANDB = True
except ImportError:
    HAS_WANDB = False
    print("âš ï¸ WandB not found. Install with `pip install wandb` for better visualization.")

# === æ€§èƒ½ä¼˜åŒ– ===
torch.backends.cuda.enable_flash_sdp(True)
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_math_sdp(False)

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model.fusion_encoder import FusionEncoder
from model.rdt_model import RDTWrapper
from utils.dataset_loader import RobotDataset
from losses.consistency_loss import compute_consistency_loss

# === è·¯å¾„é…ç½® (è¯·ç¡®ä¿è¿™äº›è·¯å¾„æ­£ç¡®) ===
VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
RDT_PATH = '/yanghaochuan/models/rdt-1b'
STATS_PATH = '/yanghaochuan/data/1223dataset_stats.json'

def train_stage_c(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ====================================================
    # 0. åˆå§‹åŒ–æ—¥å¿—
    # ====================================================
    log_dir = os.path.join(args.output_dir, "logs")
    os.makedirs(log_dir, exist_ok=True)
    tb_writer = SummaryWriter(log_dir=log_dir)
    
    if args.use_wandb and HAS_WANDB:
        wandb.init(
            project="RDT-StageC-Joint",
            # åå­—é‡Œå¸¦ä¸Š step å’Œ accï¼Œæ–¹ä¾¿åŒºåˆ†å®éªŒ
            name=f"step{args.max_train_steps}_acc{args.gradient_accumulation_steps}_{int(time.time())}",
            config=vars(args),
            resume="allow"
        )
    
    print(f"=== Stage C Joint Training (Step-Based) ===")
    print(f"ğŸ¯ Target: {args.max_train_steps} Global Steps")
    print(f"ğŸ“¦ Physical Batch Size: {args.batch_size}")
    print(f"ğŸ”‹ Gradient Accumulation: {args.gradient_accumulation_steps}")
    print(f"ğŸ”¥ Effective Batch Size: {args.batch_size * args.gradient_accumulation_steps}")

    # ====================================================
    # 1. æ¨¡å‹åŠ è½½
    # ====================================================
    print("Loading Models...")
    fusion_encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152, rdt_dim=768).to(device)
    
    # åŠ è½½ Stage B (å¦‚æœæœ‰)
    if args.stage_b_ckpt and os.path.exists(args.stage_b_ckpt):
        print(f"Loading Stage B: {args.stage_b_ckpt}")
        ckpt = torch.load(args.stage_b_ckpt, map_location='cpu')
        state_dict = ckpt['encoder_state_dict'] if 'encoder_state_dict' in ckpt else ckpt
        fusion_encoder.load_state_dict(state_dict, strict=False)
    
    # å†»ç»“ VideoMAEï¼Œåªå¾®è°ƒ Adapter
    fusion_encoder.eval() 
    for param in fusion_encoder.parameters(): param.requires_grad = True 
    for param in fusion_encoder.backbone.parameters(): param.requires_grad = False
    if fusion_encoder.text_encoder:
        for p in fusion_encoder.text_encoder.parameters(): p.requires_grad = False

    # åŠ è½½ RDT
    rdt_wrapper = RDTWrapper(action_dim=8, model_path=RDT_PATH, pred_horizon=args.pred_horizon).to(device)
    
    # RDT æƒé‡åˆ‡ç‰‡åŠ è½½é€»è¾‘ (ä¿ç•™ä½ ä¹‹å‰çš„é€»è¾‘)
    if os.path.exists(RDT_PATH) or os.path.exists(os.path.join(RDT_PATH, "pytorch_model.bin")):
        rdt_file = RDT_PATH if os.path.isfile(RDT_PATH) else os.path.join(RDT_PATH, "pytorch_model.bin")
        if os.path.exists(rdt_file):
            print("Loading RDT weights with auto-slicing...")
            state_dict = torch.load(rdt_file, map_location='cpu')
            if 'x_pos_embed' in state_dict:
                ckpt_pos = state_dict['x_pos_embed']
                curr_pos = rdt_wrapper.rdt_model.x_pos_embed
                if ckpt_pos.shape != curr_pos.shape:
                    print(f"âœ‚ï¸ Slicing position embed: {ckpt_pos.shape} -> {curr_pos.shape}")
                    state_dict['x_pos_embed'] = ckpt_pos[:, :curr_pos.shape[1], :]
            rdt_wrapper.rdt_model.load_state_dict(state_dict, strict=False)

    # LoRA é…ç½®
    print("Applying LoRA...")
    peft_config = LoraConfig(
        r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], 
        lora_dropout=0.05, bias="none"
    )
    rdt_wrapper.rdt_model = get_peft_model(rdt_wrapper.rdt_model, peft_config)
    
    # ====================================================
    # 2. ä¼˜åŒ–å™¨ & è°ƒåº¦å™¨
    # ====================================================
    params = [
        {'params': filter(lambda p: p.requires_grad, rdt_wrapper.parameters()), 'lr': 1e-4},
        {'params': filter(lambda p: p.requires_grad, fusion_encoder.parameters()), 'lr': 1e-5}
    ]
    optimizer = optim.AdamW(params, weight_decay=1e-4)
    
    # âš ï¸ è®­ç»ƒæ—¶ä¿æŒ 1000 æ­¥ï¼Œä¸è¦æ”¹è¿™é‡Œï¼æ¨ç†æ‰ç”¨ 25 æ­¥ã€‚
    noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2", prediction_type="sample")

    # ====================================================
    # 3. æ•°æ®åŠ è½½
    # ====================================================
    print(f"Loading Dataset from {args.data_root}")
    dataset = RobotDataset(hdf5_path=args.data_root, window_size=16, pred_horizon=args.pred_horizon, stats_path=STATS_PATH)
    # drop_last=True é˜²æ­¢æœ€åä¸€ä¸ª batch åªæœ‰å‡ ä¸ªæ ·æœ¬å¯¼è‡´æ¢¯åº¦ä¸ç¨³å®š
    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)

    # ====================================================
    # 4. ç²¾å‡†æ–­ç‚¹ç»­è®­é€»è¾‘
    # ====================================================
    global_step = 0
    start_epoch = 0
    resume_batch_idx = 0

    if args.resume_from_checkpoint and os.path.exists(args.resume_from_checkpoint):
        print(f"ğŸ”„ Resuming from: {args.resume_from_checkpoint}")
        checkpoint = torch.load(args.resume_from_checkpoint, map_location=device)
        
        # æ¢å¤æƒé‡
        if 'rdt_state_dict' in checkpoint: rdt_wrapper.load_state_dict(checkpoint['rdt_state_dict'], strict=False)
        if 'encoder_state_dict' in checkpoint: fusion_encoder.load_state_dict(checkpoint['encoder_state_dict'], strict=False)
        if 'optimizer_state_dict' in checkpoint: optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # æ¢å¤æ­¥æ•°
        if 'global_step' in checkpoint:
            global_step = checkpoint['global_step']
            print(f"   -> Found Global Step: {global_step}")
            
            # è®¡ç®—æˆ‘ä»¬éœ€è¦è·³è¿‡å¤šå°‘ä¸ªç‰©ç† Batch
            # å…¬å¼ï¼šæ€»ç‰©ç†Batchæ•° = Global Step * æ¢¯åº¦ç´¯ç§¯æ¬¡æ•°
            total_physical_batches = global_step * args.gradient_accumulation_steps
            
            # è®¡ç®—å½“å‰ Epoch å’Œ Batch ç´¢å¼•
            start_epoch = total_physical_batches // len(loader)
            resume_batch_idx = total_physical_batches % len(loader)
            
            print(f"   -> Resume Location: Epoch {start_epoch}, Batch {resume_batch_idx}")
            print(f"   -> Ready to continue towards {args.max_train_steps} steps.")
        
        elif 'epoch' in checkpoint:
            # å…¼å®¹æ—§é€»è¾‘
            start_epoch = checkpoint['epoch'] + 1
            global_step = start_epoch * len(loader) // args.gradient_accumulation_steps
            print(f"   -> Resuming from Epoch {start_epoch} (Approx. Step {global_step})")

    # ====================================================
    # 5. è®­ç»ƒå¾ªç¯ (16x4=64 Logic)
    # ====================================================
    print(">>> Training Started <<<")
    
    # è¿™æ˜¯ä¸€ä¸ªè¶³å¤Ÿå¤§çš„æ•°å­—ï¼Œç¡®ä¿å¾ªç¯åªç”± max_train_steps ç»ˆæ­¢
    total_epochs = 999999 
    
    for epoch in range(start_epoch, total_epochs):
        rdt_wrapper.train()
        
        for i, batch in enumerate(loader):
            # â© è·³è¿‡å·²è®­ç»ƒçš„æ•°æ® (ç²¾ç¡®ç»­è®­)
            if epoch == start_epoch and i < resume_batch_idx:
                if i % 50 == 0: print(f"â© Skipping batch {i}/{len(loader)}...", end='\r')
                continue

            # --- æ•°æ®å‡†å¤‡ ---
            video = batch['video'].to(device, non_blocking=True)
            state = batch['state'].to(device, non_blocking=True)
            text = batch['text_tokens'].to(device, non_blocking=True)
            ff = batch['first_frame'].to(device, non_blocking=True)
            actions = batch['action_target'].to(device, non_blocking=True)

            # Modality Dropout
            rand_val = torch.rand(1).item()
            mask_type = "None"
            if rand_val < 0.7: 
                video[:, 0] = 0.0
                mask_type = "Main_Masked"
            elif rand_val < 0.8: 
                video[:, 1] = 0.0
                mask_type = "Wrist_Masked"
            
            # --- å‰å‘ä¼ æ’­ ---
            with autocast('cuda', dtype=torch.bfloat16):
                # 1. Encode
                e_t = fusion_encoder(video, text, state, ff)['e_t']
                
                # 2. Noise & Target
                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (actions.shape[0],), device=device).long()
                noise = torch.randn_like(actions)
                noisy_actions = noise_scheduler.add_noise(actions, noise, timesteps)
                
                # 3. Predict
                conditions = {"e_t": e_t, "state": state[:, -1, :]}
                pred_noise = rdt_wrapper(noisy_actions, timesteps, conditions)
                
                # 4. Loss Calculation
                loss_diff = F.mse_loss(pred_noise, noise)
                loss_cons = compute_consistency_loss(fusion_encoder, batch, device)
                total_loss = loss_diff + 0.1 * loss_cons
                
                # ğŸŒŸ [å…³é”®ç‚¹ 1] æ¢¯åº¦ç´¯ç§¯å½’ä¸€åŒ–
                # å› ä¸º backward ä¼šç´¯åŠ æ¢¯åº¦ï¼Œæ‰€ä»¥ Loss è¦é™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼Œä¿è¯å¹³å‡æ¢¯åº¦å¹…åº¦ä¸å˜
                total_loss = total_loss / args.gradient_accumulation_steps

            # --- åå‘ä¼ æ’­ ---
            total_loss.backward()

            # ğŸŒŸ [å…³é”®ç‚¹ 2] å‚æ•°æ›´æ–°é€»è¾‘
            # åªæœ‰å½“ç´¯ç§¯äº†è¶³å¤Ÿçš„æ­¥æ•°ï¼Œæˆ–è€… Epoch ç»“æŸæ—¶ï¼Œæ‰æ›´æ–°å‚æ•°
            if (i + 1) % args.gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(rdt_wrapper.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()
                
                # çœŸæ­£çš„ Step å¢åŠ äº† (Update Step)
                global_step += 1
                
                # --- æ—¥å¿—è®°å½• (è¿˜åŸ Loss æ•°å€¼æ–¹ä¾¿è§‚å¯Ÿ) ---
                if global_step % 10 == 0:
                    real_loss = total_loss.item() * args.gradient_accumulation_steps
                    real_diff = loss_diff.item()
                    real_cons = loss_cons.item()
                    
                    print(f"Step [{global_step}/{args.max_train_steps}] Loss: {real_loss:.4f} (Ep {epoch})")
                    
                    tb_writer.add_scalar('Train/Total_Loss', real_loss, global_step)
                    if args.use_wandb and HAS_WANDB:
                        wandb.log({
                            "total_loss": real_loss,
                            "diff_loss": real_diff,
                            "cons_loss": real_cons,
                            "global_step": global_step,
                            "epoch": epoch
                        }, step=global_step)

                # --- è§†é¢‘å¯è§†åŒ– (æ¯ 500 æ­¥) ---
                if global_step % 500 == 0 and args.use_wandb and HAS_WANDB:
                    try:
                        vid_sample = video[0].float().cpu().numpy() 
                        main_view = np.transpose(vid_sample[0], (1, 0, 2, 3))
                        wrist_view = np.transpose(vid_sample[1], (1, 0, 2, 3))
                        combined_view = np.concatenate([main_view, wrist_view], axis=3) 
                        wandb.log({
                            "input_video": wandb.Video((combined_view * 255).astype(np.uint8), fps=4, format="gif", caption=f"S{global_step}: {mask_type}")
                        }, step=global_step)
                    except: pass

                # --- ğŸ’¾ é˜¶æ®µæ€§ä¿å­˜ (Checkpointing) ---
                if global_step % args.checkpointing_steps == 0:
                    save_path = os.path.join(args.output_dir, f"checkpoint_step_{global_step}.pt")
                    torch.save({
                        'epoch': epoch,
                        'global_step': global_step, # ä¿å­˜å½“å‰ Global Step
                        'rdt_state_dict': rdt_wrapper.state_dict(),
                        'encoder_state_dict': fusion_encoder.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'pred_horizon': args.pred_horizon
                    }, save_path)
                    print(f"ğŸ’¾ Checkpoint saved: {save_path}")

                # --- ğŸ›‘ åœæ­¢è®­ç»ƒ ---
                if global_step >= args.max_train_steps:
                    print(f"ğŸ‰ Reached target {args.max_train_steps} steps. Training Finished.")
                    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
                    final_path = os.path.join(args.output_dir, f"checkpoint_final_{global_step}.pt")
                    torch.save({
                        'epoch': epoch,
                        'global_step': global_step,
                        'rdt_state_dict': rdt_wrapper.state_dict(),
                        'encoder_state_dict': fusion_encoder.state_dict()
                    }, final_path)
                    tb_writer.close()
                    if args.use_wandb and HAS_WANDB: wandb.finish()
                    return # é€€å‡ºå‡½æ•°ï¼Œç»“æŸè„šæœ¬

    tb_writer.close()
    if args.use_wandb and HAS_WANDB: wandb.finish()

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_root', type=str, default='/yanghaochuan/data/1223pick_up_the_paper_cup.hdf5')
    parser.add_argument('--output_dir', type=str, default='/yanghaochuan/1225checkpoints')
    parser.add_argument('--stage_b_ckpt', type=str, default='/yanghaochuan/checkpoints/1223stageB_papercup.pt')
    
    # ç‰©ç† Batch Size (æ˜¾å­˜é™åˆ¶ï¼Œä¿æŒ 16)
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--pred_horizon', type=int, default=64)
    
    # === å…³é”®æ§åˆ¶å‚æ•° ===
    # æ¢¯åº¦ç´¯ç§¯ï¼šè®¾ä¸º 4ï¼Œä½¿å¾— Effective Batch Size = 16 * 4 = 64
    parser.add_argument('--gradient_accumulation_steps', type=int, default=4, 
                        help="Number of updates steps to accumulate before update pass. (Effective BS = batch_size * this)")
    
    # ç›®æ ‡æ€»æ­¥æ•° (Update Steps)
    parser.add_argument('--max_train_steps', type=int, default=10000, 
                        help="Total number of training steps (parameter updates) to perform.")
    
    # æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡
    parser.add_argument('--checkpointing_steps', type=int, default=500, 
                        help="Save checkpoint every X updates.")
    
    # ç»­è®­
    parser.add_argument('--resume_from_checkpoint', type=str, default=None)
    parser.add_argument('--use_wandb', action='store_true', default=False)
    
    args = parser.parse_args()
    train_stage_c(args)