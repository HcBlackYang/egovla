# # train/stageC_joint.py
# import sys
# import os
# import torch
# import torch.optim as optim
# import argparse
# import time
# import torch.nn.functional as F
# from torch.utils.data import DataLoader
# from torch.amp import autocast
# from diffusers import DDPMScheduler
# from peft import LoraConfig, get_peft_model
# # === [æ–°å¢] å¼ºåˆ¶å¼€å¯ Flash Attention (A100 å¿…å¤‡) ===
# torch.backends.cuda.enable_flash_sdp(True)
# torch.backends.cuda.enable_mem_efficient_sdp(True)
# torch.backends.cuda.enable_math_sdp(False) # ç¦æ­¢æ™®é€šæ•°å­¦ Attentionï¼Œé˜²æ­¢ OOM
# print(f"Flash Attention Enabled: {torch.backends.cuda.flash_sdp_enabled()}")
# # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥ models
# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# from model.fusion_encoder import FusionEncoder
# from model.rdt_model import RDTWrapper
# from utils.dataset_loader import RobotDataset
# from losses.consistency_loss import compute_consistency_loss

# # === è·¯å¾„é…ç½® (è¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´) ===
# VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
# RDT_PATH = '/yanghaochuan/models/rdt-1b'
# STATS_PATH = '/yanghaochuan/data/1223dataset_stats.json'

# def train_stage_c(args):
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     print(f"=== Stage C Joint Training (End-to-End) ===")
    
#     # ====================================================
#     # 1. åˆå§‹åŒ– Fusion Encoder
#     # ====================================================
#     print("Loading Fusion Encoder...")
#     # rdt_dim=768 å¿…é¡»ä¸ rdt_model.py ä¸­ visual_proj çš„è¾“å…¥ç»´åº¦ä¸€è‡´
#     fusion_encoder = FusionEncoder(
#         backbone_path=VIDEO_MAE_PATH, 
#         teacher_dim=1152,
#         rdt_dim=768 
#     ).to(device)
    
#     # åŠ è½½ Stage B é¢„è®­ç»ƒæƒé‡ (å¦‚æœæœ‰)
#     if args.stage_b_ckpt and os.path.exists(args.stage_b_ckpt):
#         print(f"Loading Stage B Checkpoint: {args.stage_b_ckpt}")
#         try:
#             ckpt = torch.load(args.stage_b_ckpt, map_location='cpu')
#             # å…¼å®¹ä¿å­˜æ—¶å¯èƒ½åªä¿å­˜äº† encoder_state_dict æˆ– æ•´ä¸ª dict çš„æƒ…å†µ
#             state_dict = ckpt['encoder_state_dict'] if 'encoder_state_dict' in ckpt else ckpt
#             fusion_encoder.load_state_dict(state_dict, strict=False)
#             print("âœ… Stage B weights loaded successfully.")
#         except Exception as e:
#             print(f"âš ï¸ Failed to load Stage B weights: {e}")
    
#     # å†»ç»“ VideoMAE Backboneï¼Œåªè®­ç»ƒ Adapter å’Œ Heads
#     fusion_encoder.eval() 
#     for param in fusion_encoder.parameters(): param.requires_grad = True 
#     for param in fusion_encoder.backbone.parameters(): param.requires_grad = False
#     if fusion_encoder.text_encoder:
#         for param in fusion_encoder.text_encoder.parameters(): param.requires_grad = False

#     # ====================================================
#     # 2. åˆå§‹åŒ– RDT Policy (Wrapper)
#     # ====================================================
#     print("Loading RDT Policy...")
#     rdt_wrapper = RDTWrapper(
#         action_dim=8, 
#         model_path=RDT_PATH, 
#         pred_horizon=args.pred_horizon
#     ).to(device)
    
#     # ====================================================
#     # 3. é…ç½® LoRA
#     # ====================================================
#     print("Applying LoRA to RDT...")
#     peft_config = LoraConfig(
#         r=16, 
#         lora_alpha=32,
#         target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], 
#         lora_dropout=0.05, 
#         bias="none"
#     )
#     # å°† LoRA æŒ‚è½½åˆ° RDT å†…éƒ¨çš„ Transformer ä¸Š
#     rdt_wrapper.rdt_model = get_peft_model(rdt_wrapper.rdt_model, peft_config)
#     rdt_wrapper.rdt_model.print_trainable_parameters()
    
#     # ====================================================
#     # 4. ä¼˜åŒ–å™¨ & è°ƒåº¦å™¨
#     # ====================================================
#     # è”åˆè®­ç»ƒï¼šåŒæ—¶ä¼˜åŒ– RDT çš„ LoRA å‚æ•° å’Œ FusionEncoder çš„ Adapter å‚æ•°
#     params_to_optimize = [
#         {'params': filter(lambda p: p.requires_grad, rdt_wrapper.parameters()), 'lr': 1e-4},
#         {'params': filter(lambda p: p.requires_grad, fusion_encoder.parameters()), 'lr': 1e-5}
#     ]
#     optimizer = optim.AdamW(params_to_optimize, weight_decay=1e-4)
    
#     # å™ªå£°è°ƒåº¦å™¨
#     noise_scheduler = DDPMScheduler(
#         num_train_timesteps=1000, 
#         beta_schedule="squaredcos_cap_v2", 
#         prediction_type="sample"
#     )

#     # ====================================================
#     # 5. æ•°æ®åŠ è½½
#     # ====================================================
#     print(f"Loading Dataset from {args.data_root}")
#     if not os.path.exists(args.data_root):
#         raise FileNotFoundError(f"Data file not found: {args.data_root}")

#     dataset = RobotDataset(
#         hdf5_path=args.data_root, 
#         window_size=16, 
#         pred_horizon=args.pred_horizon, 
#         stats_path=STATS_PATH
#     )
    
#     loader = DataLoader(
#         dataset, 
#         batch_size=args.batch_size, 
#         shuffle=True, 
#         num_workers=8, 
#         pin_memory=True, 
#         drop_last=True
#     )

#     # ====================================================
#     # 6. è®­ç»ƒå¾ªç¯
#     # ====================================================
#     rdt_wrapper.train()
#     fusion_encoder.train()
    
#     print(">>> Training Started... <<<")
    
#     for epoch in range(args.epochs):
#         total_loss = 0
#         start_time = time.time()
        
#         for i, batch in enumerate(loader):
#             # è·å–æ•°æ®
#             # video shape: [B, 2, 3, 16, H, W] (View 0=Main, View 1=Wrist)
#             video = batch['video'].to(device, non_blocking=True)
#             state = batch['state'].to(device, non_blocking=True) # [B, 16, 8]
#             text = batch['text_tokens'].to(device, non_blocking=True)
#             ff = batch['first_frame'].to(device, non_blocking=True)
#             actions = batch['action_target'].to(device, non_blocking=True)

#             # === Modality Dropout ===
#             # # 50% æ¦‚ç‡å°† Main Camera (View 0) å…¨é»‘ï¼Œå¼ºè¿«æ¨¡å‹å­¦ä¼šçœ‹ Wrist å’Œ å¬æŒ‡ä»¤
#             # if torch.rand(1) < 0.5:
#             #      video[:, 0] = 0.0 
#             rand_val = torch.rand(1).item()
#             if rand_val < 0.7:
#                  video[:, 0] = 0.0  # Mask Main View
            
#             # ç­–ç•¥ B: 10% çš„æ—¶é—´æŠŠæ‰‹è…•æŠ¹é»‘ (é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¯é€‰)
#             elif rand_val < 0.8:
#                  video[:, 1] = 0.0
            
#             optimizer.zero_grad()
            
#             with autocast('cuda', dtype=torch.bfloat16):
#                 # -------------------------
#                 # A. æå–è§†è§‰ç‰¹å¾
#                 # -------------------------
#                 encoder_outputs = fusion_encoder(video, text, state, ff)
#                 e_t = encoder_outputs['e_t'] # [B, 64, 768]
                
#                 # -------------------------
#                 # B. è®¡ç®— Diffusion Loss
#                 # -------------------------
#                 # éšæœºé‡‡æ ·æ—¶é—´æ­¥
#                 timesteps = torch.randint(
#                     0, noise_scheduler.config.num_train_timesteps, 
#                     (actions.shape[0],), device=device
#                 ).long()
                
#                 # åŠ å™ª
#                 noise = torch.randn_like(actions)
#                 noisy_actions = noise_scheduler.add_noise(actions, noise, timesteps)
                
#                 # [æ ¸å¿ƒä¿®æ”¹] æ„é€  Conditions å­—å…¸
#                 # å¿…é¡»ä¼ å…¥ 'state'ï¼ŒRDTWrapper ä¼šå°†å…¶ä½œä¸º State Token
#                 # æˆ‘ä»¬å–å†å²çª—å£çš„æœ€åä¸€å¸§ (current state) ä½œä¸ºæ¡ä»¶
#                 current_state = state[:, -1, :] # [B, 8]
                
#                 conditions = {
#                     "e_t": e_t, 
#                     "state": current_state 
#                 }
                
#                 # é¢„æµ‹å™ªå£°
#                 pred_noise = rdt_wrapper(noisy_actions, timesteps, conditions)
#                 loss_diff = torch.nn.functional.mse_loss(pred_noise, noise)
                
#                 # -------------------------
#                 # C. è®¡ç®— Consistency Loss
#                 # -------------------------
#                 # ç¡®ä¿ç‰¹å¾åœ¨å•æ‘„/åŒæ‘„æƒ…å†µä¸‹ä¿æŒä¸€è‡´
#                 loss_cons = compute_consistency_loss(fusion_encoder, batch, device)
                
#                 # æ€» Loss
#                 total_loss_step = loss_diff + 0.1 * loss_cons

#             # åå‘ä¼ æ’­
#             total_loss_step.backward()
#             torch.nn.utils.clip_grad_norm_(rdt_wrapper.parameters(), 1.0)
#             optimizer.step()
            
#             total_loss += total_loss_step.item()
            
#             # æ‰“å°æ—¥å¿—
#             if i % 10 == 0:
#                 elapsed = time.time() - start_time
#                 print(f"Epoch {epoch} [{i}/{len(loader)}] Diff: {loss_diff.item():.4f} Cons: {loss_cons.item():.4f} Total: {total_loss_step.item():.4f}")

#         # ä¿å­˜ Checkpoint
#         if epoch % 5 == 0 or epoch == args.epochs - 1:
#             os.makedirs(args.output_dir, exist_ok=True)
#             save_path = os.path.join(args.output_dir, f"1223stageC_joint_epoch_{epoch}.pt")
#             torch.save({
#                 'epoch': epoch,
#                 'rdt_state_dict': rdt_wrapper.state_dict(),     # åŒ…å« LoRA æƒé‡
#                 'encoder_state_dict': fusion_encoder.state_dict(), # åŒ…å« Adapter æƒé‡
#                 'optimizer_state_dict': optimizer.state_dict(),
#             }, save_path)
#             print(f"âœ… Saved checkpoint to {save_path}")

# if __name__ == '__main__':
#     parser = argparse.ArgumentParser()
#     parser.add_argument('--data_root', type=str, required=True, help="Path to HDF5 dataset")
#     parser.add_argument('--output_dir', type=str, default='./checkpoints', help="Directory to save checkpoints")
#     parser.add_argument('--stage_b_ckpt', type=str, default=None, help="Path to Stage B pretrained checkpoint")
#     parser.add_argument('--batch_size', type=int, default=16)
#     parser.add_argument('--epochs', type=int, default=50)
#     parser.add_argument('--pred_horizon', type=int, default=16, help="Prediction horizon for action chunking")
    
#     args = parser.parse_args()
    
#     # ç®€å•çš„å‚æ•°æ£€æŸ¥
#     if not args.stage_b_ckpt:
#         print("âš ï¸ Warning: No Stage B checkpoint provided. FusionEncoder will be initialized randomly (except Backbone).")
        
#     train_stage_c(args)

import sys
import os
import torch
import torch.optim as optim
import argparse
import time
import numpy as np
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.amp import autocast
from diffusers import DDPMScheduler
from peft import LoraConfig, get_peft_model
from torch.utils.tensorboard import SummaryWriter # === [æ–°å¢] TensorBoard

# === [æ–°å¢] WandB æ£€æŸ¥ä¸å¯¼å…¥ ===
try:
    import wandb
    HAS_WANDB = True
except ImportError:
    HAS_WANDB = False
    print("âš ï¸ WandB not found. Install with `pip install wandb` for better visualization.")

# === æ€§èƒ½ä¼˜åŒ– ===
torch.backends.cuda.enable_flash_sdp(True)
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_math_sdp(False)

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model.fusion_encoder import FusionEncoder
from model.rdt_model import RDTWrapper
from utils.dataset_loader import RobotDataset
from losses.consistency_loss import compute_consistency_loss

# === è·¯å¾„é…ç½® ===
VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
RDT_PATH = '/yanghaochuan/models/rdt-1b'
STATS_PATH = '/yanghaochuan/data/1223dataset_stats.json'

def train_stage_c(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ====================================================
    # 0. åˆå§‹åŒ–å¯è§†åŒ–è®°å½•å™¨ (TensorBoard + WandB)
    # ====================================================
    # TensorBoard æ—¥å¿—ç›®å½• (é€šå¸¸æœåŠ¡å™¨ç½‘é¡µä¼šè¯»å–è¿™ä¸ªè·¯å¾„)
    log_dir = os.path.join(args.output_dir, "logs")
    os.makedirs(log_dir, exist_ok=True)
    tb_writer = SummaryWriter(log_dir=log_dir)
    print(f"ğŸ“ˆ TensorBoard logging to: {log_dir}")
    
    # WandB åˆå§‹åŒ–
    if args.use_wandb and HAS_WANDB:
        wandb.init(
            project="RDT-StageC-Joint",
            name=f"run_horizon{args.pred_horizon}_{int(time.time())}",
            config=vars(args)
        )
        print("ğŸš€ WandB logging enabled.")
    
    print(f"=== Stage C Joint Training ===")
    
    # ====================================================
    # 1. æ¨¡å‹åŠ è½½ (FusionEncoder + RDT)
    # ====================================================
    print("Loading Models...")
    fusion_encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152, rdt_dim=768).to(device)
    
    # åŠ è½½ Stage B
    if args.stage_b_ckpt and os.path.exists(args.stage_b_ckpt):
        print(f"Loading Stage B: {args.stage_b_ckpt}")
        ckpt = torch.load(args.stage_b_ckpt, map_location='cpu')
        state_dict = ckpt['encoder_state_dict'] if 'encoder_state_dict' in ckpt else ckpt
        fusion_encoder.load_state_dict(state_dict, strict=False)
    
    # å†»ç»“ VideoMAE
    fusion_encoder.eval() 
    for param in fusion_encoder.parameters(): param.requires_grad = True 
    for param in fusion_encoder.backbone.parameters(): param.requires_grad = False
    if fusion_encoder.text_encoder:
        for p in fusion_encoder.text_encoder.parameters(): p.requires_grad = False

    # RDT Wrapper
    rdt_wrapper = RDTWrapper(action_dim=8, model_path=RDT_PATH, pred_horizon=args.pred_horizon).to(device)
    
    # RDT æƒé‡è‡ªåŠ¨åˆ‡ç‰‡åŠ è½½
    if os.path.exists(RDT_PATH) or os.path.exists(os.path.join(RDT_PATH, "pytorch_model.bin")):
        rdt_file = RDT_PATH if os.path.isfile(RDT_PATH) else os.path.join(RDT_PATH, "pytorch_model.bin")
        if os.path.exists(rdt_file):
            print("Loading RDT weights with auto-slicing...")
            state_dict = torch.load(rdt_file, map_location='cpu')
            if 'x_pos_embed' in state_dict:
                ckpt_pos = state_dict['x_pos_embed']
                curr_pos = rdt_wrapper.rdt_model.x_pos_embed
                if ckpt_pos.shape != curr_pos.shape:
                    print(f"âœ‚ï¸ Slicing position embed: {ckpt_pos.shape} -> {curr_pos.shape}")
                    state_dict['x_pos_embed'] = ckpt_pos[:, :curr_pos.shape[1], :]
            rdt_wrapper.rdt_model.load_state_dict(state_dict, strict=False)

    # LoRA
    print("Applying LoRA...")
    peft_config = LoraConfig(
        r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], 
        lora_dropout=0.05, bias="none"
    )
    rdt_wrapper.rdt_model = get_peft_model(rdt_wrapper.rdt_model, peft_config)
    
    # ====================================================
    # 2. ä¼˜åŒ–å™¨ & æ•°æ®
    # ====================================================
    params = [
        {'params': filter(lambda p: p.requires_grad, rdt_wrapper.parameters()), 'lr': 1e-4},
        {'params': filter(lambda p: p.requires_grad, fusion_encoder.parameters()), 'lr': 1e-5}
    ]
    optimizer = optim.AdamW(params, weight_decay=1e-4)
    noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2", prediction_type="sample")

    print(f"Loading Dataset from {args.data_root}")
    dataset = RobotDataset(hdf5_path=args.data_root, window_size=16, pred_horizon=args.pred_horizon, stats_path=STATS_PATH)
    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)

    # ====================================================
    # 3. è®­ç»ƒå¾ªç¯ (å¸¦å¯è§†åŒ–)
    # ====================================================
    print(">>> Training Started <<<")
    global_step = 0
    
    for epoch in range(args.epochs):
        rdt_wrapper.train()
        start_time = time.time()
        
        for i, batch in enumerate(loader):
            video = batch['video'].to(device, non_blocking=True) # [B, 2, 3, 16, H, W]
            state = batch['state'].to(device, non_blocking=True)
            text = batch['text_tokens'].to(device, non_blocking=True)
            ff = batch['first_frame'].to(device, non_blocking=True)
            actions = batch['action_target'].to(device, non_blocking=True)

            # Modality Dropout
            rand_val = torch.rand(1).item()
            mask_type = "None"
            if rand_val < 0.7:
                 video[:, 0] = 0.0  # Mask Main
                 mask_type = "Main_Masked"
            elif rand_val < 0.8:
                 video[:, 1] = 0.0  # Mask Wrist
                 mask_type = "Wrist_Masked"
            
            optimizer.zero_grad()
            
            with autocast('cuda', dtype=torch.bfloat16):
                # Forward
                e_t = fusion_encoder(video, text, state, ff)['e_t']
                
                # Loss
                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (actions.shape[0],), device=device).long()
                noise = torch.randn_like(actions)
                noisy_actions = noise_scheduler.add_noise(actions, noise, timesteps)
                
                conditions = {"e_t": e_t, "state": state[:, -1, :]}
                pred_noise = rdt_wrapper(noisy_actions, timesteps, conditions)
                
                loss_diff = F.mse_loss(pred_noise, noise)
                loss_cons = compute_consistency_loss(fusion_encoder, batch, device)
                total_loss = loss_diff + 0.1 * loss_cons

            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(rdt_wrapper.parameters(), 1.0)
            optimizer.step()
            
            # --- æ—¥å¿—è®°å½• ---
            if i % 10 == 0:
                # 1. æ‰“å°åˆ°æ§åˆ¶å°
                print(f"Epoch {epoch} [{i}/{len(loader)}] Loss: {total_loss.item():.4f} (Diff: {loss_diff.item():.4f} Cons: {loss_cons.item():.4f})")
                
                # 2. å†™å…¥ TensorBoard
                tb_writer.add_scalar('Train/Total_Loss', total_loss.item(), global_step)
                tb_writer.add_scalar('Train/Diff_Loss', loss_diff.item(), global_step)
                tb_writer.add_scalar('Train/Cons_Loss', loss_cons.item(), global_step)
                
                # 3. å†™å…¥ WandB
                if args.use_wandb and HAS_WANDB:
                    wandb.log({
                        "total_loss": total_loss.item(),
                        "diff_loss": loss_diff.item(),
                        "cons_loss": loss_cons.item(),
                        "epoch": epoch
                    }, step=global_step)
            
            # --- è§†é¢‘å¯è§†åŒ– (æ¯ 500 æ­¥ä¸€æ¬¡) ---
            if global_step % 500 == 0 and args.use_wandb and HAS_WANDB:
                # æå–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è§†é¢‘: [2, 3, 16, H, W]
                # View 0: Main, View 1: Wrist
                vid_sample = video[0].float().cpu().numpy() # [2, 3, 16, H, W]
                
                # è½¬æ¢ä¸º GIF æ ¼å¼ [T, C, H, W] -> wandb éœ€ [T, C, H, W]
                # æˆ‘ä»¬æŠŠ Main å’Œ Wrist æ‹¼åœ¨ä¸€èµ·æ˜¾ç¤º
                main_view = vid_sample[0] # [3, 16, H, W] -> [16, 3, H, W]
                wrist_view = vid_sample[1]
                
                # å¤„ç†ä¸€ä¸‹ç»´åº¦é¡ºåºç»™ wandb: (Time, Channel, Height, Width)
                main_view = np.transpose(main_view, (1, 0, 2, 3))
                wrist_view = np.transpose(wrist_view, (1, 0, 2, 3))
                
                # æ‹¼æ¥: å·¦å³æ‹¼æ¥
                combined_view = np.concatenate([main_view, wrist_view], axis=3) # Width ç»´åº¦æ‹¼æ¥
                
                # è®°å½•è§†é¢‘
                wandb.log({
                    "input_video": wandb.Video((combined_view * 255).astype(np.uint8), fps=4, format="gif", caption=f"E{epoch}-S{i}: {mask_type}")
                }, step=global_step)
                print("ğŸ¥ Video sample uploaded to WandB.")

            global_step += 1

        # ä¿å­˜ Checkpoint
        if epoch % 5 == 0 or epoch == args.epochs - 1:
            save_path = os.path.join(args.output_dir, f"epoch_{epoch}.pt")
            torch.save({
                'epoch': epoch,
                'rdt_state_dict': rdt_wrapper.state_dict(),
                'encoder_state_dict': fusion_encoder.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'pred_horizon': args.pred_horizon
            }, save_path)
            print(f"âœ… Saved to {save_path}")

    tb_writer.close()
    if args.use_wandb and HAS_WANDB:
        wandb.finish()

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_root', type=str, default='/yanghaochuan/data/1223pick_up_the_paper_cup.hdf5')
    parser.add_argument('--output_dir', type=str, default='/yanghaochuan/checkpoints')
    parser.add_argument('--stage_b_ckpt', type=str, default='/yanghaochuan/checkpoints/1223stageB_papercup.pt')
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--pred_horizon', type=int, default=64)
    
    # === [æ–°å¢] å¯è§†åŒ–å¼€å…³ ===
    parser.add_argument('--use_wandb', action='store_true', default=True, help="Enable WandB logging")
    
    args = parser.parse_args()
    train_stage_c(args)