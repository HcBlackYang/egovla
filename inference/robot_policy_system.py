# import sys
# import os
# import time
# import logging
# import cv2
# import numpy as np
# import math
# import threading
# from collections import deque
# from common.constants import ActionSpace
# from robots.franky_env import FrankyEnv
# from robots.robot_param import RobotParam
# from systems.tcp_client import TCPClientPolicy 

# # å¼•å…¥ä½ çš„ç›¸æœºåº“
# from cameras.realsense_env import RealSenseEnv

# class ImageRecorder(threading.Thread):
#     def __init__(self, camera, buffer_size=16):
#         super().__init__()
#         self.camera = camera
#         self.buffer_size = buffer_size
#         self.running = False
#         self.lock = threading.Lock()
        
#         # ä¸¤ä¸ª Bufferï¼š
#         # 1. raw_buffer: å­˜åŸå§‹å›¾ï¼Œç”¨äºæ˜¾ç¤º
#         # 2. video_buffer: å­˜å¤„ç†åçš„ tensor/numpyï¼Œç”¨äºæ¨ç†
#         self.latest_frame = None
        
#         # è¿™é‡Œçš„ buffer åªè¦å­˜ numpy æ•°ç»„å³å¯ï¼Œä¸éœ€è¦å­˜ Tensorï¼Œ
#         # è½¬æ¢ Tensor çš„å·¥ä½œäº¤ç»™ Server ç«¯ï¼Œæˆ–è€…åœ¨å‘é€å‰åšï¼Œå‡å°‘ä¼ è¾“å‹åŠ›
#         # ä½†ä¸ºäº†é…åˆä½ çš„ Server é€»è¾‘ï¼Œæˆ‘ä»¬è¿™é‡Œåªå­˜åŸå§‹ BGR å›¾åƒ
#         self.frame_buffer = deque(maxlen=buffer_size) 
#         self.stop_event = threading.Event()

#     def run(self):
#         self.running = True
#         self.camera.start_monitoring()
#         logging.info("[ImageRecorder] Background thread started.")
        
#         while not self.stop_event.is_set():
#             # è·å–æœ€æ–°å¸§ (è¿™æ˜¯è½»é‡çº§æ“ä½œ)
#             data = self.camera.get_latest_frame()
#             if data is not None:
#                 img = data['bgr']
                
#                 with self.lock:
#                     self.latest_frame = img.copy()
#                     # å­˜å…¥ Buffer
#                     self.frame_buffer.append(img)
                
#                 # å®æ—¶æ˜¾ç¤º (åœ¨è¿™é‡Œæ˜¾ç¤ºæœ€æµç•…)
#                 cv2.imshow("Wrist View (Real-time)", img)
#                 if cv2.waitKey(1) & 0xFF == ord('q'):
#                     self.stop_event.set()
            
#             # ä¿æŒçº¦ 30Hz çš„é‡‡æ ·ç‡ (æ ¹æ®ä½ è®­ç»ƒæ•°æ®çš„å¸§ç‡è°ƒæ•´)
#             # å¦‚æœä½ è®­ç»ƒæ˜¯ 10Hzï¼Œè¿™é‡Œæ”¹æˆ time.sleep(0.1)
#             time.sleep(0.033) 
        
#         cv2.destroyAllWindows()
#         logging.info("[ImageRecorder] Stopped.")

#     def get_inference_input(self):
#         """
#         è·å–ç”¨äºæ¨ç†çš„ snapshotã€‚
#         å¦‚æœ Buffer è¿˜æ²¡æ»¡ï¼Œå°±ç”¨ç¬¬ä¸€å¸§å¤åˆ¶å¡«å…… (Padding)ã€‚
#         """
#         with self.lock:
#             if len(self.frame_buffer) == 0:
#                 return None, None
            
#             current_img = self.latest_frame.copy()
            
#             # æ‹¿åˆ° Buffer çš„å¿«ç…§
#             frames_snapshot = list(self.frame_buffer)
        
#         # ç­–ç•¥ï¼šå¦‚æœä¸å¤Ÿ 16 å¸§ï¼Œç”¨ç¬¬ä¸€å¸§è¡¥é½å¤´éƒ¨ (Padding Head)
#         # è¿™æ ·ä¿è¯æ—¶åºç›¸å¯¹å…³ç³»æ˜¯æ­£ç¡®çš„
#         while len(frames_snapshot) < self.buffer_size:
#             frames_snapshot.insert(0, frames_snapshot[0])
            
#         # æ³¨æ„ï¼šä½ éœ€è¦ç¡®è®¤ Server ç«¯æœŸå¾…çš„æ˜¯ä»€ä¹ˆæ ¼å¼ã€‚
#         # ä½ ä¹‹å‰çš„ä»£ç åªå‘äº† "wrist_image" (ä¸€å¼ å›¾) ç»™ Serverï¼Œ
#         # Server è‡ªå·±åœ¨é‚£è¾¹ append bufferã€‚
#         # 
#         # === å…³é”®çº æ­£ ===
#         # æ—¢ç„¶ Server ç«¯æœ‰ Video Buffer é€»è¾‘ï¼Œä½†ç”±äºé€šä¿¡é¢‘ç‡å¤ªä½å¯¼è‡´ Buffer å¤±æ•ˆã€‚
#         # æœ€å¥½çš„åŠæ³•æ˜¯ï¼š**å®¢æˆ·ç«¯è‡ªå·±ç»´æŠ¤ Bufferï¼Œå‘ç»™ Server æ—¶åªå‘æœ€æ–°ä¸€å¼ å›¾**
#         # ä½†æ˜¯ï¼å‘Šè¯‰ Serverï¼š"è¯·æ¸…ç©ºä½ çš„ Bufferï¼Œåªç”¨è¿™ä¸€å¼ å›¾å½“é™æ€å›¾å¤„ç†"
#         # æˆ–è€…ï¼**æˆ‘ä»¬åœ¨å®¢æˆ·ç«¯ç»´æŠ¤å¥½ Bufferï¼Œä¸€æ¬¡æ€§æŠŠ 16 å¼ å›¾å‘è¿‡å»ï¼Ÿ**
#         # 
#         # é‰´äºæ”¹åŠ¨é€šä¿¡åè®®å¤ªéº»çƒ¦ï¼Œæˆ‘ä»¬é‡‡ç”¨ã€æ–¹æ¡ˆ Bã€‘ï¼š
#         # è¿˜æ˜¯åªå‘æœ€æ–°ä¸€å¼ å›¾ï¼Œä½†åœ¨ Server ç«¯æˆ‘ä»¬å·²ç»ä¿®æ”¹äº†é€»è¾‘ (æ¯æ¬¡æ¸…ç©º Buffer å¡«æ»¡å½“å‰å¸§)ã€‚
#         # æ‰€ä»¥è¿™é‡Œ ImageRecorder çš„ä¸»è¦ä½œç”¨æ˜¯ï¼šç¡®ä¿ robot_policy_system æ‹¿åˆ°çš„
#         # "latest_frame" æ˜¯çœŸçš„ "latest" (åˆšåˆšå‘ç”Ÿçš„)ï¼Œè€Œä¸æ˜¯ 5 ç§’å‰çš„ç¼“å­˜ã€‚
        
#         return current_img

#     def stop(self):
#         self.stop_event.set()
#         self.join()

# class RobotPolicySystem:
#     def __init__(self, action_space: ActionSpace = ActionSpace.JOINT_ANGLES, ip: str = "127.0.0.1", port: int = 6000):
#         self.action_space = action_space
        
#         # Robot
#         self.robot_env = FrankyEnv(
#             action_space=action_space, 
#             inference_mode=True, 
#             robot_param=RobotParam(np.array([ 0.0, 0.0, -math.pi / 2]), np.array([ 0.53433071, 0.52905707, 0.00440881]))
#         )
        
#         # Client
#         logging.info(f"Connecting to {ip}:{port}...")
#         self.client = TCPClientPolicy(host=ip, port=port)
#         logging.info("Connected.")
        
#         # Camera & Recorder
#         self.wrist_camera = RealSenseEnv(camera_name="wrist_image", serial_number="342222072092", width=1280, height=720)
#         # å¯åŠ¨åå°é‡‡é›†çº¿ç¨‹
#         self.recorder = ImageRecorder(self.wrist_camera, buffer_size=16)
        
#         self.gripper_status = {"current_state": 0}
#         self.stop_evaluation = threading.Event()

#     def run(self, task_name: str = "default_task"):
#         # å¯åŠ¨åå°é‡‡é›†
#         self.recorder.start()
        
#         logging.info("Waiting 2.0s for warmup...")
#         time.sleep(2.0)
        
#         # å‚æ•°è®¾ç½®
#         EXECUTION_HORIZON = 15  # ä¿¡ä»»æ¨¡å‹ï¼Œåšå®Œ 64 æ­¥
#         MAX_STEP_RAD = 0.05     # é™å¹…
#         last_executed_joints = None
        
#         logging.info("Starting inference loop...")

#         try:
#             while not self.stop_evaluation.is_set():
#                 if not self.recorder.is_alive():
#                     break

#                 t0 = time.time()
                
#                 # 1. ä»åå°çº¿ç¨‹æ‹¿ã€æœ€æ–°é²œã€‘çš„ä¸€å¼ å›¾
#                 # å³ä½¿ä¸»çº¿ç¨‹å¡äº† 5 ç§’ï¼Œè¿™é‡Œæ‹¿åˆ°çš„ä¹Ÿæ˜¯ 0.001 ç§’å‰ç›¸æœºåˆšæ‹åˆ°çš„
#                 wrist_image = self.recorder.get_inference_input()
                
#                 if wrist_image is None:
#                     time.sleep(0.01)
#                     continue

#                 # 2. è·å–æœºå™¨äººçŠ¶æ€
#                 joint_angles = self.robot_env.get_position(action_space=ActionSpace.JOINT_ANGLES)
#                 gripper_width = self.robot_env.get_gripper_width()
#                 eef_pose = self.robot_env.get_position(action_space=ActionSpace.EEF_POSE)
                
#                 qpos_8d = list(joint_angles) + [float(gripper_width)]
#                 state = np.concatenate([eef_pose, [gripper_width]])
                
#                 # 3. å‘é€è¯·æ±‚
#                 # æ³¨æ„ï¼šæˆ‘ä»¬åœ¨ Server ç«¯å·²ç»æ”¹æˆäº† "æ”¶åˆ°ä¸€å¼ å›¾ -> å¤åˆ¶å¡«æ»¡ Buffer" çš„é™æ€å›¾ç­–ç•¥
#                 # è¿™é…åˆè¿™é‡Œ "è·å–æœ€æ–°é²œçš„ä¸€å¼ å›¾" æ˜¯ç›®å‰æœ€ç¨³å¥çš„ç»„åˆ
#                 element = {
#                     "observation/agentview_image": np.zeros_like(wrist_image), 
#                     "observation/wrist_image": wrist_image,
#                     "observation/state": state,
#                     "qpos": qpos_8d, 
#                     "prompt": task_name,
#                 }

#                 # 4. æ¨ç† (Blocking 2.5s)
#                 inference_results = self.client.infer(element)
                
#                 if inference_results and "actions" in inference_results:
#                     new_actions = inference_results["actions"][0]
                    
#                     if not isinstance(new_actions, list) or len(new_actions) == 0:
#                         continue

#                     # æ‰§è¡Œ 64 æ­¥
#                     actions_to_execute = new_actions[:EXECUTION_HORIZON]
                    
#                     print(f"  >>> Executing chunk ({len(actions_to_execute)} steps)...")

#                     for action in actions_to_execute:
#                         if not isinstance(action, (list, tuple, np.ndarray)): continue
                        
#                         # æ•°æ®å¤„ç†
#                         action_np = np.array(action, dtype=np.float64)
#                         if np.all(action_np == 0) or np.isnan(action_np).any(): break
                        
#                         target_joints = action_np[:-1]
#                         gripper_val = action_np[-1]

#                         # å¹³æ»‘é™å¹…
#                         if last_executed_joints is not None:
#                             diff = np.clip(target_joints - last_executed_joints, -MAX_STEP_RAD, MAX_STEP_RAD)
#                             target_joints = last_executed_joints + diff
                        
#                         last_executed_joints = target_joints.copy()

#                         # æ‰§è¡Œ
#                         t_step_start = time.time()
#                         self.robot_env.step(target_joints, asynchronous=True)
                        
#                         # å¤¹çˆª
#                         if gripper_val > 0.06 and self.gripper_status["current_state"] != -1:
#                              self.robot_env.open_gripper(asynchronous=True)
#                              self.gripper_status["current_state"] = -1
#                         elif gripper_val < 0.02 and self.gripper_status["current_state"] != 1:
#                              self.robot_env.close_gripper(asynchronous=True)
#                              self.gripper_status["current_state"] = 1
                        
#                         # æ§é¢‘ 25Hz
#                         remain = 0.04 - (time.time() - t_step_start)
#                         if remain > 0: time.sleep(remain)

#                 latency = (time.time() - t0) * 1000
#                 print(f"\rLoop Latency: {latency:.1f}ms", end="")

#         except KeyboardInterrupt:
#             pass
#         finally:
#             self.stop()

#     def stop(self):
#         self.stop_evaluation.set()
#         self.recorder.stop()
#         time.sleep(0.5)
#         logging.info("System stopped.")

# if __name__ == "__main__":
#     logging.basicConfig(level=logging.INFO)
#     system = RobotPolicySystem(ip="127.0.0.1", port=6000)
#     system.run(task_name="pick up the paper cup")

import sys
import os
import time
import logging
import cv2
import numpy as np
import math
import threading
from collections import deque
from common.constants import ActionSpace
from robots.franky_env import FrankyEnv
from robots.robot_param import RobotParam
from systems.tcp_client import TCPClientPolicy 
from cameras.realsense_env import RealSenseEnv

# é…ç½®æ—¥å¿—æ ¼å¼ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class ImageRecorder(threading.Thread):
    def __init__(self, camera, buffer_size=16):
        super().__init__()
        self.camera = camera
        self.buffer_size = buffer_size
        self.running = False
        self.lock = threading.Lock()
        
        self.latest_frame = None
        self.frame_buffer = deque(maxlen=buffer_size) 
        self.stop_event = threading.Event()

    def run(self):
        self.running = True
        self.camera.start_monitoring()
        logging.info("[ImageRecorder] Background thread started.")
        
        while not self.stop_event.is_set():
            data = self.camera.get_latest_frame()
            if data is not None:
                img = data['bgr']
                with self.lock:
                    self.latest_frame = img.copy()
                    self.frame_buffer.append(img)
                
                # å®æ—¶æ˜¾ç¤ºï¼ŒæŒ‰ 'q' é€€å‡º
                cv2.imshow("Wrist View (Real-time)", img)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    self.stop_event.set()
            
            # ä¿æŒçº¦ 30Hz çš„é‡‡æ ·ç‡
            time.sleep(0.033) 
        
        cv2.destroyAllWindows()
        logging.info("[ImageRecorder] Stopped.")

    def get_sequence_input(self):
        """
        è·å–è¿‡å» 16 å¸§çš„å®Œæ•´åºåˆ—ã€‚
        å¦‚æœä¸è¶³ 16 å¸§ï¼Œç”¨ç¬¬ä¸€å¸§å¡«å……å¤´éƒ¨ (Padding Head)ã€‚
        """
        with self.lock:
            if len(self.frame_buffer) == 0:
                return None
            
            # å¤åˆ¶ä¸€ä»½å½“å‰ buffer
            frames_snapshot = list(self.frame_buffer)
        
        # å¤´éƒ¨è¡¥é½
        while len(frames_snapshot) < self.buffer_size:
            frames_snapshot.insert(0, frames_snapshot[0])
            
        return frames_snapshot

    def stop(self):
        self.stop_event.set()
        self.join()

class RobotPolicySystem:
    def __init__(self, action_space: ActionSpace = ActionSpace.JOINT_ANGLES, ip: str = "127.0.0.1", port: int = 6000):
        self.action_space = action_space
        
        # åˆå§‹åŒ–æœºå™¨äºº
        # æ³¨æ„ï¼šinference_mode=True é€šå¸¸æ„å‘³ç€æœºå™¨äººåŠ¨ä½œä¼šæ›´å¿«ã€æ›´ç›´æ¥
        self.robot_env = FrankyEnv(
            action_space=action_space, 
            inference_mode=True, 
            robot_param=RobotParam(np.array([ 0.0, 0.0, -math.pi / 2]), np.array([ 0.53433071, 0.52905707, 0.00440881]))
        )
        
        logging.info(f"Connecting to {ip}:{port}...")
        self.client = TCPClientPolicy(host=ip, port=port)
        logging.info("Connected.")
        
        # åˆå§‹åŒ–ç›¸æœº
        self.wrist_camera = RealSenseEnv(camera_name="wrist_image", serial_number="342222072092", width=1280, height=720)
        self.recorder = ImageRecorder(self.wrist_camera, buffer_size=16)
        
        # å¤¹çˆªçŠ¶æ€è®°å½•: 0=æœªçŸ¥, 1=é—­åˆ, -1=å¼ å¼€
        self.gripper_status = {"current_state": 0}
        self.stop_evaluation = threading.Event()

    def run(self, task_name: str = "default_task"):
        self.recorder.start()
        logging.info("Waiting 2.0s for warmup...")
        time.sleep(2.0)
        
        EXECUTION_HORIZON = 15  # æ¯æ¬¡æ¨ç†æ‰§è¡Œ 15 æ­¥ (çº¦ 0.6s)
        MAX_STEP_RAD = 0.08     # å…³èŠ‚åŠ¨ä½œé™å¹…
        last_executed_joints = None
        
        # ğŸš¨ å…³é”®å‚æ•°ï¼šå¤¹çˆªåŠ¨ä½œé˜ˆå€¼ (åŸºäº compute_stats çš„ Mean=0.0616)
        GRIPPER_THRESHOLD = 0.06 
        
        logging.info(f"Starting inference loop... (Gripper Threshold: {GRIPPER_THRESHOLD})")

        try:
            while not self.stop_evaluation.is_set():
                if not self.recorder.is_alive(): break

                t0 = time.time()
                
                # 1. è·å–å›¾åƒåºåˆ—
                wrist_images = self.recorder.get_sequence_input()
                if wrist_images is None:
                    time.sleep(0.01)
                    continue

                # 2. è·å–æœºå™¨äººçŠ¶æ€
                joint_angles = self.robot_env.get_position(action_space=ActionSpace.JOINT_ANGLES)
                gripper_width = self.robot_env.get_gripper_width()
                eef_pose = self.robot_env.get_position(action_space=ActionSpace.EEF_POSE)
                
                # æ‹¼è£…æ•°æ®
                qpos_8d = list(joint_angles) + [float(gripper_width)]
                state = np.concatenate([eef_pose, [gripper_width]])
                
                element = {
                    "observation/wrist_image": wrist_images, # List[np.array]
                    "observation/state": state,
                    "qpos": qpos_8d, 
                    "prompt": task_name,
                }

                # 3. å‘é€æ¨ç†è¯·æ±‚
                inference_results = self.client.infer(element)
                
                if inference_results and "actions" in inference_results:
                    new_actions = inference_results["actions"][0]
                    if not isinstance(new_actions, list) or len(new_actions) == 0: continue

                    # æˆªå–å‰ 15 æ­¥æ‰§è¡Œ (Receding Horizon Control)
                    actions_to_execute = new_actions[:EXECUTION_HORIZON]
                    print(f"  >>> Executing chunk ({len(actions_to_execute)} steps)...")

                    for action in actions_to_execute:
                        if not isinstance(action, (list, tuple, np.ndarray)): continue
                        
                        action_np = np.array(action, dtype=np.float64)
                        if np.all(action_np == 0) or np.isnan(action_np).any(): break
                        
                        target_joints = action_np[:-1]
                        gripper_val = action_np[-1] # è¿™æ˜¯ç‰©ç†å€¼ (çº¦ 0.04 ~ 0.08)

                        # å¹³æ»‘é™å¹…
                        if last_executed_joints is not None:
                            diff = np.clip(target_joints - last_executed_joints, -MAX_STEP_RAD, MAX_STEP_RAD)
                            target_joints = last_executed_joints + diff
                        
                        last_executed_joints = target_joints.copy()

                        # æ‰§è¡Œå…³èŠ‚è¿åŠ¨
                        t_step_start = time.time()
                        self.robot_env.step(target_joints, asynchronous=True)
                        
                        # =========================================================
                        # ğŸš¨ [ä¿®å¤] å¤¹çˆªæ§åˆ¶é€»è¾‘
                        # =========================================================
                        # é€»è¾‘ï¼šå¤§äºé˜ˆå€¼ -> å¼€ï¼Œå°äºé˜ˆå€¼ -> å…³
                        
                        # Case 1: éœ€è¦å¼ å¼€
                        if gripper_val > GRIPPER_THRESHOLD:
                            # åªæœ‰å½“å‰ä¸æ˜¯â€œå¼ å¼€â€çŠ¶æ€æ—¶æ‰å‘é€å‘½ä»¤ï¼Œé¿å…é‡å¤å‘é€
                            if self.gripper_status["current_state"] != -1:
                                logging.info(f"ğŸ‘ [Gripper] OPEN detected ({gripper_val:.4f} > {GRIPPER_THRESHOLD})")
                                self.robot_env.open_gripper(asynchronous=True)
                                self.gripper_status["current_state"] = -1
                        
                        # Case 2: éœ€è¦é—­åˆ
                        elif gripper_val < GRIPPER_THRESHOLD:
                            # åªæœ‰å½“å‰ä¸æ˜¯â€œé—­åˆâ€çŠ¶æ€æ—¶æ‰å‘é€å‘½ä»¤
                            if self.gripper_status["current_state"] != 1:
                                logging.info(f"âœŠ [Gripper] CLOSE detected ({gripper_val:.4f} < {GRIPPER_THRESHOLD})")
                                self.robot_env.close_gripper(asynchronous=True)
                                self.gripper_status["current_state"] = 1
                        # =========================================================
                        
                        # æ§é¢‘ (40Hz)
                        remain = 0.025 - (time.time() - t_step_start)
                        if remain > 0: time.sleep(remain)

                latency = (time.time() - t0) * 1000
                print(f"\rLoop Latency: {latency:.1f}ms", end="")

        except KeyboardInterrupt:
            logging.info("Keyboard Interrupt received.")
        except Exception as e:
            logging.error(f"Runtime Error: {e}")
        finally:
            self.stop()

    def stop(self):
        self.stop_evaluation.set()
        if self.recorder.is_alive():
            self.recorder.stop()
        time.sleep(0.5)
        logging.info("System stopped.")

if __name__ == "__main__":
    # ç¡®ä¿è¿™é‡Œçš„ä»»åŠ¡åç§°å’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
    system = RobotPolicySystem(ip="127.0.0.1", port=6000)
    system.run(task_name="pick up the orange ball")