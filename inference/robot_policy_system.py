import sys
import os
import time
import logging
import cv2
import numpy as np
import math
import threading
from collections import deque
from common.constants import ActionSpace
from robots.franky_env import FrankyEnv
from robots.robot_param import RobotParam
from systems.tcp_client import TCPClientPolicy 
from cameras.realsense_env import RealSenseEnv

# é…ç½®æ—¥å¿—æ ¼å¼ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class ImageRecorder(threading.Thread):
    def __init__(self, camera, buffer_size=16):
        super().__init__()
        self.camera = camera
        self.buffer_size = buffer_size
        self.running = False
        self.lock = threading.Lock()
        
        self.latest_frame = None
        self.frame_buffer = deque(maxlen=buffer_size) 
        self.stop_event = threading.Event()

    def run(self):
        self.running = True
        self.camera.start_monitoring()
        logging.info("[ImageRecorder] Background thread started.")
        
        while not self.stop_event.is_set():
            data = self.camera.get_latest_frame()
            if data is not None:
                img = data['bgr']
                with self.lock:
                    self.latest_frame = img.copy()
                    self.frame_buffer.append(img)
                
                # å®æ—¶æ˜¾ç¤ºï¼ŒæŒ‰ 'q' é€€å‡º
                cv2.imshow("Wrist View (Real-time)", img)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    self.stop_event.set()
            
            # ä¿æŒçº¦ 30Hz çš„é‡‡æ ·ç‡
            time.sleep(0.033) 
        
        cv2.destroyAllWindows()
        logging.info("[ImageRecorder] Stopped.")

    def get_sequence_input(self):
        """
        è·å–è¿‡å» 16 å¸§çš„å®Œæ•´åºåˆ—ã€‚
        å¦‚æœä¸è¶³ 16 å¸§ï¼Œç”¨ç¬¬ä¸€å¸§å¡«å……å¤´éƒ¨ (Padding Head)ã€‚
        """
        with self.lock:
            if len(self.frame_buffer) == 0:
                return None
            
            # å¤åˆ¶ä¸€ä»½å½“å‰ buffer
            frames_snapshot = list(self.frame_buffer)
        
        # å¤´éƒ¨è¡¥é½
        while len(frames_snapshot) < self.buffer_size:
            frames_snapshot.insert(0, frames_snapshot[0])
            
        return frames_snapshot

    def stop(self):
        self.stop_event.set()
        self.join()

class RobotPolicySystem:
    def __init__(self, action_space: ActionSpace = ActionSpace.JOINT_ANGLES, ip: str = "127.0.0.1", port: int = 6000):
        self.action_space = action_space
        
        # åˆå§‹åŒ–æœºå™¨äºº
        # æ³¨æ„ï¼šinference_mode=True é€šå¸¸æ„å‘³ç€æœºå™¨äººåŠ¨ä½œä¼šæ›´å¿«ã€æ›´ç›´æ¥
        self.robot_env = FrankyEnv(
            action_space=action_space, 
            inference_mode=True, 
            robot_param=RobotParam(np.array([ 0.0, 0.0, -math.pi / 2]), np.array([ 0.53433071, 0.52905707, 0.00440881]))
        )
        
        logging.info(f"Connecting to {ip}:{port}...")
        self.client = TCPClientPolicy(host=ip, port=port)
        logging.info("Connected.")
        
        # åˆå§‹åŒ–ç›¸æœº
        self.wrist_camera = RealSenseEnv(camera_name="wrist_image", serial_number="342222072092", width=1280, height=720)
        self.recorder = ImageRecorder(self.wrist_camera, buffer_size=16)
        
        # å¤¹çˆªçŠ¶æ€è®°å½•: 0=æœªçŸ¥, 1=é—­åˆ, -1=å¼ å¼€
        self.gripper_status = {"current_state": 0}
        self.stop_evaluation = threading.Event()

    def run(self, task_name: str = "default_task"):
        self.recorder.start()
        logging.info("Waiting 2.0s for warmup...")
        time.sleep(2.0)
        
        EXECUTION_HORIZON = 15  # æ¯æ¬¡æ¨ç†æ‰§è¡Œ 15 æ­¥ (çº¦ 0.6s)
        MAX_STEP_RAD = 0.08     # å…³èŠ‚åŠ¨ä½œé™å¹…
        last_executed_joints = None
        
        # ğŸš¨ å…³é”®å‚æ•°ï¼šå¤¹çˆªåŠ¨ä½œé˜ˆå€¼ (åŸºäº compute_stats çš„ Mean=0.0616)
        GRIPPER_THRESHOLD = 0.06 
        
        logging.info(f"Starting inference loop... (Gripper Threshold: {GRIPPER_THRESHOLD})")

        try:
            while not self.stop_evaluation.is_set():
                if not self.recorder.is_alive(): break

                t0 = time.time()
                
                # # 1. è·å–å›¾åƒåºåˆ—
                # wrist_images = self.recorder.get_sequence_input()
                # if wrist_images is None:
                #     time.sleep(0.01)
                #     continue
                # ğŸŸ¢ [ä¿®æ”¹ 1]ï¼šåªè·å–æœ€æ–°çš„ä¸€å¸§ï¼Œè€Œä¸æ˜¯æ•´ä¸ªåºåˆ—
                # å› ä¸º Agent å†…éƒ¨å·²ç»ç»´æŠ¤äº†é•¿è¾¾ 500 çš„ Bufferï¼Œä¸éœ€è¦æˆ‘ä»¬æ¯æ¬¡é‡å¤å‘å†å²
                with self.recorder.lock:
                    latest_img = self.recorder.latest_frame
                
                if latest_img is None:
                    time.sleep(0.01)
                    continue
                
                # åŒ…è£…æˆ List å‘é€ï¼Œå› ä¸º Agent.step æ¥å£æœŸæœ›çš„æ˜¯ List[np.array]
                wrist_images = [latest_img]

                # 2. è·å–æœºå™¨äººçŠ¶æ€
                joint_angles = self.robot_env.get_position(action_space=ActionSpace.JOINT_ANGLES)
                gripper_width = self.robot_env.get_gripper_width()
                eef_pose = self.robot_env.get_position(action_space=ActionSpace.EEF_POSE)
                
                # æ‹¼è£…æ•°æ®
                qpos_8d = list(joint_angles) + [float(gripper_width)]
                state = np.concatenate([eef_pose, [gripper_width]])
                
                element = {
                    "observation/wrist_image": wrist_images, # List[np.array]
                    "observation/state": state,
                    "qpos": qpos_8d, 
                    "prompt": task_name,
                }

                # 3. å‘é€æ¨ç†è¯·æ±‚
                inference_results = self.client.infer(element)
                
                if inference_results and "actions" in inference_results:
                    new_actions = inference_results["actions"][0]
                    if not isinstance(new_actions, list) or len(new_actions) == 0: continue

                    # æˆªå–å‰ 15 æ­¥æ‰§è¡Œ (Receding Horizon Control)
                    actions_to_execute = new_actions[:EXECUTION_HORIZON]
                    print(f"  >>> Executing chunk ({len(actions_to_execute)} steps)...")

                    for action in actions_to_execute:
                        if not isinstance(action, (list, tuple, np.ndarray)): continue
                        
                        action_np = np.array(action, dtype=np.float64)
                        if np.all(action_np == 0) or np.isnan(action_np).any(): break
                        
                        target_joints = action_np[:-1]
                        gripper_val = action_np[-1] # è¿™æ˜¯ç‰©ç†å€¼ (çº¦ 0.04 ~ 0.08)

                        # å¹³æ»‘é™å¹…
                        if last_executed_joints is not None:
                            diff = np.clip(target_joints - last_executed_joints, -MAX_STEP_RAD, MAX_STEP_RAD)
                            target_joints = last_executed_joints + diff
                        
                        last_executed_joints = target_joints.copy()

                        # æ‰§è¡Œå…³èŠ‚è¿åŠ¨
                        t_step_start = time.time()
                        self.robot_env.step(target_joints, asynchronous=True)
                        
                        # =========================================================
                        # ğŸš¨ [ä¿®å¤] å¤¹çˆªæ§åˆ¶é€»è¾‘
                        # =========================================================
                        # Case 1: éœ€è¦å¼ å¼€
                        if gripper_val > GRIPPER_THRESHOLD:
                            # åªæœ‰å½“å‰ä¸æ˜¯â€œå¼ å¼€â€çŠ¶æ€æ—¶æ‰å‘é€å‘½ä»¤ï¼Œé¿å…é‡å¤å‘é€
                            if self.gripper_status["current_state"] != -1:
                                logging.info(f"ğŸ‘ [Gripper] OPEN detected ({gripper_val:.4f} > {GRIPPER_THRESHOLD})")
                                self.robot_env.open_gripper(asynchronous=True)
                                self.gripper_status["current_state"] = -1
                        
                        # Case 2: éœ€è¦é—­åˆ
                        elif gripper_val < GRIPPER_THRESHOLD:
                            # åªæœ‰å½“å‰ä¸æ˜¯â€œé—­åˆâ€çŠ¶æ€æ—¶æ‰å‘é€å‘½ä»¤
                            if self.gripper_status["current_state"] != 1:
                                logging.info(f"âœŠ [Gripper] CLOSE detected ({gripper_val:.4f} < {GRIPPER_THRESHOLD})")
                                self.robot_env.close_gripper(asynchronous=True)
                                self.gripper_status["current_state"] = 1
                        # =========================================================
                        
                        # æ§é¢‘ (40Hz)
                        remain = 0.025 - (time.time() - t_step_start)
                        if remain > 0: time.sleep(remain)

                latency = (time.time() - t0) * 1000
                print(f"\rLoop Latency: {latency:.1f}ms", end="")
                print(f"Model Gripper: {gripper_val:.4f}", end="\r")

        except KeyboardInterrupt:
            logging.info("Keyboard Interrupt received.")
        except Exception as e:
            logging.error(f"Runtime Error: {e}")
        finally:
            self.stop()

    def stop(self):
        self.stop_evaluation.set()
        if self.recorder.is_alive():
            self.recorder.stop()
        time.sleep(0.5)
        logging.info("System stopped.")

if __name__ == "__main__":
    # ç¡®ä¿è¿™é‡Œçš„ä»»åŠ¡åç§°å’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
    system = RobotPolicySystem(ip="127.0.0.1", port=6000)
    system.run(task_name="pick up the orange ball")