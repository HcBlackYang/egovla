# # egoå•è§†è§’
# import torch
# import cv2
# import json
# import numpy as np
# from collections import deque
# from diffusers import DDIMScheduler
# import os
# from torch.amp import autocast
# from peft import LoraConfig, get_peft_model
# from transformers import T5Tokenizer
# import torch._dynamo
# from torchvision import transforms
# import time

# # === å¯¼å…¥ä½ çš„æ¨¡å‹ ===
# from model.fusion_encoder import FusionEncoder
# from model.rdt_model import RDTWrapper

# # === åŸºç¡€è·¯å¾„é…ç½® ===
# VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
# RDT_PATH = '/yanghaochuan/models/rdt-1b'
# STATS_PATH = "/yanghaochuan/data/124dataset_stats.json" 
# TOKENIZER_PATH = "/yanghaochuan/models/flan-t5-large"
# STAGE_C_PATH = '/yanghaochuan/124checkpoints_finetune/StageC_ForeSight_step_7000.pt'

# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# class SafetyController:
#     def __init__(self):
#         self.joint_limits_min = np.array([-2.89, -1.76, -2.89, -3.07, -2.89, -0.01, -2.89]) + 0.01
#         self.joint_limits_max = np.array([ 2.89,  1.76,  2.89, -0.06,  2.89,  3.75,  2.89]) - 0.01

#     def clip_actions(self, actions_batch):
#         actions_np = np.array(actions_batch)
#         joints = actions_np[:, :7]
#         gripper = actions_np[:, 7:]
#         joints_clipped = np.clip(joints, self.joint_limits_min, self.joint_limits_max)
#         return np.concatenate([joints_clipped, gripper], axis=1)

# class RealTimeAgent:
#     def __init__(self):
#         self.device = DEVICE
#         self.safety = SafetyController() 
#         self.pred_horizon = 64
#         self.trajectory_offset = None
        
#         # ğŸŸ¢ [Alignment] ä¸ dataset_loader.py ä¿æŒä¸€è‡´
#         self.history_len = 500       # æ¨¡æ‹Ÿ dataset ä¸­çš„ history_len
#         self.model_input_frames = 6  # æ¨¡æ‹Ÿ dataset ä¸­çš„ window_size
        
#         self.debug_dir = f"debug_visuals_{int(time.time())}"
#         os.makedirs(self.debug_dir, exist_ok=True)
#         self.step_counter = 0

#         print(f"[Agent] Loading Tokenizer from {TOKENIZER_PATH}...")
#         try:
#             self.tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
#         except:
#             self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
#         if not os.path.exists(STATS_PATH):
#             raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {STATS_PATH}")
#         with open(STATS_PATH, 'r') as f:
#             stats = json.load(f)
        
#         mean_raw = np.array(stats['action_mean'], dtype=np.float32)
#         std_raw = np.array(stats['action_std'], dtype=np.float32)
        
#         if mean_raw.shape[0] > 8:
#             self.action_mean = mean_raw[:8]
#             self.action_std = std_raw[:8]
#         elif mean_raw.shape[0] == 7:
#             self.action_mean = np.concatenate([mean_raw, [0.0]])
#             self.action_std = np.concatenate([std_raw, [1.0]])
#         else:
#             self.action_mean = mean_raw
#             self.action_std = std_raw
            
#         self.action_std = np.maximum(self.action_std, 1e-2)

#         # ğŸŸ¢ [Alignment] å½’ä¸€åŒ–å‚æ•°ä¸ VideoMAE/Dataset ä¸€è‡´
#         self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], 
#                                               std=[0.229, 0.224, 0.225])
#         self._init_models()
#         self._init_scheduler()
        
#         # ğŸŸ¢ [Alignment] å†å² Bufferï¼Œå¯¹åº” Dataset ä¸­çš„ sliding window
#         self.video_buffer = deque(maxlen=self.history_len)
#         self.state_buffer = deque(maxlen=self.history_len)
        
#         self.first_frame_tensor = None
#         self.text_tokens = None 
#         self.default_prompt = "pick up the orange ball and put it on the plank"
        
#         self.warmup()

#     def _init_models(self):
#         print(f"[Agent] Initializing models on {self.device}...")
#         self.encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(self.device).eval()
#         self.policy = RDTWrapper(action_dim=8, model_path=RDT_PATH, rdt_cond_dim=768, pred_horizon=64).to(self.device).eval()
        
#         print(f"[Agent] Loading Checkpoint: {STAGE_C_PATH}")
#         ckpt_c = torch.load(STAGE_C_PATH, map_location=self.device)
        
#         peft_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], lora_dropout=0.05, bias="none")
#         self.policy.rdt_model = get_peft_model(self.policy.rdt_model, peft_config)
        
#         if 'rdt_state_dict' in ckpt_c: self.policy.load_state_dict(ckpt_c['rdt_state_dict'], strict=False)
#         else: self.policy.load_state_dict(ckpt_c, strict=False)
        
#         # if 'encoder_state_dict' in ckpt_c: self.encoder.load_state_dict(ckpt_c['encoder_state_dict'], strict=False)
#         if 'encoder_state_dict' in ckpt_c: 
#             print("æ­£åœ¨åŠ è½½ Encoder æƒé‡...")
#             state_dict = ckpt_c['encoder_state_dict']
            
#             # ğŸ› ï¸ ä¿®å¤ï¼šç§»é™¤ç¼–è¯‘æˆ–DDPäº§ç”Ÿçš„å‰ç¼€
#             new_state_dict = {}
#             for k, v in state_dict.items():
#                 k_clean = k.replace("_orig_mod.", "").replace("module.", "")
#                 new_state_dict[k_clean] = v
            
#             # ğŸ” è¯Šæ–­ï¼šä¸è¦ç”¨ strict=Falseï¼Œæˆ–è€…æ‰“å°è¿”å›å€¼
#             missing, unexpected = self.encoder.load_state_dict(new_state_dict, strict=False)
            
#             if len(missing) > 0:
#                 print(f"âš ï¸ è­¦å‘Šï¼šEncoder åŠ è½½æœ‰ä¸¢å¤±é”®! (æ•°é‡: {len(missing)})")
#                 print(f"   ç¤ºä¾‹ä¸¢å¤±: {missing[:5]}")
#             else:
#                 print("âœ… Encoder æƒé‡å®Œç¾åŠ è½½ï¼")

#     def _init_scheduler(self):
#         self.scheduler = DDIMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2", prediction_type="epsilon", clip_sample=True)
#         self.inference_steps = 25
#         self.scheduler.set_timesteps(self.inference_steps)

#     def warmup(self):
#         print("ğŸ”¥ [System] Warming up model...")
#         dummy_video = torch.randn(1, 2, 3, 6, 224, 224, device=self.device, dtype=torch.bfloat16)
#         dummy_text = torch.randint(0, 1000, (1, 16), device=self.device)
#         dummy_state = torch.randn(1, 1, 8, device=self.device, dtype=torch.float32)
#         dummy_ff = torch.randn(1, 2, 3, 224, 224, device=self.device, dtype=torch.float32)
#         try:
#             with autocast('cuda', dtype=torch.bfloat16):
#                 feats = self.encoder(dummy_video, dummy_text, dummy_state, dummy_ff)
#                 feats["state"] = dummy_state[:, -1, :]
#                 latents = torch.randn(1, self.pred_horizon, 8, device=self.device)
#                 t = torch.tensor([0], device=self.device)
#                 _ = self.policy(latents, t, feats)
#             print("âœ… Warmup done.")
#         except Exception as e:
#             print(f"âŒ Warmup failed: {e}")

#     def preprocess_image(self, img_np):
#         resized = cv2.resize(img_np, (224, 224))
#         rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
#         tensor = torch.tensor(rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
#         # ğŸŸ¢ [Alignment] å¿…é¡»å½’ä¸€åŒ–
#         tensor = self.normalize(tensor) 
#         return tensor

#     def save_model_input_visuals(self, vid_tensor, step_idx):
#         try:
#             wrist_t = vid_tensor[0, 1] 
#             mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1, 1).to(wrist_t.device)
#             std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1, 1).to(wrist_t.device)
#             wrist_t = wrist_t * std + mean
#             wrist_t = torch.clamp(wrist_t, 0, 1)
#             imgs = wrist_t.permute(1, 2, 3, 0).detach().cpu().numpy()
#             imgs = (imgs * 255).astype(np.uint8)
#             concat_img = np.hstack([imgs[i] for i in range(6)])
#             concat_img = cv2.cvtColor(concat_img, cv2.COLOR_RGB2BGR)
#             save_path = os.path.join(self.debug_dir, f"step_{step_idx:04d}_buffer.jpg")
#             cv2.imwrite(save_path, concat_img)
#         except Exception as e:
#             print(f"âš ï¸ Visualization Failed: {e}")

#     def reset_session(self, first_frame_img, current_qpos=None):
#         print("[Agent] Resetting session (Cold Start)...")
#         self.video_buffer.clear()
#         self.state_buffer.clear()
        
#         # ğŸŸ¢ [Alignment] é¦–å¸§å¤„ç†
#         wrist_tensor = self.preprocess_image(first_frame_img)
#         main_fake = torch.zeros_like(wrist_tensor)
#         self.first_frame_tensor = torch.stack([main_fake, wrist_tensor], dim=0).unsqueeze(0).to(self.device)
        
#         tokens = self.tokenizer(self.default_prompt, return_tensors="pt", padding="max_length", max_length=16, truncation=True).input_ids
#         self.text_tokens = tokens.to(self.device)
        
#         # Buffer åˆå§‹å¡«å…¥è¿™ä¸€å¸§
#         video_frame_unit = torch.stack([main_fake, wrist_tensor], dim=0)
#         self.video_buffer.append(video_frame_unit)
            
#         if current_qpos is None: current_qpos = np.zeros(8)
#         else: 
#             if len(current_qpos) == 7: current_qpos = list(current_qpos) + [0.0]
#             current_qpos = np.array(current_qpos, dtype=np.float32)
            
#         print(f"   ğŸš© [Reset QPos] {current_qpos[:7]} ... Grip: {current_qpos[7]}")
        
#         norm_qpos = (current_qpos - self.action_mean) / self.action_std
#         self.state_buffer.append(norm_qpos)
#         # === ğŸŸ¢ æ·»åŠ è¿™å‡ è¡Œè¯Šæ–­ä»£ç  ===
#         print(f"\nğŸ” [Stats Check] J0 Mean: {self.action_mean[0]:.4f}, Std: {self.action_std[0]:.4f}")
#         print(f"ğŸ“‰ [Input Norm Check] Current J0: {current_qpos[0]:.4f} -> Normalized: {norm_qpos[0]:.4f}")
#         if abs(norm_qpos[0]) > 3.0:
#             print("âš ï¸ è­¦å‘Šï¼šåˆå§‹çŠ¶æ€ä¸¥é‡åç¦»è®­ç»ƒåˆ†å¸ƒ (OOD)ï¼æ¨¡å‹å¯èƒ½ä¼šå¤±æ•ˆï¼")
#         # ============================
#         self.trajectory_offset = None  # æ–°å¢ï¼šç¡®ä¿æ¯æ¬¡æ–°åŠ¨ä½œå¼€å§‹æ—¶é‡æ–°è®¡ç®—å¯¹é½
#         print("[Agent] Trajectory offset reset.")

#     @torch.no_grad()
#     def step(self, frames_list, current_qpos):
#         """
#         Stop-and-Think æ¨¡å¼:
#         1. æ¥æ”¶ frames_list (è¿™äº›æ˜¯æœºå™¨äººåœ¨æ‰§è¡Œä¸Šä¸€ä¸ªåŠ¨ä½œç‰‡æ®µæ—¶æ•è·çš„â€˜å†å²â€™å¸§)
#         2. å°†å®ƒä»¬**å…¨éƒ¨**åŠ å…¥ Buffer (æ¨¡æ‹Ÿæ—¶é—´æµé€)
#         3. è¿›è¡Œå‡åŒ€é‡‡æ · (æ¨¡æ‹Ÿ Training Loader)
#         4. æ¨ç†ä¸‹ä¸€ä¸ªåŠ¨ä½œ
#         """
#         # ========================================================
#         # ğŸŸ¢ Phase 1: Update History (Movement Phase Replay)
#         # ========================================================
#         # å°†ä¼ å…¥çš„æ‰€æœ‰å¸§æŒ‰é¡ºåºåŠ å…¥ Buffer
#         # è¿™å®Œå…¨å¯¹åº”äº†è®­ç»ƒé›†ä¸­ï¼Œæ»‘çª—éšç€æ—¶é—´æ­¥ t å‰è¿›è€Œå‰è¿›
#         for frame in frames_list:
#             wrist_tensor = self.preprocess_image(frame)
#             main_fake = torch.zeros_like(wrist_tensor)
#             combined_frame = torch.stack([main_fake, wrist_tensor], dim=0)
#             self.video_buffer.append(combined_frame) 
        
#         # ğŸŸ¢ State Update
#         # æˆ‘ä»¬å‡è®¾è¿™æ‰¹å›¾åƒå¯¹åº”çš„çŠ¶æ€è¿‘ä¼¼äºå½“å‰çŠ¶æ€ (æˆ–è€…ä½ å¯ä»¥è®©Clientä¼ çŠ¶æ€åˆ—è¡¨)
#         # ä¸ºäº†ä¿è¯ Video/State Buffer é•¿åº¦å¯¹é½ï¼Œæˆ‘ä»¬é‡å¤ append å½“å‰çŠ¶æ€
#         if len(current_qpos) == 7: current_qpos = list(current_qpos) + [0.0]
#         qpos_np = np.array(current_qpos, dtype=np.float32)
#         norm_qpos_np = (qpos_np - self.action_mean) / self.action_std
        
#         # é‡å¤å¡«å……ï¼Œä½¿å¾—çŠ¶æ€å†å²é•¿åº¦ä¸è§†è§‰å†å²é•¿åº¦åŒ¹é… (è™½ç„¶æ¨¡å‹åªç”¨æœ€åä¸€ä¸ª)
#         for _ in range(len(frames_list)):
#             self.state_buffer.append(norm_qpos_np)
        
#         # ========================================================
#         # ğŸŸ¢ Phase 2: Inference (Stop Phase)
#         # ========================================================
        
#         # 1. Uniform Sampling (å®Œå…¨å¤åˆ» Dataset __getitem__ é€»è¾‘)
#         curr_len = len(self.video_buffer)
        
#         # np.linspace(0, valid_len-1, 6)
#         indices = np.linspace(0, curr_len - 1, self.model_input_frames).astype(int)
#         selected_frames = [self.video_buffer[i] for i in indices]
        
#         # æ„é€  Batch
#         vid_t = torch.stack(selected_frames).to(self.device)
#         vid_t = vid_t.permute(1, 2, 0, 3, 4).unsqueeze(0) # [1, 2, 3, 6, H, W]

#         # ä¿å­˜ Debug å›¾ç‰‡ (ç¡®è®¤æ¨¡å‹åˆ°åº•çœ‹åˆ°äº†ä»€ä¹ˆ)
#         self.save_model_input_visuals(vid_t, self.step_counter)
#         self.step_counter += 1

#         # State: å–æœ€æ–°çš„ (FusionEncoder åªå…³æ³¨å½“å‰æ—¶åˆ»)
#         state_t = torch.tensor(norm_qpos_np, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)
        
#         # 2. Diffusion Inference
#         self.scheduler.set_timesteps(self.inference_steps)
#         with autocast('cuda', dtype=torch.bfloat16):
#             features = self.encoder(vid_t, self.text_tokens, state_t, self.first_frame_tensor)
#             features["state"] = state_t[:, -1, :] 
#             latents = torch.randn(1, self.pred_horizon, 8, device=self.device) 
            
#             for t in self.scheduler.timesteps:
#                 model_input = self.scheduler.scale_model_input(latents, t)
#                 t_tensor = torch.tensor([t], device=self.device)
#                 noise_pred = self.policy(model_input, t_tensor, features)
#                 latents = self.scheduler.step(noise_pred, t, latents).prev_sample
            
#         # 3. Denormalize & Output
#         normalized_actions = latents[0].float()
#         action_pred_np = normalized_actions.detach().cpu().numpy()
#         denormalized_actions = action_pred_np * self.action_std + self.action_mean
        
#         # å¤¹çˆªäºŒå€¼åŒ–
#         # GRIPPER_OPEN_VAL = 0.0804  
#         # GRIPPER_CLOSE_VAL = 0.0428 
#         # GRIPPER_THRESHOLD = 0.0616 

#         # raw_gripper_pred = denormalized_actions[:, 7]
#         # binary_gripper = np.where(raw_gripper_pred > GRIPPER_THRESHOLD, GRIPPER_OPEN_VAL, GRIPPER_CLOSE_VAL)
#         # denormalized_actions[:, 7] = binary_gripper

#         # if self.trajectory_offset is None:
#         #     # è®¡ç®—æ¨¡å‹é¢„æµ‹çš„ç¬¬ 0 æ­¥ä¸å½“å‰æœºå™¨äººçœŸå®ä½ç½®çš„å·®å€¼
#         #     # åªé’ˆå¯¹å‰ 7 ä¸ªå…³èŠ‚ (J0-J6)
#         #     pred_start = denormalized_actions[0, :7]
#         #     real_start = qpos_np[:7]
#         #     self.trajectory_offset = pred_start - real_start
#         #     print(f"ğŸš© [Aligner] Offset calculated: {self.trajectory_offset}")
            
#         # # === å°†æ‰“å°é€»è¾‘ç§»åˆ°è¿™é‡Œ ===
#         # print(f"\n{'='*25} ALIGNED RDT Action (First 15 Steps) {'='*25}")
#         # header = f"{'Step':<4} | {'J0':^7} {'J1':^7} {'J2':^7} {'J3':^7} {'J4':^7} {'J5':^7} {'J6':^7} | {'Grip':^6}"
#         # print(header)
#         # for i in range(min(15, len(denormalized_actions))):
#         #     step_data = denormalized_actions[i]
#         #     joints_str = " ".join([f"{x: .4f}" for x in step_data[:7]])
#         #     print(f"{i:<4} | {joints_str} | {step_data[7]:.4f}")
#         # # ========================


#         # # 1. è·å–å®æ—¶ä½ç½® (qpos_np æ˜¯ä½ åœ¨ step å¼€å§‹æ—¶å¤„ç†å¥½çš„å½“å‰ç‰©ç†çŠ¶æ€)
#         # real_start_pos = qpos_np[:8] 

#         # # 2. å¼ºåˆ¶è¦†ç›– Step 0ï¼Œç¡®ä¿ç‰©ç†å±‚é¢ç»å¯¹é‡åˆ
#         # # è¿™æ ·æœºå™¨äººæ‰§è¡Œç¬¬ä¸€ä¸ªåŠ¨ä½œæ—¶å°±ä¸ä¼šæœ‰ä»»ä½•â€œç¬è·³â€
#         # denormalized_actions[0, :8] = real_start_pos
        
#         # # ç®€å•æ—¥å¿—
#         # print(f"   >>> [Infer] BufferLen: {curr_len} | Pred J0: {denormalized_actions[0,0]:.3f}", end='\r')
        
#         # safe_actions = self.safety.clip_actions(denormalized_actions)
#         # return safe_actions.tolist()

#         # 2. è½¨è¿¹å¯¹é½é€»è¾‘ (Trajectory Aligner)
#         if self.trajectory_offset is None:
#             # è®°å½•æ¨¡å‹é¢„æµ‹çš„èµ·ç‚¹ä¸çœŸå®èµ·ç‚¹çš„åå·®
#             # æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»ä½¿ç”¨ .copy() é¿å…å¼•ç”¨å¹²æ‰°
#             pred_start = denormalized_actions[0, :7].copy()
#             real_start = qpos_np[:7].copy()
#             self.trajectory_offset = pred_start - real_start
#             print(f"\n   ğŸ”§ [Aligner] Calibration Done. Offset J0: {self.trajectory_offset[0]:.4f}")

#         # 3. åº”ç”¨å¯¹é½ï¼šå‡å»å…¨å±€åå·®
#         denormalized_actions[:, :7] -= self.trajectory_offset

#         # 4. ã€å…³é”®ä¿®å¤ã€‘ç‰©ç†å¼ºåˆ¶è¦†ç›– (Physical Overwrite)
#         # æ— è®ºæ¨¡å‹é¢„æµ‹å’Œå¯¹é½è®¡ç®—ç»“æœå¦‚ä½•ï¼Œå¼ºåˆ¶ç¬¬ä¸€æ­¥ç»å¯¹ç­‰äºå½“å‰ç‰©ç†ä½ç½®
#         # è¿™æ¶ˆé™¤äº†æ‰€æœ‰è®¡ç®—æ®‹å·®ï¼Œä¿è¯èµ·æ­¥ç»å¯¹å¹³æ»‘
#         denormalized_actions[0, :7] = qpos_np[:7]

#         # 5. å¤¹çˆªäºŒå€¼åŒ–å¤„ç†
#         GRIPPER_OPEN_VAL, GRIPPER_CLOSE_VAL, GRIPPER_THRESHOLD = 0.0804, 0.0428, 0.0616
#         raw_gripper_pred = denormalized_actions[:, 7]
#         denormalized_actions[:, 7] = np.where(raw_gripper_pred > GRIPPER_THRESHOLD, GRIPPER_OPEN_VAL, GRIPPER_CLOSE_VAL)

#         # 6. ã€ç»Ÿä¸€æ‰“å°ã€‘åœ¨æ‰€æœ‰ä¿®æ­£å®Œæˆåå†æ‰“å°åŠ¨ä½œè¡¨
#         self._print_aligned_table(denormalized_actions)

#         # 7. å®‰å…¨è£å‰ªå¹¶è¿”å›
#         safe_actions = self.safety.clip_actions(denormalized_actions)
#         return safe_actions.tolist()

#     def _print_aligned_table(self, actions):
#         """è¾…åŠ©æ–¹æ³•ï¼šæ‰“å°æœ€ç»ˆå‘é€ç»™æœºæ¢°è‡‚çš„åŠ¨ä½œåºåˆ—"""
#         print(f"\n{'='*25} FINAL EXECUTABLE ACTION (Step 0-14) {'='*25}")
#         header = f"{'Step':<4} | {'J0':^7} {'J1':^7} {'J2':^7} {'J3':^7} {'J4':^7} {'J5':^7} {'J6':^7} | {'Grip':^6}"
#         print(header)
#         print("-" * 82)
#         for i in range(15):
#             joints = actions[i, :7]
#             print(f"{i:<4} | {' '.join([f'{x: .4f}' for x in joints])} | {actions[i, 7]:.4f}")
#         print("=" * 82 + "\n")



# # egoçº¯æ•°å€¼
# import torch
# import cv2
# import json
# import numpy as np
# from collections import deque
# from diffusers import DDIMScheduler
# import os
# from torch.amp import autocast
# from peft import LoraConfig, get_peft_model
# from transformers import T5Tokenizer
# from torchvision import transforms
# import time

# # å¯¼å…¥æ¨¡å‹ç»„ä»¶
# from model.fusion_encoder import FusionEncoder
# from model.rdt_model import RDTWrapper

# # é…ç½®è·¯å¾„
# VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
# RDT_PATH = '/yanghaochuan/models/rdt-1b'
# STATS_PATH = "/yanghaochuan/data/124dataset_stats.json" 
# TOKENIZER_PATH = "/yanghaochuan/models/flan-t5-large"
# STAGE_C_PATH = '/yanghaochuan/124checkpoints_finetune/StageC_ForeSight_step_7000.pt'

# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# class SafetyController:
#     def __init__(self):
#         # å…³èŠ‚é™ä½ä¿æŠ¤
#         self.joint_limits_min = np.array([-2.89, -1.76, -2.89, -3.07, -2.89, -0.01, -2.89]) + 0.01
#         self.joint_limits_max = np.array([ 2.89,  1.76,  2.89, -0.06,  2.89,  3.75,  2.89]) - 0.01

#     def clip_actions(self, actions_batch):
#         actions_np = np.array(actions_batch)
#         joints = actions_np[:, :7]
#         gripper = actions_np[:, 7:]
#         joints_clipped = np.clip(joints, self.joint_limits_min, self.joint_limits_max)
#         return np.concatenate([joints_clipped, gripper], axis=1)

# class RealTimeAgent:
#     def __init__(self):
#         self.device = DEVICE
#         self.safety = SafetyController() 
#         self.pred_horizon = 64
#         self.history_len = 500       
#         self.model_input_frames = 6  
        
#         # 1. åŠ è½½ Tokenizer å¹¶é¢„åˆå§‹åŒ– text_tokens
#         print(f"[Agent] Loading Tokenizer from {TOKENIZER_PATH}...")
#         self.tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
#         self.default_prompt = "pick up the orange ball and put it on the plank"
        
#         # ğŸŸ¢ ä¿®å¤ ValueError: ç¡®ä¿åœ¨åˆå§‹åŒ–æ—¶å°±ç”Ÿæˆ text_tokens
#         self.text_tokens = self.tokenizer(
#             self.default_prompt, 
#             return_tensors="pt", 
#             padding="max_length", 
#             max_length=16, 
#             truncation=True
#         ).input_ids.to(self.device)

#         # 2. å½’ä¸€åŒ–å‚æ•°åŠ è½½
#         if not os.path.exists(STATS_PATH):
#             raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {STATS_PATH}")
#         with open(STATS_PATH, 'r') as f:
#             stats = json.load(f)
#         self.action_mean = np.array(stats['action_mean'][:8], dtype=np.float32)
#         self.action_std = np.maximum(np.array(stats['action_std'][:8], dtype=np.float32), 1e-2)

#         self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        
#         # 3. åˆå§‹åŒ–æ¨¡å‹ç»“æ„
#         self._init_models()
#         self._init_scheduler()
        
#         self.video_buffer = deque(maxlen=self.history_len)
#         self.state_buffer = deque(maxlen=self.history_len)
#         self.first_frame_tensor = None
        
#         # 4. æ‰§è¡Œé¢„çƒ­
#         self.warmup()

#     def _init_models(self):
#         print(f"[Agent] Initializing models on {self.device}...")
#         self.encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(self.device).eval()
#         self.policy = RDTWrapper(action_dim=8, model_path=RDT_PATH, rdt_cond_dim=768, pred_horizon=64).to(self.device).eval()
        
#         print(f"[Agent] Loading Checkpoint: {STAGE_C_PATH}")
#         ckpt_c = torch.load(STAGE_C_PATH, map_location=self.device)
        
#         # LoRA é…ç½®
#         peft_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], lora_dropout=0.05, bias="none")
#         self.policy.rdt_model = get_peft_model(self.policy.rdt_model, peft_config)
        
#         if 'rdt_state_dict' in ckpt_c: self.policy.load_state_dict(ckpt_c['rdt_state_dict'], strict=False)
#         else: self.policy.load_state_dict(ckpt_c, strict=False)
        
#         if 'encoder_state_dict' in ckpt_c: self.encoder.load_state_dict(ckpt_c['encoder_state_dict'], strict=False)

#     def _init_scheduler(self):
#         self.scheduler = DDIMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2", prediction_type="epsilon", clip_sample=True)
#         self.scheduler.set_timesteps(25)

#     def warmup(self):
#         """ğŸŸ¢ ä¿®å¤ AttributeError: ç¡®ä¿ç±»å®šä¹‰å†…åŒ…å«æ­¤æ–¹æ³•"""
#         print("ğŸ”¥ [System] Warming up model...")
#         dummy_video = torch.randn(1, 2, 3, 6, 224, 224, device=self.device, dtype=torch.bfloat16)
#         dummy_state = torch.randn(1, 1, 8, device=self.device, dtype=torch.float32)
#         dummy_ff = torch.randn(1, 2, 3, 224, 224, device=self.device, dtype=torch.float32)
#         try:
#             with autocast('cuda', dtype=torch.bfloat16):
#                 # ä½¿ç”¨å·²ç»åˆå§‹åŒ–å¥½çš„ self.text_tokens
#                 feats = self.encoder(dummy_video, self.text_tokens, dummy_state, dummy_ff)
#                 feats["state"] = dummy_state[:, -1, :]
#                 latents = torch.randn(1, self.pred_horizon, 8, device=self.device)
#                 t_tensor = torch.tensor([0], device=self.device)
#                 _ = self.policy(latents, t_tensor, feats)
#             print("âœ… Warmup done.")
#         except Exception as e:
#             print(f"âŒ Warmup failed: {e}")

#     def preprocess_image(self, img_np):
#         resized = cv2.resize(img_np, (224, 224))
#         rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
#         tensor = torch.tensor(rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
#         return self.normalize(tensor)

#     def reset_session(self, first_frame_img, current_qpos=None):
#         print("[Agent] Resetting session...")
#         self.video_buffer.clear()
#         self.state_buffer.clear()
        
#         wrist_tensor = self.preprocess_image(first_frame_img)
#         main_fake = torch.zeros_like(wrist_tensor)
#         self.first_frame_tensor = torch.stack([main_fake, wrist_tensor], dim=0).unsqueeze(0).to(self.device)
        
#         # æ›´æ–° text_tokensï¼ˆå¦‚æœ prompt æ”¹å˜ï¼‰
#         self.text_tokens = self.tokenizer(self.default_prompt, return_tensors="pt", padding="max_length", max_length=16, truncation=True).input_ids.to(self.device)
        
#         video_frame_unit = torch.stack([main_fake, wrist_tensor], dim=0)
#         self.video_buffer.append(video_frame_unit)
        
#         if current_qpos is None: current_qpos = np.zeros(8)
#         norm_qpos = (np.array(current_qpos[:8]) - self.action_mean) / self.action_std
#         self.state_buffer.append(norm_qpos)

#     @torch.no_grad()
#     def step(self, frames_list, current_qpos):
#         # 1. æ›´æ–° Buffer
#         for frame in frames_list:
#             wrist_t = self.preprocess_image(frame)
#             self.video_buffer.append(torch.stack([torch.zeros_like(wrist_t), wrist_t], dim=0)) 
        
#         qpos_np = np.array(current_qpos[:8], dtype=np.float32)
#         norm_qpos_np = (qpos_np - self.action_mean) / self.action_std
#         self.state_buffer.append(norm_qpos_np)
        
#         # 2. é‡‡æ ·è¾“å…¥
#         curr_len = len(self.video_buffer)
#         indices = np.linspace(0, curr_len - 1, self.model_input_frames).astype(int)
#         vid_t = torch.stack([self.video_buffer[i] for i in indices]).to(self.device).permute(1, 2, 0, 3, 4).unsqueeze(0)
#         state_t = torch.tensor(norm_qpos_np, dtype=torch.float32).view(1, 1, 8).to(self.device)
        
#         # 3. æ¨ç†
#         with autocast('cuda', dtype=torch.bfloat16):
#             # ğŸŸ¢ æ­¤æ—¶ self.text_tokens å·²åœ¨ __init__ ç¡®ä¿éç©º
#             features = self.encoder(vid_t, self.text_tokens, state_t, self.first_frame_tensor)
#             features["state"] = state_t[:, -1, :] 
#             latents = torch.randn(1, self.pred_horizon, 8, device=self.device) 
            
#             for t in self.scheduler.timesteps:
#                 model_input = self.scheduler.scale_model_input(latents, t)
#                 t_tensor = torch.tensor([t], device=self.device)
#                 noise_pred = self.policy(model_input, t_tensor, features)
#                 latents = self.scheduler.step(noise_pred, t, latents).prev_sample
            
#         # 4. åå½’ä¸€åŒ–
#         action_pred_np = latents[0].float().cpu().numpy()
#         denormalized_actions = action_pred_np * self.action_std + self.action_mean
        
#         # å¤¹çˆªäºŒå€¼åŒ–
#         GRIPPER_THRESHOLD = 0.0616
#         denormalized_actions[:, 7] = np.where(denormalized_actions[:, 7] > GRIPPER_THRESHOLD, 0.0804, 0.0428)
        
#         # å®‰å…¨è£å‰ªå¹¶è¿”å›
#         return self.safety.clip_actions(denormalized_actions).tolist()




#ego äºŒå€¼åŒ–
import torch
import cv2
import json
import numpy as np
from collections import deque
from diffusers import DDIMScheduler
import os
from torch.amp import autocast
from peft import LoraConfig, get_peft_model
from transformers import T5Tokenizer
from torchvision import transforms
import time

# å¯¼å…¥æ¨¡å‹ç»„ä»¶
from model.fusion_encoder import FusionEncoder
from model.rdt_model import RDTWrapper

# é…ç½®è·¯å¾„
VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
RDT_PATH = '/yanghaochuan/models/rdt-1b'
STATS_PATH = "/yanghaochuan/data/131dataset_stats.json" 
TOKENIZER_PATH = "/yanghaochuan/models/flan-t5-large"
STAGE_C_PATH = '/yanghaochuan/131checkpoints_finetune/StageC_ForeSight_step_10000.pt'

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class SafetyController:
    def __init__(self):
        # å…³èŠ‚é™ä½ä¿æŠ¤
        self.joint_limits_min = np.array([-2.89, -1.76, -2.89, -3.07, -2.89, -0.01, -2.89]) + 0.01
        self.joint_limits_max = np.array([ 2.89,  1.76,  2.89, -0.06,  2.89,  3.75,  2.89]) - 0.01

    def clip_actions(self, actions_batch):
        actions_np = np.array(actions_batch)
        joints = actions_np[:, :7]
        gripper = actions_np[:, 7:]
        joints_clipped = np.clip(joints, self.joint_limits_min, self.joint_limits_max)
        return np.concatenate([joints_clipped, gripper], axis=1)

class RealTimeAgent:
    def __init__(self):
        self.device = DEVICE
        self.safety = SafetyController() 
        self.pred_horizon = 64
        self.history_len = 500       
        self.model_input_frames = 6  
        
        # ğŸŸ¢ [æ–°å¢] å®šä¹‰ç”¨äºè¾“å…¥è½¬æ¢çš„ç‰©ç†é˜ˆå€¼
        # è¿™æ˜¯ç‰©ç†ä¸–ç•Œä¸­åˆ¤æ–­å¼€é—­çš„ç•Œé™ (æ ¹æ®ä½ çš„ stats æ–‡ä»¶)
        self.PHYSICAL_GRIPPER_THRESHOLD = 0.0616 

        # 1. åŠ è½½ Tokenizer å¹¶é¢„åˆå§‹åŒ– text_tokens
        print(f"[Agent] Loading Tokenizer from {TOKENIZER_PATH}...")
        try:
            self.tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
        except:
            self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
            
        self.default_prompt = "pick up the orange ball and put it on the plank"
        
        self.text_tokens = self.tokenizer(
            self.default_prompt, 
            return_tensors="pt", 
            padding="max_length", 
            max_length=16, 
            truncation=True
        ).input_ids.to(self.device)

        # 2. å½’ä¸€åŒ–å‚æ•°åŠ è½½
        if not os.path.exists(STATS_PATH):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {STATS_PATH}")
        with open(STATS_PATH, 'r') as f:
            stats = json.load(f)
        self.action_mean = np.array(stats['action_mean'][:8], dtype=np.float32)
        self.action_std = np.maximum(np.array(stats['action_std'][:8], dtype=np.float32), 1e-2)

        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        
        # 3. åˆå§‹åŒ–æ¨¡å‹ç»“æ„
        self._init_models()
        self._init_scheduler()
        
        self.video_buffer = deque(maxlen=self.history_len)
        self.state_buffer = deque(maxlen=self.history_len)
        self.first_frame_tensor = None
        
        # 4. æ‰§è¡Œé¢„çƒ­
        self.warmup()

    # def _init_models(self):
    #     print(f"[Agent] Initializing models on {self.device}...")
    #     self.encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(self.device).eval()
    #     self.policy = RDTWrapper(action_dim=8, model_path=RDT_PATH, rdt_cond_dim=768, pred_horizon=64).to(self.device).eval()
        
    #     print(f"[Agent] Loading Checkpoint: {STAGE_C_PATH}")
    #     ckpt_c = torch.load(STAGE_C_PATH, map_location=self.device)
        
    #     # LoRA é…ç½®
    #     peft_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], lora_dropout=0.05, bias="none")
    #     self.policy.rdt_model = get_peft_model(self.policy.rdt_model, peft_config)
        
    #     if 'rdt_state_dict' in ckpt_c: self.policy.load_state_dict(ckpt_c['rdt_state_dict'], strict=False)
    #     else: self.policy.load_state_dict(ckpt_c, strict=False)
        
    #     if 'encoder_state_dict' in ckpt_c: self.encoder.load_state_dict(ckpt_c['encoder_state_dict'], strict=False)

    def _init_models(self):
        print(f"[Agent] Initializing models on {self.device}...")
        self.encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(self.device).eval()
        
        # ... (RDT åˆå§‹åŒ–ä»£ç ä¸å˜) ...
        self.policy = RDTWrapper(action_dim=8, model_path=RDT_PATH, rdt_cond_dim=768, pred_horizon=64).to(self.device).eval()
        
        # LoRA é…ç½® (ä¸å˜)
        peft_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], lora_dropout=0.05, bias="none")
        self.policy.rdt_model = get_peft_model(self.policy.rdt_model, peft_config)

        print(f"[Agent] Loading Checkpoint: {STAGE_C_PATH}")
        ckpt_c = torch.load(STAGE_C_PATH, map_location=self.device)
        
        # === ğŸ” ä¸¥è°¨çš„åŠ è½½æ£€æŸ¥ä»£ç  ===
        
        # 1. å‡†å¤‡ State Dict
        if 'rdt_state_dict' in ckpt_c:
            rdt_state_dict = ckpt_c['rdt_state_dict']
        else:
            rdt_state_dict = ckpt_c
            
        # 2. åŠ è½½å¹¶æ•è·è¿”å›ç»“æœ (strict=False)
        # load_result æ˜¯ä¸€ä¸ª namedtuple: (missing_keys, unexpected_keys)
        load_result = self.policy.load_state_dict(rdt_state_dict, strict=False)
        
        # 3. æ‰“å°åˆ†æ
        missing = load_result.missing_keys
        unexpected = load_result.unexpected_keys
        
        print("\n" + "="*50)
        print("ğŸ§ Checkpoint Loading Inspection")
        print("="*50)
        
        # æ£€æŸ¥ LoRA æ˜¯å¦åŠ è½½
        lora_keys = [k for k in missing if 'lora' in k]
        if len(lora_keys) > 0:
            print(f"âŒ è­¦å‘Š! LoRA å‚æ•°æœªåŠ è½½ (Missing {len(lora_keys)} keys):")
            print(f"   Example: {lora_keys[0]}")
        else:
            print("âœ… LoRA å‚æ•°å·²æˆåŠŸåŠ è½½ã€‚")

        # æ£€æŸ¥ Head (è¾“å‡ºå±‚) æ˜¯å¦åŠ è½½
        # é€šå¸¸ RDT çš„è¾“å‡ºå¤´åŒ…å« 'head' æˆ– 'final_layer'ï¼Œæ ¹æ®å…·ä½“æ¨¡å‹ç»“æ„è°ƒæ•´
        # è¿™é‡Œå‡è®¾æœ€åä¸€å±‚åŒ…å« 'linear' æˆ–ç‰¹å®šåç§°
        head_missing = [k for k in missing if 'head' in k or 'out_proj' in k] # ä¸¾ä¾‹
        if len(head_missing) > 0 and len(head_missing) < 10: # å…è®¸å°‘é‡ç¼ºå¤±ï¼Œä½†ä¸èƒ½å…¨ç¼º
             print(f"âš ï¸ æ³¨æ„: éƒ¨åˆ†è¾“å‡ºå±‚å‚æ•°ç¼ºå¤±: {head_missing}")
        
        # æ£€æŸ¥ Encoder
        if 'encoder_state_dict' in ckpt_c: 
            self.encoder.load_state_dict(ckpt_c['encoder_state_dict'], strict=False)
            print("âœ… Encoder (Adapter) weights loaded.")
        else:
            print("âš ï¸ Warning: No encoder_state_dict found in checkpoint!")

        print("="*50 + "\n")

    def _init_scheduler(self):
        self.scheduler = DDIMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2", prediction_type="epsilon", clip_sample=True)
        self.scheduler.set_timesteps(25)

    def warmup(self):
        print("ğŸ”¥ [System] Warming up model...")
        dummy_video = torch.randn(1, 2, 3, 6, 224, 224, device=self.device, dtype=torch.bfloat16)
        dummy_state = torch.randn(1, 1, 8, device=self.device, dtype=torch.float32)
        dummy_ff = torch.randn(1, 2, 3, 224, 224, device=self.device, dtype=torch.float32)
        try:
            with autocast('cuda', dtype=torch.bfloat16):
                feats = self.encoder(dummy_video, self.text_tokens, dummy_state, dummy_ff)
                feats["state"] = dummy_state[:, -1, :]
                latents = torch.randn(1, self.pred_horizon, 8, device=self.device)
                t_tensor = torch.tensor([0], device=self.device)
                _ = self.policy(latents, t_tensor, feats)
            print("âœ… Warmup done.")
        except Exception as e:
            print(f"âŒ Warmup failed: {e}")

    def preprocess_image(self, img_np):
        resized = cv2.resize(img_np, (224, 224))
        rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        tensor = torch.tensor(rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
        return self.normalize(tensor)

    # ğŸŸ¢ [æ–°å¢] è¾…åŠ©å‡½æ•°ï¼šå¤„ç†è¾“å…¥çš„ç‰©ç†çŠ¶æ€
    def _preprocess_qpos_for_model(self, current_qpos):
        """
        å°†æœºå™¨äººçš„ç‰©ç†çŠ¶æ€è½¬æ¢ä¸ºæ¨¡å‹ç†è§£çš„çŠ¶æ€ã€‚
        ç‰¹åˆ«æ˜¯å°†å¤¹çˆªçš„ç‰©ç†æ•°å€¼ (0.04~0.08) è½¬æ¢ä¸ºè®­ç»ƒæ—¶çš„äºŒå€¼ (1.0/-1.0)
        """
        if current_qpos is None: 
            qpos_new = np.zeros(8, dtype=np.float32)
        else:
            qpos_new = np.array(current_qpos, dtype=np.float32).copy()
            # è¡¥é½ç»´åº¦
            if len(qpos_new) == 7: 
                qpos_new = np.concatenate([qpos_new, [0.0]])
            elif len(qpos_new) > 8:
                 qpos_new = qpos_new[:8]
        
        # === å…³é”®è½¬æ¢ ===
        # å¦‚æœç‰©ç†å€¼ > 0.0616ï¼Œæ¨¡å‹åº”è¯¥çœ‹åˆ° 1.0 (Open)
        # å¦‚æœç‰©ç†å€¼ < 0.0616ï¼Œæ¨¡å‹åº”è¯¥çœ‹åˆ° -1.0 (Close)
        raw_gripper = qpos_new[7]
        if raw_gripper > self.PHYSICAL_GRIPPER_THRESHOLD:
            qpos_new[7] = 1.0
        else:
            qpos_new[7] = -1.0
            
        return qpos_new

    def reset_session(self, first_frame_img, current_qpos=None):
        print("[Agent] Resetting session...")
        self.video_buffer.clear()
        self.state_buffer.clear()
        
        # 1. å›¾åƒå¤„ç†
        wrist_tensor = self.preprocess_image(first_frame_img)
        main_fake = torch.zeros_like(wrist_tensor)
        self.first_frame_tensor = torch.stack([main_fake, wrist_tensor], dim=0).unsqueeze(0).to(self.device)
        
        # æ›´æ–° text_tokens
        self.text_tokens = self.tokenizer(self.default_prompt, return_tensors="pt", padding="max_length", max_length=16, truncation=True).input_ids.to(self.device)
        
        video_frame_unit = torch.stack([main_fake, wrist_tensor], dim=0)
        self.video_buffer.append(video_frame_unit)
        
        # 2. çŠ¶æ€å¤„ç† [ä¿®æ”¹ç‚¹]
        # å…ˆå°†ç‰©ç†çŠ¶æ€è½¬ä¸ºäºŒå€¼åŒ–çŠ¶æ€ï¼Œå†è¿›è¡Œå½’ä¸€åŒ–
        model_input_qpos = self._preprocess_qpos_for_model(current_qpos)
        
        print(f"   ğŸš© [Reset QPos] {current_qpos[:8]}")
        print(f"   ğŸš© [Input Check] Raw Grip: {current_qpos[7] if current_qpos is not None else 0:.4f} -> Model Input: {model_input_qpos[7]:.1f}")
        
        norm_qpos = (model_input_qpos - self.action_mean) / self.action_std
        self.state_buffer.append(norm_qpos)

    @torch.no_grad()
    def step(self, frames_list, current_qpos):
        # 1. æ›´æ–°å›¾åƒ Buffer
        for frame in frames_list:
            wrist_t = self.preprocess_image(frame)
            self.video_buffer.append(torch.stack([torch.zeros_like(wrist_t), wrist_t], dim=0)) 
        
        # 2. æ›´æ–°çŠ¶æ€ Buffer [ä¿®æ”¹ç‚¹]
        # åŒæ ·ï¼Œå¿…é¡»å…ˆäºŒå€¼åŒ–ï¼Œå†å½’ä¸€åŒ–
        model_input_qpos = self._preprocess_qpos_for_model(current_qpos)
        norm_qpos_np = (model_input_qpos - self.action_mean) / self.action_std
        self.state_buffer.append(norm_qpos_np)
        
        # 3. é‡‡æ ·è¾“å…¥
        curr_len = len(self.video_buffer)
        indices = np.linspace(0, curr_len - 1, self.model_input_frames).astype(int)
        vid_t = torch.stack([self.video_buffer[i] for i in indices]).to(self.device).permute(1, 2, 0, 3, 4).unsqueeze(0)
        state_t = torch.tensor(norm_qpos_np, dtype=torch.float32).view(1, 1, 8).to(self.device)
        
        # 4. æ¨ç†
        with autocast('cuda', dtype=torch.bfloat16):
            features = self.encoder(vid_t, self.text_tokens, state_t, self.first_frame_tensor)
            features["state"] = state_t[:, -1, :] 
            latents = torch.randn(1, self.pred_horizon, 8, device=self.device) 
            
            for t in self.scheduler.timesteps:
                model_input = self.scheduler.scale_model_input(latents, t)
                t_tensor = torch.tensor([t], device=self.device)
                noise_pred = self.policy(model_input, t_tensor, features)
                latents = self.scheduler.step(noise_pred, t, latents).prev_sample
            
        # 5. åå½’ä¸€åŒ–
        action_pred_np = latents[0].float().cpu().numpy()
        denormalized_actions = action_pred_np * self.action_std + self.action_mean
        
        # === ğŸŸ¢ [æ ¸å¿ƒä¿®æ”¹] è¾“å‡ºäºŒå€¼åŒ–é€»è¾‘ ===
        # æ¨¡å‹é¢„æµ‹å€¼ > 0.0 -> è¾“å‡º 1.0 (Open)
        # æ¨¡å‹é¢„æµ‹å€¼ <= 0.0 -> è¾“å‡º -1.0 (Close)
        # è¿™æ ·å®¢æˆ·ç«¯æ¥æ”¶åˆ°æ˜ç¡®çš„ä¿¡å·ï¼Œä¸ä¼šæ”¶åˆ° 0.3 è¿™ç§ä¸­é—´å€¼
        raw_gripper_pred = denormalized_actions[:, 7]
        denormalized_actions[:, 7] = np.where(raw_gripper_pred > 0.0, 1.0, -1.0)
        
        # å®‰å…¨è£å‰ªå¹¶è¿”å›
        return self.safety.clip_actions(denormalized_actions).tolist()





# #egoåŒè§†è§’
# import torch
# import cv2
# import json
# import numpy as np
# from collections import deque
# from diffusers import DDIMScheduler
# import os
# import h5py
# from torch.amp import autocast
# from peft import LoraConfig, get_peft_model
# from transformers import T5Tokenizer
# import torch._dynamo
# from torchvision import transforms
# import time

# # === å¯¼å…¥ä½ çš„æ¨¡å‹ ===
# from model.fusion_encoder import FusionEncoder
# from model.rdt_model import RDTWrapper

# # === åŸºç¡€è·¯å¾„é…ç½® ===
# VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
# RDT_PATH = '/yanghaochuan/models/rdt-1b'
# STATS_PATH = "/yanghaochuan/data/121dataset_stats.json" 
# TOKENIZER_PATH = "/yanghaochuan/models/flan-t5-large"
# STAGE_C_PATH = '/yanghaochuan/121checkpoints_finetune/StageC_ForeSight_step_7000.pt'
# ANCHOR_DATA_PATH = '/yanghaochuan/data/hdf5/pick_up_the_orange_ball_and_put_it_on_the_plank.hdf5'

# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# class SafetyController:
#     def __init__(self):
#         self.joint_limits_min = np.array([-2.89, -1.76, -2.89, -3.07, -2.89, -0.01, -2.89]) + 0.01
#         self.joint_limits_max = np.array([ 2.89,  1.76,  2.89, -0.06,  2.89,  3.75,  2.89]) - 0.01

#     def clip_actions(self, actions_batch):
#         actions_np = np.array(actions_batch)
#         joints = actions_np[:, :7]
#         gripper = actions_np[:, 7:]
#         joints_clipped = np.clip(joints, self.joint_limits_min, self.joint_limits_max)
#         return np.concatenate([joints_clipped, gripper], axis=1)

# class RealTimeAgent:
#     def __init__(self):
#         self.device = DEVICE
#         self.safety = SafetyController() 
#         self.pred_horizon = 64
#         self.history_len = 500       
#         self.model_input_frames = 6 
        
#         self.debug_dir = f"debug_visuals_{int(time.time())}"
#         os.makedirs(self.debug_dir, exist_ok=True)
#         self.step_counter = 0

#         # === 1. åˆå§‹åŒ–å¯¹é½å™¨å˜é‡ ===
#         self.trajectory_offset = None # ç”¨äºå­˜å‚¨ (Model_Start - Real_Start) çš„åå·®

#         print(f"[Agent] Loading Tokenizer from {TOKENIZER_PATH}...")
#         try:
#             self.tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
#         except:
#             self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
#         if not os.path.exists(STATS_PATH):
#             raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {STATS_PATH}")
#         with open(STATS_PATH, 'r') as f:
#             stats = json.load(f)
        
#         mean_raw = np.array(stats['action_mean'], dtype=np.float32)
#         std_raw = np.array(stats['action_std'], dtype=np.float32)
        
#         if mean_raw.shape[0] > 8:
#             self.action_mean = mean_raw[:8]
#             self.action_std = std_raw[:8]
#         elif mean_raw.shape[0] == 7:
#             self.action_mean = np.concatenate([mean_raw, [0.0]])
#             self.action_std = np.concatenate([std_raw, [1.0]])
#         else:
#             self.action_mean = mean_raw
#             self.action_std = std_raw
            
#         self.action_std = np.maximum(self.action_std, 1e-2)

#         # ä¿ç•™ä¹‹å‰çš„ç»Ÿè®¡å­¦è¡¥ä¸ï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸
#         PATCH_STD_VAL = 0.5
#         if self.action_std[3] < PATCH_STD_VAL: self.action_std[3] = PATCH_STD_VAL
#         if self.action_std[5] < PATCH_STD_VAL: self.action_std[5] = PATCH_STD_VAL

#         self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
#         self.anchor_main_tensor = self._load_anchor_image()

#         self._init_models()
#         self._init_scheduler()
        
#         self.video_buffer = deque(maxlen=self.history_len)
#         self.state_buffer = deque(maxlen=self.history_len)
#         self.first_frame_tensor = None
#         self.text_tokens = None 
#         self.default_prompt = "pick up the orange ball and put it on the plank"
        
#         self.warmup()

#     def _load_anchor_image(self):
#         print(f"ğŸ“¥ [Agent] Loading Anchor Image from {ANCHOR_DATA_PATH}...")
#         try:
#             with h5py.File(ANCHOR_DATA_PATH, 'r') as f:
#                 demo_grp = f['data']['demo_0'] 
#                 main_key = 'agentview_image' if 'agentview_image' in demo_grp['obs'] else 'agentview_rgb'
#                 img_np = demo_grp['obs'][main_key][0]
#                 cv2.imwrite("debug_anchor_main.jpg", cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR))
#                 return self.preprocess_image(img_np)
#         except Exception as e:
#             print(f"âš ï¸ Anchor Load Failed: {e}")
#             return None

#     def _init_models(self):
#         print(f"[Agent] Initializing models on {self.device}...")
#         self.encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(self.device).eval()
#         self.policy = RDTWrapper(action_dim=8, model_path=RDT_PATH, rdt_cond_dim=768, pred_horizon=64).to(self.device).eval()
        
#         ckpt_c = torch.load(STAGE_C_PATH, map_location=self.device)
#         peft_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], lora_dropout=0.05, bias="none")
#         self.policy.rdt_model = get_peft_model(self.policy.rdt_model, peft_config)
        
#         if 'rdt_state_dict' in ckpt_c: self.policy.load_state_dict(ckpt_c['rdt_state_dict'], strict=False)
#         else: self.policy.load_state_dict(ckpt_c, strict=False)
        
#         if 'encoder_state_dict' in ckpt_c: 
#             state_dict = ckpt_c['encoder_state_dict']
#             new_state_dict = {}
#             for k, v in state_dict.items():
#                 k_clean = k.replace("_orig_mod.", "").replace("module.", "")
#                 new_state_dict[k_clean] = v
#             self.encoder.load_state_dict(new_state_dict, strict=False)

#     def _init_scheduler(self):
#         self.scheduler = DDIMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2", prediction_type="epsilon", clip_sample=True)
#         self.inference_steps = 25
#         self.scheduler.set_timesteps(self.inference_steps)

#     def warmup(self):
#         print("ğŸ”¥ [System] Warming up model...")
#         dummy_video = torch.randn(1, 2, 3, 6, 224, 224, device=self.device, dtype=torch.bfloat16)
#         dummy_text = torch.randint(0, 1000, (1, 16), device=self.device)
#         dummy_state = torch.randn(1, 1, 8, device=self.device, dtype=torch.float32)
#         dummy_ff = torch.randn(1, 2, 3, 224, 224, device=self.device, dtype=torch.float32)
#         try:
#             with autocast('cuda', dtype=torch.bfloat16):
#                 feats = self.encoder(dummy_video, dummy_text, dummy_state, dummy_ff)
#                 feats["state"] = dummy_state[:, -1, :]
#                 latents = torch.randn(1, self.pred_horizon, 8, device=self.device)
#                 t = torch.tensor([0], device=self.device)
#                 _ = self.policy(latents, t, feats)
#             print("âœ… Warmup done.")
#         except Exception as e:
#             print(f"âŒ Warmup failed: {e}")

#     def preprocess_image(self, img_np):
#         resized = cv2.resize(img_np, (224, 224))
#         rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
#         tensor = torch.tensor(rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
#         tensor = self.normalize(tensor) 
#         return tensor

#     def save_model_input_visuals(self, vid_tensor, step_idx):
#         try:
#             wrist_t = vid_tensor[0, 1] 
#             mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1, 1).to(wrist_t.device)
#             std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1, 1).to(wrist_t.device)
#             wrist_t = wrist_t * std + mean
#             wrist_t = torch.clamp(wrist_t, 0, 1)
#             imgs = wrist_t.permute(1, 2, 3, 0).detach().cpu().numpy()
#             imgs = (imgs * 255).astype(np.uint8)
#             concat_img = np.hstack([imgs[i] for i in range(6)])
#             concat_img = cv2.cvtColor(concat_img, cv2.COLOR_RGB2BGR)
#             save_path = os.path.join(self.debug_dir, f"step_{step_idx:04d}_buffer.jpg")
#             cv2.imwrite(save_path, concat_img)
#         except Exception: pass

#     def reset_session(self, first_frame_img, current_qpos=None):
#         print("[Agent] Resetting session (Cold Start)...")
#         self.video_buffer.clear()
#         self.state_buffer.clear()
        
#         # é‡ç½®å¯¹é½åå·®ï¼Œè¿™å°†åœ¨ç¬¬ä¸€æ¬¡ step æ—¶é‡æ–°è®¡ç®—
#         self.trajectory_offset = None
        
#         wrist_tensor = self.preprocess_image(first_frame_img)
        
#         if self.anchor_main_tensor is not None:
#             main_fake = self.anchor_main_tensor.clone()
#         else:
#             main_fake = torch.zeros_like(wrist_tensor)
            
#         self.first_frame_tensor = torch.stack([main_fake, wrist_tensor], dim=0).unsqueeze(0).to(self.device)
        
#         tokens = self.tokenizer(self.default_prompt, return_tensors="pt", padding="max_length", max_length=16, truncation=True).input_ids
#         self.text_tokens = tokens.to(self.device)
        
#         video_frame_unit = torch.stack([main_fake, wrist_tensor], dim=0)
            
#         if current_qpos is None: current_qpos = np.zeros(8)
#         else: 
#             if len(current_qpos) == 7: current_qpos = list(current_qpos) + [0.0]
#             current_qpos = np.array(current_qpos, dtype=np.float32)
            
#         norm_qpos = (current_qpos - self.action_mean) / self.action_std
        
#         for _ in range(self.model_input_frames):
#             self.video_buffer.append(video_frame_unit)
#             self.state_buffer.append(norm_qpos)
            
#         print(f"   ğŸš© [Reset QPos] {current_qpos[:6]}")

#     @torch.no_grad()
#     def step(self, frames_list, current_qpos):
#         # 1. æ›´æ–° Buffer
#         for frame in frames_list:
#             wrist_tensor = self.preprocess_image(frame)
#             if self.anchor_main_tensor is not None:
#                 main_fake = self.anchor_main_tensor.clone()
#             else:
#                 main_fake = torch.zeros_like(wrist_tensor)
            
#             combined_frame = torch.stack([main_fake, wrist_tensor], dim=0)
#             self.video_buffer.append(combined_frame) 
        
#         if len(current_qpos) == 7: current_qpos = list(current_qpos) + [0.0]
#         qpos_np = np.array(current_qpos, dtype=np.float32)
#         norm_qpos_np = (qpos_np - self.action_mean) / self.action_std
        
#         for _ in range(len(frames_list)):
#             self.state_buffer.append(norm_qpos_np)
        
#         # 2. å‡†å¤‡è¾“å…¥
#         curr_len = len(self.video_buffer)
#         indices = np.linspace(0, curr_len - 1, self.model_input_frames).astype(int)
#         selected_frames = [self.video_buffer[i] for i in indices]
        
#         vid_t = torch.stack(selected_frames).to(self.device)
#         vid_t = vid_t.permute(1, 2, 0, 3, 4).unsqueeze(0) 

#         self.save_model_input_visuals(vid_t, self.step_counter)
#         self.step_counter += 1

#         state_t = torch.tensor(norm_qpos_np, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)
        
#         # 3. æ¨¡å‹æ¨ç†
#         self.scheduler.set_timesteps(self.inference_steps)
#         with autocast('cuda', dtype=torch.bfloat16):
#             features = self.encoder(vid_t, self.text_tokens, state_t, self.first_frame_tensor)
#             features["state"] = state_t[:, -1, :] 
#             latents = torch.randn(1, self.pred_horizon, 8, device=self.device) 
            
#             for t in self.scheduler.timesteps:
#                 model_input = self.scheduler.scale_model_input(latents, t)
#                 t_tensor = torch.tensor([t], device=self.device)
#                 noise_pred = self.policy(model_input, t_tensor, features)
#                 latents = self.scheduler.step(noise_pred, t, latents).prev_sample
            
#         normalized_actions = latents[0].float()
#         action_pred_np = normalized_actions.detach().cpu().numpy()
#         denormalized_actions = action_pred_np * self.action_std + self.action_mean

#         # =========================================================
#         # ğŸŸ¢ [æ ¸å¿ƒä¿®å¤] è½¨è¿¹å¯¹é½å™¨ (Trajectory Aligner)
#         # =========================================================
#         # é€»è¾‘ï¼šåœ¨ Reset åçš„ç¬¬ä¸€æ­¥ï¼Œè®¡ç®— Model è®¤ä¸ºçš„èµ·å§‹ç‚¹ä¸çœŸå®èµ·å§‹ç‚¹çš„å·®å€¼ (Offset)
#         # ç„¶åå°†è¿™ä¸ª Offset åº”ç”¨åˆ°æ•´æ¡è½¨è¿¹ï¼Œå¼ºåˆ¶ Step 0 == Real Position
#         # =========================================================
        
#         if self.trajectory_offset is None:
#             # 1. è·å–æ¨¡å‹é¢„æµ‹çš„ Step 0 (åªçœ‹å‰ 7 ä¸ªå…³èŠ‚ï¼Œä¸æ”¹å¤¹çˆª)
#             pred_start = denormalized_actions[0, :7]
#             real_start = qpos_np[:7]
            
#             # 2. è®¡ç®—åå·®: Model - Real
#             self.trajectory_offset = pred_start - real_start
            
#             print(f"\n   ğŸ”§ [Aligner] Calibration Done.")
#             print(f"      Real Start:  {real_start[3]:.3f} (J3), {real_start[5]:.3f} (J5)")
#             print(f"      Model Start: {pred_start[3]:.3f} (J3), {pred_start[5]:.3f} (J5)")
#             print(f"      Offset:      {self.trajectory_offset[3]:.3f} (J3), {self.trajectory_offset[5]:.3f} (J5)")
#             print(f"      >> Applying negative offset to align trajectory.\n")

#         # åº”ç”¨å¯¹é½ (Subtract Offset)
#         # åªå¯¹å…³èŠ‚åº”ç”¨ï¼Œä¸å¯¹å¤¹çˆªåº”ç”¨
#         denormalized_actions[:, :7] -= self.trajectory_offset
        
#         # =========================================================

#         # å¤¹çˆªäºŒå€¼åŒ–
#         GRIPPER_OPEN_VAL = 0.0804  
#         GRIPPER_CLOSE_VAL = 0.0428 
#         GRIPPER_THRESHOLD = 0.0616 

#         raw_gripper_pred = denormalized_actions[:, 7]
#         binary_gripper = np.where(raw_gripper_pred > GRIPPER_THRESHOLD, GRIPPER_OPEN_VAL, GRIPPER_CLOSE_VAL)
#         denormalized_actions[:, 7] = binary_gripper
        
#         print(f"   >>> [Infer] BufferLen: {curr_len} | Pred J0: {denormalized_actions[0,0]:.3f} | J3: {denormalized_actions[0,3]:.3f} | J5: {denormalized_actions[0,5]:.3f}", end='\r')
        
#         safe_actions = self.safety.clip_actions(denormalized_actions)
#         return safe_actions.tolist()