# import torch
# import cv2
# import json
# import numpy as np
# from collections import deque
# from diffusers import DDIMScheduler
# import os
# from torch.amp import autocast
# from peft import LoraConfig, get_peft_model
# from transformers import T5Tokenizer
# import torch._dynamo

# # === å¯¼å…¥ä½ çš„æ¨¡å‹ ===
# from model.fusion_encoder import FusionEncoder
# from model.rdt_model import RDTWrapper

# # === åŸºç¡€è·¯å¾„é…ç½® ===
# VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
# RDT_PATH = '/yanghaochuan/models/rdt-1b'
# STATS_PATH = "/yanghaochuan/data/13dataset_stats.json"
# TOKENIZER_PATH = "/yanghaochuan/models/flan-t5-large"

# # [ä¿®æ”¹ç‚¹] æŒ‡å‘ Stage C çš„ checkpoint
# STAGE_C_PATH = '/yanghaochuan/checkpoints/12stageC_step_4000.pt'

# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# # =============================================================================
# # ğŸ›¡ï¸ å®‰å…¨æ§åˆ¶å™¨
# # =============================================================================
# class SafetyController:
#     def __init__(self):
#         # Franka å…³èŠ‚æé™ (å®‰å…¨ä½™é‡ 0.05)
#         self.joint_limits_min = np.array([-2.89, -1.76, -2.89, -3.07, -2.89, -0.01, -2.89]) + 0.05
#         self.joint_limits_max = np.array([ 2.89,  1.76,  2.89, -0.06,  2.89,  3.75,  2.89]) - 0.05

#     def clip_actions(self, actions_batch):
#         actions_np = np.array(actions_batch)
#         joints = actions_np[:, :7]
#         gripper = actions_np[:, 7:]
#         # å…³èŠ‚é™ä½
#         joints_clipped = np.clip(joints, self.joint_limits_min, self.joint_limits_max)
#         return np.concatenate([joints_clipped, gripper], axis=1)

# # =============================================================================
# # ğŸ¤– å®æ—¶æ¨ç† Agent
# # =============================================================================
# class RealTimeAgent:
#     def __init__(self):
#         self.device = DEVICE
#         self.safety = SafetyController() 
#         self.pred_horizon = 64  # æ˜ç¡®æ¨¡å‹é¢„æµ‹é•¿åº¦

#         print(f"[Agent] Loading Tokenizer from {TOKENIZER_PATH}...")
#         try:
#             self.tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
#         except:
#             self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
#         # --- 1. åŠ è½½ç»Ÿè®¡æ•°æ®å¹¶ä¿®æ­£ç»´åº¦ ---
#         if not os.path.exists(STATS_PATH):
#             raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {STATS_PATH}")
#         with open(STATS_PATH, 'r') as f:
#             stats = json.load(f)
        
#         mean_raw = np.array(stats['action_mean'], dtype=np.float32)
#         std_raw = np.array(stats['action_std'], dtype=np.float32)
        
#         # å¼ºåˆ¶æˆªæ–­æˆ–è¡¥é½åˆ° 8 ç»´ (7 Joint + 1 Gripper)
#         if mean_raw.shape[0] > 8:
#             print(f"[Agent] âš ï¸ ç»Ÿè®¡æ•°æ®ç»´åº¦ {mean_raw.shape[0]} > 8ï¼Œè¿›è¡Œæˆªæ–­ã€‚")
#             self.action_mean = mean_raw[:8]
#             self.action_std = std_raw[:8]
#         elif mean_raw.shape[0] == 7:
#             print(f"[Agent] âš ï¸ ç»Ÿè®¡æ•°æ®ç»´åº¦ä¸º 7ï¼Œè¡¥é½ Gripper=0ã€‚")
#             self.action_mean = np.concatenate([mean_raw, [0.0]])
#             self.action_std = np.concatenate([std_raw, [1.0]])
#         else:
#             self.action_mean = mean_raw
#             self.action_std = std_raw
            
#         self.action_std = np.maximum(self.action_std, 1e-2)
        
#         self._init_models()
#         self._init_scheduler()
        
#         self.window_size = 16
#         # ä½¿ç”¨ deque è‡ªåŠ¨ç»´æŠ¤æ»‘åŠ¨çª—å£
#         self.video_buffer = deque(maxlen=self.window_size)
#         self.state_buffer = deque(maxlen=self.window_size)
        
#         self.first_frame_tensor = None
#         self.text_tokens = None 
        
#         self.default_prompt = "pick up the orange ball"
#         print(f"[Agent] Prompt: '{self.default_prompt}'")

#     def _init_models(self):
#         print(f"[Agent] Initializing models on {self.device}...")
#         try:
#             # Init Base Models
#             self.encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(self.device).eval()
#             self.policy = RDTWrapper(action_dim=8, model_path=RDT_PATH, rdt_cond_dim=768, pred_horizon=64).to(self.device).eval()
            
#             # Load Stage C Checkpoint
#             print(f"[Agent] ğŸš€ Loading Joint Checkpoint: {STAGE_C_PATH}")
#             ckpt_c = torch.load(STAGE_C_PATH, map_location=self.device)

#             # Load Policy (LoRA)
#             peft_config = LoraConfig(
#                 r=16, lora_alpha=32,
#                 target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], 
#                 lora_dropout=0.05, bias="none"
#             )
#             self.policy.rdt_model = get_peft_model(self.policy.rdt_model, peft_config)
            
#             if 'rdt_state_dict' in ckpt_c:
#                 self.policy.load_state_dict(ckpt_c['rdt_state_dict'], strict=False)
#             else:
#                 self.policy.load_state_dict(ckpt_c, strict=False)
#             print("[Agent] âœ… Policy weights loaded.")

#             # Load Encoder (Joint Finetuned)
#             if 'encoder_state_dict' in ckpt_c:
#                 self.encoder.load_state_dict(ckpt_c['encoder_state_dict'], strict=False)
#                 print("[Agent] âœ… Encoder weights loaded from Stage C.")
#             else:
#                 raise ValueError(f"âŒ ä¸¥é‡é”™è¯¯: {STAGE_C_PATH} ä¸­æ²¡æœ‰ 'encoder_state_dict'ï¼")
            
#             # Compile (Optional)
#             print("[Agent] Compiling FusionEncoder...")
#             torch._dynamo.config.suppress_errors = True
#             try:
#                 self.encoder = torch.compile(self.encoder, mode="default")
#             except Exception as e:
#                 print(f"[Warning] Encoder compile failed: {e}")

#         except Exception as e:
#             print(f"[Error] Model Init Failed: {e}")
#             raise e

#     def _init_scheduler(self):
#         self.scheduler = DDIMScheduler(
#             num_train_timesteps=1000,
#             beta_schedule="squaredcos_cap_v2",
#             prediction_type="epsilon", 
#             clip_sample=True
#         )
#         self.inference_steps = 25
#         self.scheduler.set_timesteps(self.inference_steps)

#     def reset_session(self, first_frame_img, current_qpos=None):
#         """
#         é‡ç½®ä¼šè¯ï¼Œå®ç°å†·å¯åŠ¨é€»è¾‘
#         :param first_frame_img: å…¨å±€é¦–å¸§ï¼ˆRGBï¼ŒHWCï¼‰
#         :param current_qpos: åˆå§‹æœºæ¢°è‡‚çŠ¶æ€ï¼ˆå¯é€‰ï¼Œ7æˆ–8ç»´ï¼‰
#         """
#         print("[Agent] Resetting session (Cold Start)...")
#         self.video_buffer.clear()
#         self.state_buffer.clear()
        
#         # 1. å¤„ç†é¦–å¸§ (Context) - è¿™æ˜¯æˆ‘ä»¬çš„é”šç‚¹
#         ff_resized = cv2.resize(first_frame_img, (224, 224))
#         ff_rgb = cv2.cvtColor(ff_resized, cv2.COLOR_BGR2RGB)
#         wrist_tensor = torch.tensor(ff_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
        
#         # æ„é€ å…¨é»‘ä¸»æ‘„ (Inference Mode: Main is always black)
#         main_fake = torch.zeros_like(wrist_tensor)
        
#         # Context Frame: [2, 3, 224, 224]
#         self.first_frame_tensor = torch.stack([main_fake, wrist_tensor], dim=0).unsqueeze(0).to(self.device)
        
#         # 2. ç¼–ç æŒ‡ä»¤
#         tokens = self.tokenizer(
#             self.default_prompt, return_tensors="pt", padding="max_length", max_length=16, truncation=True
#         ).input_ids
#         self.text_tokens = tokens.to(self.device)
        
#         # 3. é¢„å¡«å…… Video Buffer (å†·å¯åŠ¨ï¼šå¤åˆ¶é¦–å¸§ 16 æ¬¡)
#         # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾åˆå§‹æ—¶åˆ»æœºå™¨äººè§†è§’ä¹Ÿè¿‘ä¼¼äºé¦–å¸§ï¼ˆæˆ–è€…æˆ‘ä»¬æ²¡æœ‰æ›´å¥½çš„å†å²ï¼‰
#         # å®é™…æ¨ç†ä¸­ï¼ŒMain View æ˜¯å…¨é»‘çš„ï¼ŒWrist View æ˜¯å½“å‰çš„ RGB
#         # è¿™é‡Œä¸ºäº†ç®€ä¾¿ï¼ŒVideo Buffer é‡Œçš„ Main ä¹Ÿæ˜¯å…¨é»‘ï¼ŒWrist æ˜¯é¦–å¸§å›¾åƒ
#         video_frame_unit = torch.stack([main_fake, wrist_tensor], dim=0) # [2, 3, H, W]
        
#         for _ in range(self.window_size):
#             self.video_buffer.append(video_frame_unit) 
            
#         # 4. é¢„å¡«å…… State Buffer
#         if current_qpos is None:
#             current_qpos = np.zeros(8)
#         else:
#              if len(current_qpos) == 7: current_qpos = list(current_qpos) + [0.0]
#              current_qpos = np.array(current_qpos, dtype=np.float32)

#         # å½’ä¸€åŒ–åˆå§‹çŠ¶æ€
#         norm_qpos = (current_qpos - self.action_mean) / self.action_std
#         for _ in range(self.window_size):
#             self.state_buffer.append(norm_qpos)

#     @torch.no_grad()
#     def step(self, current_frame, current_qpos):
#         """
#         å•æ­¥æ¨ç†
#         :param current_frame: å½“å‰æ‰‹è…•ç›¸æœºå›¾åƒ (RGB)
#         :param current_qpos: å½“å‰æœºæ¢°è‡‚å…³èŠ‚çŠ¶æ€
#         """
#         # 1. Image Preprocess (Wrist View)
#         frame_resized = cv2.resize(current_frame, (224, 224))
#         frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
#         wrist_tensor = torch.tensor(frame_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
        
#         # Main View is ALWAYS Black during inference (Student Mode)
#         main_fake = torch.zeros_like(wrist_tensor)
#         combined_frame = torch.stack([main_fake, wrist_tensor], dim=0) # [2, 3, 224, 224]
        
#         # =========================================================================
#         # [æ ¸å¿ƒä¿®å¤] æ»‘åŠ¨çª—å£ç­–ç•¥ (Sliding Window Strategy)
#         # ä¸è¦ clear()! åªæ˜¯ appendï¼Œdeque ä¼šè‡ªåŠ¨æŒ¤å‡ºæœ€æ—§çš„ä¸€å¸§ã€‚
#         # è¿™æ ·ä¿ç•™äº†æ—¶åºåŠ¨æ€ä¿¡æ¯ã€‚
#         # =========================================================================
#         self.video_buffer.append(combined_frame)
        
#         # 2. State Preprocess
#         if len(current_qpos) == 7:
#             current_qpos = list(current_qpos) + [0.0]
        
#         qpos_np = np.array(current_qpos, dtype=np.float32)
#         norm_qpos_np = (qpos_np - self.action_mean) / self.action_std
        
#         self.state_buffer.append(norm_qpos_np)
        
#         # 3. Batch Construction
#         # Video: [1, 2, 3, 16, 224, 224]
#         # list(deque) ä¼šæŒ‰æ—¶åºè¿”å›åˆ—è¡¨
#         vid_t = torch.stack(list(self.video_buffer)).to(self.device)
#         vid_t = vid_t.permute(1, 2, 0, 3, 4).unsqueeze(0)
        
#         # State: [1, 16, 8]
#         state_t = torch.tensor(np.array(list(self.state_buffer)), dtype=torch.float32).unsqueeze(0).to(self.device)
        
#         # 4. Inference
#         self.scheduler.set_timesteps(self.inference_steps)
        
#         with autocast('cuda', dtype=torch.bfloat16):
#             # Encoder æ¥æ”¶: 
#             # - vid_t: å½“å‰æ»‘çª—è§†é¢‘ (Mainé»‘, Wristå®)
#             # - first_frame_tensor: å®Œç¾çš„å…¨å±€é¦–å¸§ (Context Anchor)
#             features = self.encoder(vid_t, self.text_tokens, state_t, self.first_frame_tensor)
            
#             # RDT é¢„æµ‹æœªæ¥ 64 æ­¥
#             latents = torch.randn(1, self.pred_horizon, 8, device=self.device) 
            
#             for t in self.scheduler.timesteps:
#                 model_input = self.scheduler.scale_model_input(latents, t)
#                 t_tensor = torch.tensor([t], device=self.device)
#                 noise_pred = self.policy(model_input, t_tensor, features)
#                 latents = self.scheduler.step(noise_pred, t, latents).prev_sample
            
#         # 5. Post-process
#         # æˆ‘ä»¬é€šå¸¸åªå–ç¬¬ä¸€æ­¥æˆ–å‰å‡ æ­¥åŠ¨ä½œæ‰§è¡Œ (Receding Horizon Control)
#         # è¿™é‡Œè¿”å›å®Œæ•´çš„ 64 æ­¥ï¼Œç”± robot_policy_system å†³å®šæ‰§è¡Œå¤šå°‘æ­¥
#         normalized_actions = latents[0].float()
        
#         action_pred_np = normalized_actions.detach().cpu().numpy() # [64, 8]
#         denormalized_actions = action_pred_np * self.action_std + self.action_mean
        
#         # å®‰å…¨é™ä½
#         safe_actions = self.safety.clip_actions(denormalized_actions)
#         return safe_actions.tolist()


import torch
import cv2
import json
import numpy as np
from collections import deque
from diffusers import DDIMScheduler
import os
from torch.amp import autocast
from peft import LoraConfig, get_peft_model
from transformers import T5Tokenizer
import torch._dynamo

# === å¯¼å…¥ä½ çš„æ¨¡å‹ ===
from model.fusion_encoder import FusionEncoder
from model.rdt_model import RDTWrapper

# === åŸºç¡€è·¯å¾„é…ç½® ===
VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
RDT_PATH = '/yanghaochuan/models/rdt-1b'
STATS_PATH = "/yanghaochuan/data/13dataset_stats.json"
TOKENIZER_PATH = "/yanghaochuan/models/flan-t5-large"
STAGE_C_PATH = '/yanghaochuan/checkpoints/12stageC_step_4000.pt'

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class SafetyController:
    def __init__(self):
        self.joint_limits_min = np.array([-2.89, -1.76, -2.89, -3.07, -2.89, -0.01, -2.89]) + 0.01
        self.joint_limits_max = np.array([ 2.89,  1.76,  2.89, -0.06,  2.89,  3.75,  2.89]) - 0.01

    def clip_actions(self, actions_batch):
        actions_np = np.array(actions_batch)
        joints = actions_np[:, :7]
        gripper = actions_np[:, 7:]
        joints_clipped = np.clip(joints, self.joint_limits_min, self.joint_limits_max)
        return np.concatenate([joints_clipped, gripper], axis=1)

class RealTimeAgent:
    def __init__(self):
        self.device = DEVICE
        self.safety = SafetyController() 
        self.pred_horizon = 64

        print(f"[Agent] Loading Tokenizer from {TOKENIZER_PATH}...")
        try:
            self.tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
        except:
            self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
        if not os.path.exists(STATS_PATH):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {STATS_PATH}")
        with open(STATS_PATH, 'r') as f:
            stats = json.load(f)
        
        mean_raw = np.array(stats['action_mean'], dtype=np.float32)
        std_raw = np.array(stats['action_std'], dtype=np.float32)
        
        if mean_raw.shape[0] > 8:
            self.action_mean = mean_raw[:8]
            self.action_std = std_raw[:8]
        elif mean_raw.shape[0] == 7:
            self.action_mean = np.concatenate([mean_raw, [0.0]])
            self.action_std = np.concatenate([std_raw, [1.0]])
        else:
            self.action_mean = mean_raw
            self.action_std = std_raw
            
        self.action_std = np.maximum(self.action_std, 1e-2)
        
        self._init_models()
        self._init_scheduler()
        
        self.window_size = 16
        self.video_buffer = deque(maxlen=self.window_size)
        self.state_buffer = deque(maxlen=self.window_size)
        self.first_frame_tensor = None
        self.text_tokens = None 
        self.default_prompt = "pick up the orange ball"

    def _init_models(self):
        # ... (æ¨¡å‹åˆå§‹åŒ–ä»£ç ä¿æŒä¸å˜) ...
        print(f"[Agent] Initializing models on {self.device}...")
        self.encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(self.device).eval()
        self.policy = RDTWrapper(action_dim=8, model_path=RDT_PATH, rdt_cond_dim=768, pred_horizon=64).to(self.device).eval()
        ckpt_c = torch.load(STAGE_C_PATH, map_location=self.device)
        peft_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], lora_dropout=0.05, bias="none")
        self.policy.rdt_model = get_peft_model(self.policy.rdt_model, peft_config)
        
        if 'rdt_state_dict' in ckpt_c: self.policy.load_state_dict(ckpt_c['rdt_state_dict'], strict=False)
        else: self.policy.load_state_dict(ckpt_c, strict=False)
        
        if 'encoder_state_dict' in ckpt_c: self.encoder.load_state_dict(ckpt_c['encoder_state_dict'], strict=False)
        else: raise ValueError(f"âŒ No encoder_state_dict in {STAGE_C_PATH}")
        
        torch._dynamo.config.suppress_errors = True
        try: self.encoder = torch.compile(self.encoder, mode="default")
        except: pass

    def _init_scheduler(self):
        self.scheduler = DDIMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2", prediction_type="epsilon", clip_sample=True)
        self.inference_steps = 25
        self.scheduler.set_timesteps(self.inference_steps)

    def reset_session(self, first_frame_img, current_qpos=None):
        print("[Agent] Resetting session (Cold Start)...")
        self.video_buffer.clear()
        self.state_buffer.clear()
        
        ff_resized = cv2.resize(first_frame_img, (224, 224))
        ff_rgb = cv2.cvtColor(ff_resized, cv2.COLOR_BGR2RGB)
        wrist_tensor = torch.tensor(ff_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
        main_fake = torch.zeros_like(wrist_tensor)
        self.first_frame_tensor = torch.stack([main_fake, wrist_tensor], dim=0).unsqueeze(0).to(self.device)
        
        tokens = self.tokenizer(self.default_prompt, return_tensors="pt", padding="max_length", max_length=16, truncation=True).input_ids
        self.text_tokens = tokens.to(self.device)
        
        # Buffer åˆå§‹åŒ– (è™½ç„¶é©¬ä¸Šä¼šè¢« step è¦†ç›–ï¼Œä½†ä¸ºäº†å®‰å…¨å…ˆå¡«æ»¡)
        video_frame_unit = torch.stack([main_fake, wrist_tensor], dim=0) 
        for _ in range(self.window_size):
            self.video_buffer.append(video_frame_unit) 
            
        if current_qpos is None: current_qpos = np.zeros(8)
        else: 
            if len(current_qpos) == 7: current_qpos = list(current_qpos) + [0.0]
            current_qpos = np.array(current_qpos, dtype=np.float32)
        norm_qpos = (current_qpos - self.action_mean) / self.action_std
        for _ in range(self.window_size):
            self.state_buffer.append(norm_qpos)

    @torch.no_grad()
    def step(self, frames_list, current_qpos):
        """
        :param frames_list: åŒ…å« 16 å¸§çœŸå®å†å²å›¾åƒçš„åˆ—è¡¨ (List[np.array])
        :param current_qpos: å½“å‰æœºå™¨äººå…³èŠ‚çŠ¶æ€
        """
        # =========================================================================
        # [é€»è¾‘ä¿®æ­£] å®Œå…¨é‡ç½® Bufferï¼Œå¡«å…¥çœŸå®çš„ 16 å¸§å†å²
        # =========================================================================
        self.video_buffer.clear()
        
        for frame in frames_list:
            # é¢„å¤„ç†æ¯ä¸€å¸§
            frame_resized = cv2.resize(frame, (224, 224))
            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
            wrist_tensor = torch.tensor(frame_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
            
            main_fake = torch.zeros_like(wrist_tensor)
            combined_frame = torch.stack([main_fake, wrist_tensor], dim=0)
            
            self.video_buffer.append(combined_frame)
        
        # ç¡®ä¿å¡«æ»¡äº† (å¦‚æœå®¢æˆ·ç«¯ä¼ æ¥çš„ä¸è¶³16å¸§ï¼Œåº”è¯¥åœ¨å®¢æˆ·ç«¯è¡¥é½ï¼Œä½†è¿™é‡ŒåŒé‡ä¿é™©)
        while len(self.video_buffer) < self.window_size:
            self.video_buffer.append(self.video_buffer[-1])

        # 2. State Preprocess
        if len(current_qpos) == 7:
            current_qpos = list(current_qpos) + [0.0]
        
        qpos_np = np.array(current_qpos, dtype=np.float32)
        norm_qpos_np = (qpos_np - self.action_mean) / self.action_std
        
        # çŠ¶æ€ Buffer ä¹Ÿåº”è¯¥åˆ·æ–°ï¼Œä½†é€šå¸¸æˆ‘ä»¬åªæœ‰å½“å‰çŠ¶æ€
        # ç­–ç•¥ï¼šå‡è®¾è¿‡å»16å¸§çš„çŠ¶æ€éƒ½è¿‘ä¼¼äºå½“å‰çŠ¶æ€ (æˆ–è€…ä½ å¯ä»¥è®©å®¢æˆ·ç«¯ä¹Ÿä¼ çŠ¶æ€å†å²)
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå¡«æ»¡å½“å‰çŠ¶æ€
        self.state_buffer.clear()
        for _ in range(self.window_size):
            self.state_buffer.append(norm_qpos_np)
        
        # 3. Batch Construction
        vid_t = torch.stack(list(self.video_buffer)).to(self.device)
        vid_t = vid_t.permute(1, 2, 0, 3, 4).unsqueeze(0) # [1, 2, 3, 16, 224, 224]
        
        state_t = torch.tensor(np.array(list(self.state_buffer)), dtype=torch.float32).unsqueeze(0).to(self.device)
        
        # 4. Inference
        self.scheduler.set_timesteps(self.inference_steps)
        # with autocast('cuda', dtype=torch.bfloat16):
        #     features = self.encoder(vid_t, self.text_tokens, state_t, self.first_frame_tensor)
        #     latents = torch.randn(1, self.pred_horizon, 8, device=self.device) 
            
        #     for t in self.scheduler.timesteps:
        #         model_input = self.scheduler.scale_model_input(latents, t)
        #         t_tensor = torch.tensor([t], device=self.device)
        #         noise_pred = self.policy(model_input, t_tensor, features)
        #         latents = self.scheduler.step(noise_pred, t, latents).prev_sample
        with autocast('cuda', dtype=torch.bfloat16):
            # (1) è·å–è§†è§‰ç‰¹å¾
            features = self.encoder(vid_t, self.text_tokens, state_t, self.first_frame_tensor)
            
            # =================================================================
            # ğŸš¨ [å…³é”®ä¿®å¤] æ‰‹åŠ¨æ³¨å…¥ Stateï¼
            # å¿…é¡»ä¸è®­ç»ƒæ—¶çš„ behavior ä¸€è‡´ï¼šå–æ—¶é—´çª—å£çš„æœ€åä¸€å¸§ state[:, -1, :]
            # =================================================================
            features["state"] = state_t[:, -1, :] 
            
            latents = torch.randn(1, self.pred_horizon, 8, device=self.device) 
            
            for t in self.scheduler.timesteps:
                model_input = self.scheduler.scale_model_input(latents, t)
                t_tensor = torch.tensor([t], device=self.device)
                
                # (2) ä¼ å…¥åŒ…å« state çš„å®Œæ•´å­—å…¸
                noise_pred = self.policy(model_input, t_tensor, features)
                
                latents = self.scheduler.step(noise_pred, t, latents).prev_sample
            
        # ... (åç»­ä»£ç ä¸å˜) ...
            
        normalized_actions = latents[0].float()
        action_pred_np = normalized_actions.detach().cpu().numpy()
        denormalized_actions = action_pred_np * self.action_std + self.action_mean
        
        gripper_val = denormalized_actions[0, 7]
        print(f"   >>> [Model Output] Gripper: {gripper_val:.4f} (Threshold: <0.06 Close)", end='\r')
        safe_actions = self.safety.clip_actions(denormalized_actions)
        return safe_actions.tolist()