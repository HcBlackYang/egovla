# import torch
# import cv2
# import json
# import numpy as np
# from collections import deque
# from diffusers import DDIMScheduler
# import os
# from torch.amp import autocast
# from peft import LoraConfig, get_peft_model
# from transformers import T5Tokenizer
# import torch._dynamo

# # === å¯¼å…¥ä½ çš„æ¨¡å‹ ===
# from model.fusion_encoder import FusionEncoder
# from model.rdt_model import RDTWrapper

# # === åŸºç¡€è·¯å¾„é…ç½® ===
# VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
# RDT_PATH = '/yanghaochuan/models/rdt-1b'
# STATS_PATH = "/yanghaochuan/data/13dataset_stats.json"
# TOKENIZER_PATH = "/yanghaochuan/models/flan-t5-large"
# STAGE_C_PATH = '/yanghaochuan/checkpoints/12stageC_step_4000.pt'

# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# class SafetyController:
#     def __init__(self):
#         self.joint_limits_min = np.array([-2.89, -1.76, -2.89, -3.07, -2.89, -0.01, -2.89]) + 0.01
#         self.joint_limits_max = np.array([ 2.89,  1.76,  2.89, -0.06,  2.89,  3.75,  2.89]) - 0.01

#     def clip_actions(self, actions_batch):
#         actions_np = np.array(actions_batch)
#         joints = actions_np[:, :7]
#         gripper = actions_np[:, 7:]
#         joints_clipped = np.clip(joints, self.joint_limits_min, self.joint_limits_max)
#         return np.concatenate([joints_clipped, gripper], axis=1)

# class RealTimeAgent:
#     def __init__(self):
#         self.device = DEVICE
#         self.safety = SafetyController() 
#         self.pred_horizon = 64

#         print(f"[Agent] Loading Tokenizer from {TOKENIZER_PATH}...")
#         try:
#             self.tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
#         except:
#             self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
#         if not os.path.exists(STATS_PATH):
#             raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {STATS_PATH}")
#         with open(STATS_PATH, 'r') as f:
#             stats = json.load(f)
        
#         mean_raw = np.array(stats['action_mean'], dtype=np.float32)
#         std_raw = np.array(stats['action_std'], dtype=np.float32)
        
#         if mean_raw.shape[0] > 8:
#             self.action_mean = mean_raw[:8]
#             self.action_std = std_raw[:8]
#         elif mean_raw.shape[0] == 7:
#             self.action_mean = np.concatenate([mean_raw, [0.0]])
#             self.action_std = np.concatenate([std_raw, [1.0]])
#         else:
#             self.action_mean = mean_raw
#             self.action_std = std_raw
            
#         self.action_std = np.maximum(self.action_std, 1e-2)
        
#         self._init_models()
#         self._init_scheduler()
        
#         self.window_size = 16
#         self.video_buffer = deque(maxlen=self.window_size)
#         self.state_buffer = deque(maxlen=self.window_size)
#         self.first_frame_tensor = None
#         self.text_tokens = None 
#         self.default_prompt = "pick up the orange ball"

#     def _init_models(self):
#         # ... (æ¨¡å‹åˆå§‹åŒ–ä»£ç ä¿æŒä¸å˜) ...
#         print(f"[Agent] Initializing models on {self.device}...")
#         self.encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(self.device).eval()
#         self.policy = RDTWrapper(action_dim=8, model_path=RDT_PATH, rdt_cond_dim=768, pred_horizon=64).to(self.device).eval()
#         ckpt_c = torch.load(STAGE_C_PATH, map_location=self.device)
#         peft_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], lora_dropout=0.05, bias="none")
#         self.policy.rdt_model = get_peft_model(self.policy.rdt_model, peft_config)
        
#         if 'rdt_state_dict' in ckpt_c: self.policy.load_state_dict(ckpt_c['rdt_state_dict'], strict=False)
#         else: self.policy.load_state_dict(ckpt_c, strict=False)
        
#         if 'encoder_state_dict' in ckpt_c: self.encoder.load_state_dict(ckpt_c['encoder_state_dict'], strict=False)
#         else: raise ValueError(f"âŒ No encoder_state_dict in {STAGE_C_PATH}")
        
#         torch._dynamo.config.suppress_errors = True
#         try: self.encoder = torch.compile(self.encoder, mode="default")
#         except: pass

#     def _init_scheduler(self):
#         self.scheduler = DDIMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2", prediction_type="epsilon", clip_sample=True)
#         self.inference_steps = 25
#         self.scheduler.set_timesteps(self.inference_steps)

#     def reset_session(self, first_frame_img, current_qpos=None):
#         print("[Agent] Resetting session (Cold Start)...")
#         self.video_buffer.clear()
#         self.state_buffer.clear()
        
#         ff_resized = cv2.resize(first_frame_img, (224, 224))
#         ff_rgb = cv2.cvtColor(ff_resized, cv2.COLOR_BGR2RGB)
#         wrist_tensor = torch.tensor(ff_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
#         main_fake = torch.zeros_like(wrist_tensor)
#         self.first_frame_tensor = torch.stack([main_fake, wrist_tensor], dim=0).unsqueeze(0).to(self.device)
        
#         tokens = self.tokenizer(self.default_prompt, return_tensors="pt", padding="max_length", max_length=16, truncation=True).input_ids
#         self.text_tokens = tokens.to(self.device)
        
#         # Buffer åˆå§‹åŒ– (è™½ç„¶é©¬ä¸Šä¼šè¢« step è¦†ç›–ï¼Œä½†ä¸ºäº†å®‰å…¨å…ˆå¡«æ»¡)
#         video_frame_unit = torch.stack([main_fake, wrist_tensor], dim=0) 
#         for _ in range(self.window_size):
#             self.video_buffer.append(video_frame_unit) 
            
#         if current_qpos is None: current_qpos = np.zeros(8)
#         else: 
#             if len(current_qpos) == 7: current_qpos = list(current_qpos) + [0.0]
#             current_qpos = np.array(current_qpos, dtype=np.float32)
#         norm_qpos = (current_qpos - self.action_mean) / self.action_std
#         for _ in range(self.window_size):
#             self.state_buffer.append(norm_qpos)

#     @torch.no_grad()
#     def step(self, frames_list, current_qpos):
#         """
#         :param frames_list: åŒ…å« 16 å¸§çœŸå®å†å²å›¾åƒçš„åˆ—è¡¨ (List[np.array])
#         :param current_qpos: å½“å‰æœºå™¨äººå…³èŠ‚çŠ¶æ€
#         """
#         # =========================================================================
#         # [é€»è¾‘ä¿®æ­£] å®Œå…¨é‡ç½® Bufferï¼Œå¡«å…¥çœŸå®çš„ 16 å¸§å†å²
#         # =========================================================================
#         self.video_buffer.clear()
        
#         for frame in frames_list:
#             # é¢„å¤„ç†æ¯ä¸€å¸§
#             frame_resized = cv2.resize(frame, (224, 224))
#             frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
#             wrist_tensor = torch.tensor(frame_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
            
#             main_fake = torch.zeros_like(wrist_tensor)
#             combined_frame = torch.stack([main_fake, wrist_tensor], dim=0)
            
#             self.video_buffer.append(combined_frame)
        
#         # ç¡®ä¿å¡«æ»¡äº† (å¦‚æœå®¢æˆ·ç«¯ä¼ æ¥çš„ä¸è¶³16å¸§ï¼Œåº”è¯¥åœ¨å®¢æˆ·ç«¯è¡¥é½ï¼Œä½†è¿™é‡ŒåŒé‡ä¿é™©)
#         while len(self.video_buffer) < self.window_size:
#             self.video_buffer.append(self.video_buffer[-1])

#         # 2. State Preprocess
#         if len(current_qpos) == 7:
#             current_qpos = list(current_qpos) + [0.0]
        
#         qpos_np = np.array(current_qpos, dtype=np.float32)
#         norm_qpos_np = (qpos_np - self.action_mean) / self.action_std
        
#         # çŠ¶æ€ Buffer ä¹Ÿåº”è¯¥åˆ·æ–°ï¼Œä½†é€šå¸¸æˆ‘ä»¬åªæœ‰å½“å‰çŠ¶æ€
#         # ç­–ç•¥ï¼šå‡è®¾è¿‡å»16å¸§çš„çŠ¶æ€éƒ½è¿‘ä¼¼äºå½“å‰çŠ¶æ€ (æˆ–è€…ä½ å¯ä»¥è®©å®¢æˆ·ç«¯ä¹Ÿä¼ çŠ¶æ€å†å²)
#         # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå¡«æ»¡å½“å‰çŠ¶æ€
#         self.state_buffer.clear()
#         for _ in range(self.window_size):
#             self.state_buffer.append(norm_qpos_np)
        
#         # 3. Batch Construction
#         vid_t = torch.stack(list(self.video_buffer)).to(self.device)
#         vid_t = vid_t.permute(1, 2, 0, 3, 4).unsqueeze(0) # [1, 2, 3, 16, 224, 224]
        
#         state_t = torch.tensor(np.array(list(self.state_buffer)), dtype=torch.float32).unsqueeze(0).to(self.device)
        
#         # 4. Inference
#         self.scheduler.set_timesteps(self.inference_steps)
#         # with autocast('cuda', dtype=torch.bfloat16):
#         #     features = self.encoder(vid_t, self.text_tokens, state_t, self.first_frame_tensor)
#         #     latents = torch.randn(1, self.pred_horizon, 8, device=self.device) 
            
#         #     for t in self.scheduler.timesteps:
#         #         model_input = self.scheduler.scale_model_input(latents, t)
#         #         t_tensor = torch.tensor([t], device=self.device)
#         #         noise_pred = self.policy(model_input, t_tensor, features)
#         #         latents = self.scheduler.step(noise_pred, t, latents).prev_sample
#         with autocast('cuda', dtype=torch.bfloat16):
#             # (1) è·å–è§†è§‰ç‰¹å¾
#             features = self.encoder(vid_t, self.text_tokens, state_t, self.first_frame_tensor)
            
#             # =================================================================
#             # ğŸš¨ [å…³é”®ä¿®å¤] æ‰‹åŠ¨æ³¨å…¥ Stateï¼
#             # å¿…é¡»ä¸è®­ç»ƒæ—¶çš„ behavior ä¸€è‡´ï¼šå–æ—¶é—´çª—å£çš„æœ€åä¸€å¸§ state[:, -1, :]
#             # =================================================================
#             features["state"] = state_t[:, -1, :] 
            
#             latents = torch.randn(1, self.pred_horizon, 8, device=self.device) 
            
#             for t in self.scheduler.timesteps:
#                 model_input = self.scheduler.scale_model_input(latents, t)
#                 t_tensor = torch.tensor([t], device=self.device)
                
#                 # (2) ä¼ å…¥åŒ…å« state çš„å®Œæ•´å­—å…¸
#                 noise_pred = self.policy(model_input, t_tensor, features)
                
#                 latents = self.scheduler.step(noise_pred, t, latents).prev_sample
            
#         # ... (åç»­ä»£ç ä¸å˜) ...
            
#         normalized_actions = latents[0].float()
#         action_pred_np = normalized_actions.detach().cpu().numpy()
#         denormalized_actions = action_pred_np * self.action_std + self.action_mean
        
#         gripper_val = denormalized_actions[0, 7]
#         print(f"   >>> [Model Output] Gripper: {gripper_val:.4f} (Threshold: <0.06 Close)", end='\r')
#         safe_actions = self.safety.clip_actions(denormalized_actions)
#         return safe_actions.tolist()

# import torch
# import cv2
# import json
# import numpy as np
# from collections import deque
# from diffusers import DDIMScheduler
# import os
# from torch.amp import autocast
# from peft import LoraConfig, get_peft_model
# from transformers import T5Tokenizer
# import torch._dynamo
# from torchvision import transforms

# # === å¯¼å…¥ä½ çš„æ¨¡å‹ ===
# from model.fusion_encoder import FusionEncoder
# from model.rdt_model import RDTWrapper

# # === åŸºç¡€è·¯å¾„é…ç½® ===
# VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
# RDT_PATH = '/yanghaochuan/models/rdt-1b'
# # ä½¿ç”¨æ–°çš„ 16dataset_stats (å¯¹åº”æ–°çš„é‡‡æ ·ç­–ç•¥)
# STATS_PATH = "/yanghaochuan/data/111dataset_stats.json"
# TOKENIZER_PATH = "/yanghaochuan/models/flan-t5-large"
# # ä½¿ç”¨ ForeSight è®­ç»ƒå‡ºçš„ Checkpoint
# STAGE_C_PATH = '/yanghaochuan/114checkpoints_finetune/StageC_ForeSight_step_4000.pt'

# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# class SafetyController:
#     def __init__(self):
#         # Franka å…³èŠ‚æé™ (å®‰å…¨ä½™é‡)
#         self.joint_limits_min = np.array([-2.89, -1.76, -2.89, -3.07, -2.89, -0.01, -2.89]) + 0.01
#         self.joint_limits_max = np.array([ 2.89,  1.76,  2.89, -0.06,  2.89,  3.75,  2.89]) - 0.01

#     def clip_actions(self, actions_batch):
#         actions_np = np.array(actions_batch)
#         joints = actions_np[:, :7]
#         gripper = actions_np[:, 7:]
#         joints_clipped = np.clip(joints, self.joint_limits_min, self.joint_limits_max)
#         return np.concatenate([joints_clipped, gripper], axis=1)

# class RealTimeAgent:
#     def __init__(self):
#         self.device = DEVICE
#         self.safety = SafetyController() 
#         self.pred_horizon = 64

#         # === ğŸŸ¢ ForeSight æ ¸å¿ƒå‚æ•° ===
#         self.history_len = 500       # Buffer é•¿åº¦ï¼šè¦†ç›–è¿‡å» 2-3 ç§’
#         self.model_input_frames = 6 # æ¨¡å‹å®é™…è¾“å…¥ï¼šå‡åŒ€é‡‡æ · 6 å¸§
#         # ===========================

#         print(f"[Agent] Loading Tokenizer from {TOKENIZER_PATH}...")
#         try:
#             self.tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
#         except:
#             self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
#         # åŠ è½½ç»Ÿè®¡æ•°æ®
#         if not os.path.exists(STATS_PATH):
#             raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {STATS_PATH}")
#         with open(STATS_PATH, 'r') as f:
#             stats = json.load(f)
        
#         mean_raw = np.array(stats['action_mean'], dtype=np.float32)
#         std_raw = np.array(stats['action_std'], dtype=np.float32)
        
#         # ç»´åº¦ä¿®æ­£
#         if mean_raw.shape[0] > 8:
#             self.action_mean = mean_raw[:8]
#             self.action_std = std_raw[:8]
#         elif mean_raw.shape[0] == 7:
#             self.action_mean = np.concatenate([mean_raw, [0.0]])
#             self.action_std = np.concatenate([std_raw, [1.0]])
#         else:
#             self.action_mean = mean_raw
#             self.action_std = std_raw
            
#         self.action_std = np.maximum(self.action_std, 1e-2)

#         # ğŸŸ¢ [æ–°å¢] å½’ä¸€åŒ– (ä¸è®­ç»ƒå®Œå…¨ä¸€è‡´)
#         self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], 
#                                               std=[0.229, 0.224, 0.225])
#         self._init_models()
#         self._init_scheduler()
        
#         # åˆå§‹åŒ– Buffer (é•¿åº¦ä¸º history_len)
#         self.video_buffer = deque(maxlen=self.history_len)
#         self.state_buffer = deque(maxlen=self.history_len)
        
#         self.first_frame_tensor = None
#         self.text_tokens = None 
#         self.default_prompt = "pick up the orange ball and put it on the plank"

#         self.warmup()

#     def _init_models(self):
#         print(f"[Agent] Initializing models on {self.device}...")
#         self.encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(self.device).eval()
#         self.policy = RDTWrapper(action_dim=8, model_path=RDT_PATH, rdt_cond_dim=768, pred_horizon=64).to(self.device).eval()
        
#         print(f"[Agent] Loading Checkpoint: {STAGE_C_PATH}")
#         ckpt_c = torch.load(STAGE_C_PATH, map_location=self.device)
        
#         peft_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], lora_dropout=0.05, bias="none")
#         self.policy.rdt_model = get_peft_model(self.policy.rdt_model, peft_config)
        
#         if 'rdt_state_dict' in ckpt_c: self.policy.load_state_dict(ckpt_c['rdt_state_dict'], strict=False)
#         else: self.policy.load_state_dict(ckpt_c, strict=False)
        
#         if 'encoder_state_dict' in ckpt_c: self.encoder.load_state_dict(ckpt_c['encoder_state_dict'], strict=False)
        
#         torch._dynamo.config.suppress_errors = True
#         try: self.encoder = torch.compile(self.encoder, mode="default")
#         except: pass

#     def _init_scheduler(self):
#         self.scheduler = DDIMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2", prediction_type="epsilon", clip_sample=True)
#         self.inference_steps = 25
#         self.scheduler.set_timesteps(self.inference_steps)


#     # ğŸŸ¢ [æ–°å¢] é¢„çƒ­å‡½æ•°
#     def warmup(self):
#         print("ğŸ”¥ [System] Warming up model (compilation)... This may take 1 min.")
#         # æ„é€ å‡çš„è¾“å…¥ (Batch=1, View=2, Channel=3, Time=6, H=224, W=224)
#         dummy_video = torch.randn(1, 2, 3, 6, 224, 224, device=self.device, dtype=torch.bfloat16)
#         dummy_text = torch.randint(0, 1000, (1, 16), device=self.device)
#         dummy_state = torch.randn(1, 1, 8, device=self.device, dtype=torch.float32)
#         dummy_ff = torch.randn(1, 2, 3, 224, 224, device=self.device, dtype=torch.float32)
        
#         try:
#             with autocast('cuda', dtype=torch.bfloat16):
#                 # è·‘ä¸€æ¬¡ Encoder
#                 feats = self.encoder(dummy_video, dummy_text, dummy_state, dummy_ff)
#                 feats["state"] = dummy_state[:, -1, :]
#                 # è·‘ä¸€æ¬¡ Policy
#                 latents = torch.randn(1, self.pred_horizon, 8, device=self.device)
#                 t = torch.tensor([0], device=self.device)
#                 _ = self.policy(latents, t, feats)
#             print("âœ… Warmup done. Ready to serve.")
#         except Exception as e:
#             print(f"âŒ Warmup failed: {e}")

#     # ğŸŸ¢ [æ–°å¢] å›¾åƒé¢„å¤„ç† (æš´åŠ› Resize + å½’ä¸€åŒ–)
#     def preprocess_image(self, img_np):
#         # 1. æš´åŠ› Resize: 1280x720 -> 224x224
#         # è¿™ä¼šäº§ç”Ÿç•¸å˜ï¼Œä½†ä¿ç•™äº†æ‰€æœ‰è¾¹ç¼˜ä¿¡æ¯ï¼Œä¸”ä¸ä½ çš„è®­ç»ƒæ•°æ®é¢„å¤„ç†(preprocess_with_teachers.py)ä¸€è‡´
#         resized = cv2.resize(img_np, (224, 224))
        
#         # 2. BGR -> RGB
#         rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        
#         # 3. To Tensor & Normalize
#         tensor = torch.tensor(rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
#         tensor = self.normalize(tensor) # <--- å…³é”®ï¼
        
#         return tensor


#     def reset_session(self, first_frame_img, current_qpos=None):
#         print("[Agent] Resetting session (Cold Start)...")
#         self.video_buffer.clear()
#         self.state_buffer.clear()
        
#         # å¤„ç†é¦–å¸§ (Anchor)
#         ff_resized = cv2.resize(first_frame_img, (224, 224))
#         ff_rgb = cv2.cvtColor(ff_resized, cv2.COLOR_BGR2RGB)
#         # wrist_tensor = torch.tensor(ff_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
#         # 1. åŸºç¡€è½¬æ¢
#         wrist_tensor_raw = torch.tensor(ff_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
        
#         # 2. ğŸŸ¢ [ä¿®å¤] å¿…é¡»åŠ ä¸Šå½’ä¸€åŒ–ï¼
#         wrist_tensor = self.normalize(wrist_tensor_raw)
#         main_fake = torch.zeros_like(wrist_tensor)
#         self.first_frame_tensor = torch.stack([main_fake, wrist_tensor], dim=0).unsqueeze(0).to(self.device)
        
#         tokens = self.tokenizer(self.default_prompt, return_tensors="pt", padding="max_length", max_length=16, truncation=True).input_ids
#         self.text_tokens = tokens.to(self.device)
        
#         # # å¡«æ»¡ buffer (å†·å¯åŠ¨å¡«å……)
#         # # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¡«æ»¡ history_lenï¼Œè¿™æ ·åˆå§‹é‡‡æ ·å°±æ˜¯å…¨æ˜¯é¦–å¸§
#         # video_frame_unit = torch.stack([main_fake, wrist_tensor], dim=0) 
#         # for _ in range(self.history_len):
#         #     self.video_buffer.append(video_frame_unit) 

#         # ğŸŸ¢ [ä¿®æ”¹] åŠ¨æ€ Buffer ç­–ç•¥
#         # åªå­˜å…¥å½“å‰è¿™ 1 å¸§ã€‚ç»ä¸å¡«å…… 500 æ¬¡ï¼
#         video_frame_unit = torch.stack([main_fake, wrist_tensor], dim=0)
#         self.video_buffer.append(video_frame_unit)
            
#         if current_qpos is None: current_qpos = np.zeros(8)
#         else: 
#             if len(current_qpos) == 7: current_qpos = list(current_qpos) + [0.0]
#             current_qpos = np.array(current_qpos, dtype=np.float32)
#         norm_qpos = (current_qpos - self.action_mean) / self.action_std
        
#         # for _ in range(self.history_len):
#         #     self.state_buffer.append(norm_qpos)
#         self.state_buffer.append(norm_qpos)

#     # @torch.no_grad()
#     # def step(self, frames_list, current_qpos):
#     #     """
#     #     :param frames_list: åŒ…å«è‹¥å¹²å¸§çœŸå®å†å²å›¾åƒçš„åˆ—è¡¨ (é€šå¸¸æ˜¯å®¢æˆ·ç«¯å‘æ¥çš„æœ€æ–°å‡ å¸§)
#     #     """
#     #     # 1. æ›´æ–° Video Buffer
#     #     # æ³¨æ„ï¼šå®¢æˆ·ç«¯å¯èƒ½å‘æ¥ 16 å¸§ï¼Œä¹Ÿå¯èƒ½åªå‘æ¥æœ€æ–° 1 å¸§ã€‚
#     #     # æˆ‘ä»¬å°†å®ƒä»¬å…¨éƒ¨ append åˆ°é•¿ Buffer ä¸­ã€‚
#     #     for frame in frames_list:
#     #         frame_resized = cv2.resize(frame, (224, 224))
#     #         frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
#     #         wrist_tensor = torch.tensor(frame_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
            
#     #         main_fake = torch.zeros_like(wrist_tensor)
#     #         combined_frame = torch.stack([main_fake, wrist_tensor], dim=0)
#     #         self.video_buffer.append(combined_frame)
        
#     #     # 2. State Preprocess & Update
#     #     if len(current_qpos) == 7:
#     #         current_qpos = list(current_qpos) + [0.0]
        
#     #     qpos_np = np.array(current_qpos, dtype=np.float32)
#     #     norm_qpos_np = (qpos_np - self.action_mean) / self.action_std
        
#     #     # æ›´æ–°çŠ¶æ€ Buffer (åªå­˜æœ€æ–°çš„å³å¯ï¼Œæˆ–è€…å­˜å†å²)
#     #     # è¿™é‡Œç®€å•èµ·è§ï¼Œappend æœ€æ–°çš„
#     #     self.state_buffer.append(norm_qpos_np)
        
#     #     # =========================================================
#     #     # ğŸŸ¢ æ ¸å¿ƒï¼šå‡åŒ€é‡‡æ · (Uniform Sampling)
#     #     # =========================================================
#     #     curr_len = len(self.video_buffer)
#     #     # ä» Buffer ä¸­å‡åŒ€é€‰å– model_input_frames (6) å¸§
#     #     # np.linspace ç”Ÿæˆå‡åŒ€é—´éš”çš„ç´¢å¼•
#     #     indices = np.linspace(0, curr_len - 1, self.model_input_frames).astype(int)
        
#     #     # å–å‡ºé€‰ä¸­çš„å¸§
#     #     buffer_list = list(self.video_buffer)
#     #     selected_frames = [buffer_list[i] for i in indices]
        
#     #     # å †å  -> [6, 2, 3, 224, 224]
#     #     vid_t = torch.stack(selected_frames).to(self.device)
#     #     # è°ƒæ•´ç»´åº¦ -> [1, 2, 3, 6, 224, 224] (Batch=1, T=6)
#     #     vid_t = vid_t.permute(1, 2, 0, 3, 4).unsqueeze(0)
        

#     @torch.no_grad()
#     def step(self, frames_list, current_qpos):
#         # 1. æ›´æ–° Video Buffer
#         for frame in frames_list:
#             wrist_tensor = self.preprocess_image(frame)
#             main_fake = torch.zeros_like(wrist_tensor)
#             combined_frame = torch.stack([main_fake, wrist_tensor], dim=0)
#             self.video_buffer.append(combined_frame) 
#             # é˜Ÿåˆ—ä¼šè‡ªåŠ¨æŒ¤å‡ºæ—§çš„ï¼Œä¿æŒæœ€æ–°çš„500å¸§
        
#         # 2. æ›´æ–° State
#         if len(current_qpos) == 7: current_qpos = list(current_qpos) + [0.0]
#         qpos_np = np.array(current_qpos, dtype=np.float32)
#         norm_qpos_np = (qpos_np - self.action_mean) / self.action_std
#         self.state_buffer.append(norm_qpos_np)
        
#         # ğŸŸ¢ [ä¿®æ”¹] åŠ¨æ€å‡åŒ€é‡‡æ · (æ ¸å¿ƒé€»è¾‘)
#         curr_len = len(self.video_buffer)
        
#         # æ— è®ºå½“å‰ Buffer æ˜¯ 1 å¸§è¿˜æ˜¯ 500 å¸§ï¼Œéƒ½å‡åŒ€å–å‡º 6 å¸§
#         # è¿™ä¿è¯äº†æ¨¡å‹å§‹ç»ˆèƒ½çœ‹åˆ°â€œå…¨å†å²â€çš„æ¦‚è²Œï¼Œè€Œä¸æ˜¯â€œå±€éƒ¨é™æ­¢åˆ‡ç‰‡â€
#         indices = np.linspace(0, curr_len - 1, self.model_input_frames).astype(int)
        
#         selected_frames = [self.video_buffer[i] for i in indices]
        
#         # Stack -> [6, 2, 3, 224, 224]
#         vid_t = torch.stack(selected_frames).to(self.device)
#         # Permute -> [1, 2, 3, 6, 224, 224] (Batch, View, Channel, Time, H, W)
#         vid_t = vid_t.permute(1, 2, 0, 3, 4).unsqueeze(0)

#         # State: å–å½“å‰æœ€æ–°çš„çŠ¶æ€å³å¯ (å› ä¸º FusionEncoder åªç”¨ state[:, -1, :])
#         # ä¸ºäº†æ ¼å¼ç»Ÿä¸€ï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ª [1, 1, 8] çš„ Tensor
#         state_t = torch.tensor(norm_qpos_np, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)
        
#         # 4. Inference
#         self.scheduler.set_timesteps(self.inference_steps)
#         with autocast('cuda', dtype=torch.bfloat16):
#             # (1) è·å–è§†è§‰ç‰¹å¾ (åŒ…å« ForeSight çš„æœªæ¥é¢„æµ‹)
#             features = self.encoder(vid_t, self.text_tokens, state_t, self.first_frame_tensor)
            
#             # (2) æ‰‹åŠ¨æ³¨å…¥å½“å‰ State (ç¡®ä¿ RDT æ‹¿åˆ°çš„æ˜¯æœ€æ–°æœ¬ä½“æ„ŸçŸ¥)
#             # state_t æ˜¯ [1, 1, 8], å– [:, -1, :] å¾—åˆ° [1, 8]
#             features["state"] = state_t[:, -1, :] 
            
#             latents = torch.randn(1, self.pred_horizon, 8, device=self.device) 
            
#             for t in self.scheduler.timesteps:
#                 model_input = self.scheduler.scale_model_input(latents, t)
#                 t_tensor = torch.tensor([t], device=self.device)
                
#                 noise_pred = self.policy(model_input, t_tensor, features)
                
#                 latents = self.scheduler.step(noise_pred, t, latents).prev_sample
            
#         normalized_actions = latents[0].float()
#         action_pred_np = normalized_actions.detach().cpu().numpy()
#         denormalized_actions = action_pred_np * self.action_std + self.action_mean
        
#         # =========================================================
#         # ğŸš¨ [å…³é”®ä¿®å¤] å¤¹çˆªäºŒå€¼åŒ– (Thresholding)
#         # =========================================================
#         # å®šä¹‰ç‰©ç†æé™
#         GRIPPER_OPEN_VAL = 0.0804  
#         GRIPPER_CLOSE_VAL = 0.0428 
#         GRIPPER_THRESHOLD = 0.0616 

#         # è·å–åŸå§‹é¢„æµ‹å€¼
#         raw_gripper_pred = denormalized_actions[:, 7]

#         # äºŒå€¼åŒ–åˆ¤æ–­
#         binary_gripper = np.where(raw_gripper_pred > GRIPPER_THRESHOLD, GRIPPER_OPEN_VAL, GRIPPER_CLOSE_VAL)
        
#         # è¦†ç›–å›å»
#         denormalized_actions[:, 7] = binary_gripper
        
#         print(f"   >>> [Gripper] Raw: {raw_gripper_pred[0]:.4f} -> Binary: {binary_gripper[0]:.4f}", end='\r')
#         # =========================================================
        
#         safe_actions = self.safety.clip_actions(denormalized_actions)
#         return safe_actions.tolist()


import torch
import cv2
import json
import numpy as np
from collections import deque
from diffusers import DDIMScheduler
import os
from torch.amp import autocast
from peft import LoraConfig, get_peft_model
from transformers import T5Tokenizer
import torch._dynamo
from torchvision import transforms

# === å¯¼å…¥ä½ çš„æ¨¡å‹ ===
from model.fusion_encoder import FusionEncoder
from model.rdt_model import RDTWrapper

# === åŸºç¡€è·¯å¾„é…ç½® ===
VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
RDT_PATH = '/yanghaochuan/models/rdt-1b'
STATS_PATH = "/yanghaochuan/data/115dataset_stats.json" 
TOKENIZER_PATH = "/yanghaochuan/models/flan-t5-large"
STAGE_C_PATH = '/yanghaochuan/114checkpoints_finetune/StageC_ForeSight_step_10000.pt'

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class SafetyController:
    def __init__(self):
        self.joint_limits_min = np.array([-2.89, -1.76, -2.89, -3.07, -2.89, -0.01, -2.89]) + 0.01
        self.joint_limits_max = np.array([ 2.89,  1.76,  2.89, -0.06,  2.89,  3.75,  2.89]) - 0.01

    def clip_actions(self, actions_batch):
        actions_np = np.array(actions_batch)
        joints = actions_np[:, :7]
        gripper = actions_np[:, 7:]
        joints_clipped = np.clip(joints, self.joint_limits_min, self.joint_limits_max)
        return np.concatenate([joints_clipped, gripper], axis=1)

class RealTimeAgent:
    def __init__(self):
        self.device = DEVICE
        self.safety = SafetyController() 
        self.pred_horizon = 64
        self.history_len = 500       
        self.model_input_frames = 6 
        
        import time
        import os
        # å®šä¹‰ä¿å­˜ç›®å½•
        self.debug_dir = f"debug_visuals_{int(time.time())}"
        os.makedirs(self.debug_dir, exist_ok=True)
        self.step_counter = 0

        print(f"[Agent] Loading Tokenizer from {TOKENIZER_PATH}...")
        try:
            self.tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
        except:
            self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
        # åŠ è½½ç»Ÿè®¡æ•°æ®
        if not os.path.exists(STATS_PATH):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {STATS_PATH}")
        with open(STATS_PATH, 'r') as f:
            stats = json.load(f)
        
        mean_raw = np.array(stats['action_mean'], dtype=np.float32)
        std_raw = np.array(stats['action_std'], dtype=np.float32)
        
        if mean_raw.shape[0] > 8:
            self.action_mean = mean_raw[:8]
            self.action_std = std_raw[:8]
        elif mean_raw.shape[0] == 7:
            self.action_mean = np.concatenate([mean_raw, [0.0]])
            self.action_std = np.concatenate([std_raw, [1.0]])
        else:
            self.action_mean = mean_raw
            self.action_std = std_raw
            
        self.action_std = np.maximum(self.action_std, 1e-2)

        print(f"ğŸ“Š [Stats Loaded] Mean[0]: {self.action_mean[0]:.3f}, GripperMean: {self.action_mean[7]:.3f}")
        print(f"ğŸ“Š [Stats Loaded] Std[0]:  {self.action_std[0]:.3f}, GripperStd:  {self.action_std[7]:.3f}")

        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                              std=[0.229, 0.224, 0.225])
        self._init_models()
        self._init_scheduler()
        
        self.video_buffer = deque(maxlen=self.history_len)
        self.state_buffer = deque(maxlen=self.history_len)
        self.first_frame_tensor = None
        self.text_tokens = None 
        self.default_prompt = "pick up the orange ball and put it on the plank"
        
        # ğŸŸ¢ [è¯Šæ–­] å…³é—­ torch.compile ä»¥æ’é™¤ç¼–è¯‘é”™è¯¯å¹²æ‰°
        # torch._dynamo.config.suppress_errors = True
        # try: self.encoder = torch.compile(self.encoder, mode="default")
        # except: pass
        self.warmup()

    def _init_models(self):
        print(f"[Agent] Initializing models on {self.device}...")
        self.encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(self.device).eval()
        self.policy = RDTWrapper(action_dim=8, model_path=RDT_PATH, rdt_cond_dim=768, pred_horizon=64).to(self.device).eval()
        
        print(f"[Agent] Loading Checkpoint: {STAGE_C_PATH}")
        ckpt_c = torch.load(STAGE_C_PATH, map_location=self.device)
        
        peft_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], lora_dropout=0.05, bias="none")
        self.policy.rdt_model = get_peft_model(self.policy.rdt_model, peft_config)
        
        if 'rdt_state_dict' in ckpt_c: self.policy.load_state_dict(ckpt_c['rdt_state_dict'], strict=False)
        else: self.policy.load_state_dict(ckpt_c, strict=False)
        
        if 'encoder_state_dict' in ckpt_c: self.encoder.load_state_dict(ckpt_c['encoder_state_dict'], strict=False)

    def _init_scheduler(self):
        self.scheduler = DDIMScheduler(num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2", prediction_type="epsilon", clip_sample=True)
        self.inference_steps = 25
        self.scheduler.set_timesteps(self.inference_steps)

    def warmup(self):
        print("ğŸ”¥ [System] Warming up model...")
        dummy_video = torch.randn(1, 2, 3, 6, 224, 224, device=self.device, dtype=torch.bfloat16)
        dummy_text = torch.randint(0, 1000, (1, 16), device=self.device)
        dummy_state = torch.randn(1, 1, 8, device=self.device, dtype=torch.float32)
        dummy_ff = torch.randn(1, 2, 3, 224, 224, device=self.device, dtype=torch.float32)
        try:
            with autocast('cuda', dtype=torch.bfloat16):
                feats = self.encoder(dummy_video, dummy_text, dummy_state, dummy_ff)
                feats["state"] = dummy_state[:, -1, :]
                latents = torch.randn(1, self.pred_horizon, 8, device=self.device)
                t = torch.tensor([0], device=self.device)
                _ = self.policy(latents, t, feats)
            print("âœ… Warmup done.")
        except Exception as e:
            print(f"âŒ Warmup failed: {e}")

    def preprocess_image(self, img_np):
        resized = cv2.resize(img_np, (224, 224))
        rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        tensor = torch.tensor(rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
        tensor = self.normalize(tensor) 
        return tensor

    def save_debug_image(self, tensor, name="debug.png"):
        try:
            t = tensor.detach().cpu().clone()
            # Un-Normalize: x * std + mean
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            t = t * std + mean
            t = torch.clamp(t, 0, 1)
            img_np = (t.permute(1, 2, 0).numpy() * 255).astype(np.uint8)
            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
            cv2.imwrite(name, img_bgr)
            # print(f"ğŸ“¸ [Debug] Saved model input view to {name}")
        except Exception as e:
            pass

    # ğŸŸ¢ [æ–°å¢] è¿™æ˜¯ä¸€ä¸ªä¸“é—¨æŠŠ Tensor è¿˜åŸæˆå›¾ç‰‡çš„å‡½æ•°
    def save_model_input_visuals(self, vid_tensor, step_idx):
        """
        å°†æ¨¡å‹è¾“å…¥çš„ 6 å¸§ Tensor åå½’ä¸€åŒ–å¹¶æ‹¼å›¾ä¿å­˜
        vid_tensor shape: [1, 2, 3, 6, 224, 224] (Batch, View, Channel, Time, H, W)
        """
        try:
            # å–å‡º wrist è§†è§’ (View Index 1), å»æ‰ Batch ç»´ -> [3, 6, 224, 224]
            # æ³¨æ„ï¼šä½ çš„ä»£ç é‡Œ Main æ˜¯ 0 (å…¨é»‘), Wrist æ˜¯ 1
            wrist_t = vid_tensor[0, 1] 
            
            # åå½’ä¸€åŒ–å‚æ•° (ImageNet)
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1, 1).to(wrist_t.device)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1, 1).to(wrist_t.device)
            
            # åå½’ä¸€åŒ–: x * std + mean
            wrist_t = wrist_t * std + mean
            wrist_t = torch.clamp(wrist_t, 0, 1)
            
            # è½¬ä¸º Numpy: [3, 6, 224, 224] -> [6, 224, 224, 3]
            imgs = wrist_t.permute(1, 2, 3, 0).detach().cpu().numpy()
            imgs = (imgs * 255).astype(np.uint8)
            
            # æ‹¼æ¥ 6 å¸§æˆä¸€è¡Œé•¿å›¾
            # imgs[0] æ˜¯ Buffer é‡Œæœ€æ—©çš„ä¸€å¸§ï¼Œimgs[-1] æ˜¯æœ€æ–°çš„ä¸€å¸§
            concat_img = np.hstack([imgs[i] for i in range(6)])
            
            # è½¬ä¸º BGR ä¾› cv2 ä¿å­˜
            concat_img = cv2.cvtColor(concat_img, cv2.COLOR_RGB2BGR)
            
            # ä¿å­˜
            save_path = os.path.join(self.debug_dir, f"step_{step_idx:04d}_buffer.jpg")
            cv2.imwrite(save_path, concat_img)
            # print(f"ğŸ“¸ Saved buffer visual to {save_path}") # åˆ·å±å¯æ³¨é‡Šæ‰
            
        except Exception as e:
            print(f"âš ï¸ Visualization Failed: {e}")

    def reset_session(self, first_frame_img, current_qpos=None):
        print("[Agent] Resetting session (Cold Start)...")
        self.video_buffer.clear()
        self.state_buffer.clear()
        
        # === ğŸŸ¢ [Double Check] å¤„ç†é¦–å¸§ ===
        wrist_tensor = self.preprocess_image(first_frame_img)
        main_fake = torch.zeros_like(wrist_tensor)
        self.first_frame_tensor = torch.stack([main_fake, wrist_tensor], dim=0).unsqueeze(0).to(self.device)
        self.save_debug_image(wrist_tensor, "debug_first_frame_wrist.png")
        
        tokens = self.tokenizer(self.default_prompt, return_tensors="pt", padding="max_length", max_length=16, truncation=True).input_ids
        self.text_tokens = tokens.to(self.device)
        
        video_frame_unit = torch.stack([main_fake, wrist_tensor], dim=0)
        self.video_buffer.append(video_frame_unit)
            
        if current_qpos is None: current_qpos = np.zeros(8)
        else: 
            if len(current_qpos) == 7: current_qpos = list(current_qpos) + [0.0]
            current_qpos = np.array(current_qpos, dtype=np.float32)
            
        # ğŸ” æ‰“å°åˆå§‹çŠ¶æ€
        print(f"   ğŸš© [Reset QPos] {current_qpos[:6]} ... Grip: {current_qpos[7]}")
        
        norm_qpos = (current_qpos - self.action_mean) / self.action_std
        self.state_buffer.append(norm_qpos)

    @torch.no_grad()
    def step(self, frames_list, current_qpos):
        # 1. æ›´æ–° Video
        for frame in frames_list:
            wrist_tensor = self.preprocess_image(frame)
            main_fake = torch.zeros_like(wrist_tensor)
            combined_frame = torch.stack([main_fake, wrist_tensor], dim=0)
            self.video_buffer.append(combined_frame) 
        
        # 2. æ›´æ–° State
        if len(current_qpos) == 7: current_qpos = list(current_qpos) + [0.0]
        qpos_np = np.array(current_qpos, dtype=np.float32)
        
        # ğŸŸ¢ [æ ¸å¿ƒè¯Šæ–­] è®¡ç®— Normalized State
        norm_qpos_np = (qpos_np - self.action_mean) / self.action_std
        self.state_buffer.append(norm_qpos_np)
        
        # =================================================================
        # ğŸš¨ [å…³é”®ç›‘æ§] å¦‚æœè¿™é‡Œçš„æ•°å€¼ > 3.0 æˆ– < -3.0ï¼Œè¯´æ˜çŠ¶æ€è¾“å…¥é”™äº†ï¼
        # =================================================================
        gripper_norm = norm_qpos_np[7]
        joint0_norm = norm_qpos_np[0]
        if abs(gripper_norm) > 3.0 or abs(joint0_norm) > 3.0:
             print(f"\nâš ï¸ STATE OOD! J0_Norm: {joint0_norm:.2f}, Grip_Norm: {gripper_norm:.2f} | Raw Grip: {qpos_np[7]:.4f}")
        
        # 3. é‡‡æ ·
        curr_len = len(self.video_buffer)
        indices = np.linspace(0, curr_len - 1, self.model_input_frames).astype(int)
        selected_frames = [self.video_buffer[i] for i in indices]
        
        vid_t = torch.stack(selected_frames).to(self.device)
        vid_t = vid_t.permute(1, 2, 0, 3, 4).unsqueeze(0)

        # =========================================================
        # ğŸŸ¢ [æ’å…¥] åœ¨è¿™é‡Œä¿å­˜æ¨¡å‹çœ‹åˆ°çš„ç”»é¢ï¼
        # =========================================================
        self.save_model_input_visuals(vid_t, self.step_counter)
        self.step_counter += 1

        # State: å–å½“å‰ state
        state_t = torch.tensor(norm_qpos_np, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)
        
        # 4. Inference
        self.scheduler.set_timesteps(self.inference_steps)
        with autocast('cuda', dtype=torch.bfloat16):
            features = self.encoder(vid_t, self.text_tokens, state_t, self.first_frame_tensor)
            features["state"] = state_t[:, -1, :] 
            latents = torch.randn(1, self.pred_horizon, 8, device=self.device) 
            
            for t in self.scheduler.timesteps:
                model_input = self.scheduler.scale_model_input(latents, t)
                t_tensor = torch.tensor([t], device=self.device)
                noise_pred = self.policy(model_input, t_tensor, features)
                latents = self.scheduler.step(noise_pred, t, latents).prev_sample
            
        normalized_actions = latents[0].float()
        # === ğŸŸ¢ [æ–°å¢] è¯Šæ–­ä»£ç  ===
        # è®¡ç®—å½“å‰é¢„æµ‹åŠ¨ä½œçš„â€œå¹³å‡ç»å¯¹å€¼â€
        mean_abs_val = torch.mean(torch.abs(normalized_actions)).item()
        
        # æ‰“å°ç¬¬ä¸€æ­¥çš„å½’ä¸€åŒ–æ•°å€¼ (çœ‹å®ƒæ˜¯ä¸æ˜¯å…¨æ˜¯ 0.x)
        first_step_norm = normalized_actions[0].detach().cpu().numpy()
        print(f"\nğŸ” [Diagnosis] Normalized Mean Abs: {mean_abs_val:.4f}")
        print(f"   First Step Norm: {np.round(first_step_norm, 3)}")
        # =========================
        action_pred_np = normalized_actions.detach().cpu().numpy()
        denormalized_actions = action_pred_np * self.action_std + self.action_mean
        
        # å¤¹çˆªäºŒå€¼åŒ–
        GRIPPER_OPEN_VAL = 0.0804  
        GRIPPER_CLOSE_VAL = 0.0428 
        GRIPPER_THRESHOLD = 0.0616 

        raw_gripper_pred = denormalized_actions[:, 7]
        binary_gripper = np.where(raw_gripper_pred > GRIPPER_THRESHOLD, GRIPPER_OPEN_VAL, GRIPPER_CLOSE_VAL)
        denormalized_actions[:, 7] = binary_gripper
        
        print(f"   >>> [Step] NormState J0: {joint0_norm:.2f} G: {gripper_norm:.2f} | Pred J0: {denormalized_actions[0,0]:.3f}", end='\r')
        
        safe_actions = self.safety.clip_actions(denormalized_actions)
        return safe_actions.tolist()