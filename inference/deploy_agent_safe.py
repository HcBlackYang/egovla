import torch
import cv2
import json
import time
import numpy as np
from collections import deque
from diffusers import DDIMScheduler
import os
from torch.amp import autocast
from peft import LoraConfig, get_peft_model
from transformers import T5Tokenizer
import torch._dynamo

# === å¯¼å…¥ä½ çš„æ¨¡å‹ ===
from model.fusion_encoder import FusionEncoder
from model.rdt_model import RDTWrapper

# === åŸºç¡€è·¯å¾„ ===
VIDEO_MAE_PATH = '/yanghaochuan/models/VideoMAEv2-Large'
RDT_PATH = '/yanghaochuan/models/rdt-1b'
STATS_PATH = "/yanghaochuan/projects/data/dataset_stats.json"
TOKENIZER_PATH = "/yanghaochuan/models/flan-t5-large"

STAGE_B_PATH = '/yanghaochuan/projects/checkpoints/stageB_papercup.pt'
STAGE_C_PATH = '/yanghaochuan/projects/checkpoints/stageC_lora_epoch_40.pt'

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =============================================================================
# âš¡ï¸ æ ¸å¿ƒå‚æ•°è°ƒä¼˜ (ç¨³å¥ç‰ˆ)
# =============================================================================
# 1. å¢ç›Šå›å½’æ­£å¸¸ (é˜²æ­¢ä¹±åŠ¨)
EXECUTION_GAIN = 1.0 

# 2. ã€å¤§æ‹›ã€‘Zè½´é‡åŠ›åç½® (Gravity Bias)
# åªè¦å¤¹çˆªæ˜¯å¼€çš„ï¼Œæ¯ä¸€æ­¥éƒ½é¢å¤–å‘ä¸‹å‹ 4cmã€‚è¿™æ˜¯æ‰“ç ´æ‚¬åœçš„ç¥å™¨ã€‚
Z_AXIS_BIAS = -0.04 

# =============================================================================
# ğŸ›¡ï¸ å®‰å…¨æ§åˆ¶å™¨
# =============================================================================
class SafetyController:
    def __init__(self, joint_dim=7):
        self.joint_limits_min = np.array([-2.89, -1.76, -2.89, -3.07, -2.89, -0.01, -2.89]) + 0.05
        self.joint_limits_max = np.array([ 2.89,  1.76,  2.89, -0.06,  2.89,  3.75,  2.89]) - 0.05
        self.max_delta = 0.06
        self.last_action = None

    def apply_safety(self, target_qpos, current_qpos):
        target_qpos = np.array(target_qpos)
        current_qpos = np.array(current_qpos)
        
        target_joints = target_qpos[:7]
        current_joints = current_qpos[:7]
        target_gripper = target_qpos[7:] 

        delta = target_joints - current_joints
        delta_clipped = np.clip(delta, -self.max_delta, self.max_delta)
        
        safe_joints = current_joints + delta_clipped
        safe_joints = np.clip(safe_joints, self.joint_limits_min, self.joint_limits_max)

        safe_action_8d = np.concatenate([safe_joints, target_gripper])
        
        self.last_action = safe_action_8d
        return safe_action_8d.tolist()

    def reset(self):
        self.last_action = None


# =============================================================================
# ğŸ¤– å®æ—¶æ¨ç† Agent
# =============================================================================
class RealTimeAgent:
    def __init__(self):
        self.device = DEVICE
        self.safety = SafetyController() 
        self.pred_horizon = 64

        print(f"[Agent] Loading Tokenizer from {TOKENIZER_PATH}...")
        try:
            self.tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
        except:
            self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
        
        if not os.path.exists(STATS_PATH):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°ç»Ÿè®¡æ–‡ä»¶: {STATS_PATH}")
        with open(STATS_PATH, 'r') as f:
            stats = json.load(f)
        self.action_mean = np.array(stats['action_mean'], dtype=np.float32)
        self.action_std = np.array(stats['action_std'], dtype=np.float32)
        self.action_std = np.maximum(self.action_std, 1e-2)
        
        self._init_models()
        self._init_scheduler()
        
        self.window_size = 16
        self.video_buffer = deque(maxlen=self.window_size)
        self.state_buffer = deque(maxlen=self.window_size)
        self.first_frame_tensor = None
        self.text_tokens = None 
        
        self.default_prompt = "pick up the paper cup"
        print(f"[Agent] Prompt: '{self.default_prompt}'")
        print(f"[Agent] ğŸ“‰ Gravity Bias Enabled: {Z_AXIS_BIAS}m per step")

    def _init_models(self):
        print(f"[Agent] Initializing models on {self.device}...")
        try:
            self.encoder = FusionEncoder(backbone_path=VIDEO_MAE_PATH, teacher_dim=1152).to(self.device).eval()
            self.policy = RDTWrapper(action_dim=8, model_path=RDT_PATH, rdt_cond_dim=768).to(self.device).eval()
            
            print(f"[Agent] Loading Encoder: {STAGE_B_PATH}")
            ckpt_b = torch.load(STAGE_B_PATH, map_location=self.device)
            if isinstance(ckpt_b, dict) and 'encoder_state_dict' in ckpt_b:
                self.encoder.load_state_dict(ckpt_b['encoder_state_dict'], strict=False)
            else:
                self.encoder.load_state_dict(ckpt_b, strict=False)

            print(f"[Agent] Loading Policy: {STAGE_C_PATH}")
            peft_config = LoraConfig(
                r=16, lora_alpha=32,
                target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2", "linear"], 
                lora_dropout=0.05, bias="none"
            )
            self.policy.rdt_model = get_peft_model(self.policy.rdt_model, peft_config)
            
            ckpt_c = torch.load(STAGE_C_PATH, map_location=self.device)
            if 'rdt_state_dict' in ckpt_c:
                self.policy.load_state_dict(ckpt_c['rdt_state_dict'], strict=False)
            else:
                self.policy.load_state_dict(ckpt_c, strict=False)
            print("[Agent] âœ… All weights loaded.")

            print("[Agent] Compiling FusionEncoder...")
            torch._dynamo.config.suppress_errors = True
            try:
                self.encoder = torch.compile(self.encoder, mode="default")
            except Exception as e:
                print(f"[Warning] Encoder compile failed: {e}")

        except Exception as e:
            print(f"[Error] Model Init Failed: {e}")
            raise e

    def _init_scheduler(self):
        self.scheduler = DDIMScheduler(
            num_train_timesteps=1000,
            beta_schedule="squaredcos_cap_v2",
            prediction_type="epsilon", 
            clip_sample=True
        )
        self.inference_steps = 10
        self.scheduler.set_timesteps(self.inference_steps)

    def reset_session(self, first_frame_img):
        print("[Agent] Resetting session...")
        self.video_buffer.clear()
        self.state_buffer.clear()
        self.safety.reset() 
        
        # 1. å¤„ç†æ‰‹è…•å›¾åƒ (Wrist Image)
        # è°ƒæ•´å¤§å° -> RGB -> Tensor [3, 224, 224]
        ff_resized = cv2.resize(first_frame_img, (224, 224))
        ff_rgb = cv2.cvtColor(ff_resized, cv2.COLOR_BGR2RGB)
        wrist_tensor = torch.tensor(ff_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
        
        # 2. æ„é€ å…¨é»‘ä¸»æ‘„ (Fake Main Image) - å…³é”®ä¿®æ”¹
        # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†åŒ¹é…è®­ç»ƒæ—¶çš„ Modality Dropout å’Œ step å‡½æ•°çš„è¾“å…¥æ ¼å¼
        main_fake = torch.zeros_like(wrist_tensor)
        
        # 3. å †å å¾—åˆ°åŒæ‘„å¸§ (Stack Views)
        # Shape: [2, 3, 224, 224] (View 0: Fake Main, View 1: Real Wrist)
        dual_frame = torch.stack([main_fake, wrist_tensor], dim=0)
        
        # 4. è®¾ç½® First Frame Tensor (Task Anchor)
        # å¢åŠ  Batch ç»´åº¦ -> [1, 2, 3, 224, 224]
        self.first_frame_tensor = dual_frame.unsqueeze(0).to(self.device)
        
        # 5. ç¼–ç æ–‡æœ¬æŒ‡ä»¤ (Text Instruction)
        tokens = self.tokenizer(
            self.default_prompt, 
            return_tensors="pt", 
            padding="max_length", 
            max_length=16, 
            truncation=True
        ).input_ids
        self.text_tokens = tokens.to(self.device)
        
        # 6. é¢„å¡«å……ç¼“å†²åŒº (Buffer Padding)
        # ä½¿ç”¨ç¬¬ä¸€å¸§é‡å¤å¡«å……ï¼Œé˜²æ­¢ step å‡½æ•°å› ä¸ºæ•°æ®ä¸è¶³è€ŒæŠ¥é”™
        # æ³¨æ„ï¼šè¿™é‡Œå­˜å…¥çš„æ˜¯ dual_frame (Tensor)ï¼Œè€Œä¸æ˜¯åŸå§‹å›¾ç‰‡
        for _ in range(self.window_size):
            self.video_buffer.append(dual_frame) 
            self.state_buffer.append(np.zeros(8)) # å‡è®¾å½’ä¸€åŒ–åçš„"å‡å€¼çŠ¶æ€"ä¸º0


    # def reset_session(self, first_frame_img):
    #     print("[Agent] Resetting session...")
    #     self.video_buffer.clear()
    #     self.state_buffer.clear()
    #     self.safety.reset() 
        
    #     ff = cv2.resize(first_frame_img, (224, 224))
    #     ff = cv2.cvtColor(ff, cv2.COLOR_BGR2RGB)
    #     tensor = torch.tensor(ff, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).unsqueeze(0)
        
    #     self.first_frame_tensor = tensor.to(self.device) / 255.0
        
    #     tokens = self.tokenizer(
    #         self.default_prompt, 
    #         return_tensors="pt", 
    #         padding="max_length", 
    #         max_length=16, 
    #         truncation=True
    #     ).input_ids
    #     self.text_tokens = tokens.to(self.device)
        
    #     for _ in range(self.window_size):
    #         self.video_buffer.append(ff)
    #         self.state_buffer.append(np.zeros(8)) 


    @torch.no_grad()
    def step(self, current_frame, current_qpos):
        """
        è¾“å…¥:
            current_frame: numpy array, [H, W, 3], BGR (cv2 default), æ‰‹è…•ç›¸æœºå›¾åƒ
            current_qpos: list or array, é•¿åº¦ 7 (å…³èŠ‚) æˆ– 8 (å…³èŠ‚+å¤¹çˆª)
        è¾“å‡º:
            actions: list of lists, shape [16, 8], åŒ…å«æœªæ¥16æ­¥çš„åŠ¨ä½œåºåˆ—
        """
        # 1. é¢„å¤„ç†æ‰‹è…•å›¾åƒ (Wrist Image)
        frame_resized = cv2.resize(current_frame, (224, 224))
        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
        # [H, W, 3] -> [3, H, W] -> Normalize
        wrist_tensor = torch.tensor(frame_rgb, dtype=torch.float32).permute(2, 0, 1) / 255.0
        
        # 2. æ„é€ å…¨é»‘çš„ä¸»æ‘„å›¾åƒ (Fake Main Image)
        # è¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼šå®ƒæ¨¡æ‹Ÿäº†è®­ç»ƒæ—¶çš„ Dropout æƒ…å†µï¼Œè®©æ¨¡å‹ä»¥ä¸º"ä¸»æ‘„ä¸¢äº†"ï¼Œä»è€Œè‡ªä¿¡åœ°ä½¿ç”¨æ‰‹è…•ç›¸æœº
        main_fake = torch.zeros_like(wrist_tensor)
        
        # 3. å †å åŒæ‘„ (Stack Views)
        # Result shape: [2, 3, 224, 224] (View 0: Fake Main, View 1: Real Wrist)
        combined_frame = torch.stack([main_fake, wrist_tensor], dim=0)
        
        # 4. å­˜å…¥ç¼“å†²åŒº (Video Buffer)
        self.video_buffer.append(combined_frame)
        
        # 5. å¤„ç†çŠ¶æ€ (Proprioception)
        if len(current_qpos) == 7:
            current_qpos = list(current_qpos) + [0.0] # è¡¥é½å¤¹çˆª
            
        # å½’ä¸€åŒ–å½“å‰çŠ¶æ€
        # æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»ä½¿ç”¨ä¸ Dataset Loader ç›¸åŒçš„ç»Ÿè®¡é‡ (Z-Score)
        raw_qpos_t = torch.tensor(current_qpos, dtype=torch.float32, device=self.device)
        norm_qpos_t = (raw_qpos_t - self.action_mean) / self.action_std
        self.state_buffer.append(norm_qpos_t.cpu().numpy()) # å­˜å…¥ buffer
        
        # 6. å†·å¯åŠ¨å¤„ç† (Cold Start)
        # å¦‚æœå†å²æ•°æ®ä¸è¶³ window_size (16å¸§)ï¼Œåˆ™æ— æ³•æ¨ç†ï¼Œè¿”å›å½“å‰ä½ç½®ä¿æŒä¸åŠ¨
        if len(self.video_buffer) < self.window_size:
            # è¿”å› 16 æ­¥"ä¿æŒå½“å‰ä½ç½®"çš„åŠ¨ä½œ
            return [current_qpos] * self.pred_horizon
        
        # 7. æ„é€ æ¨¡å‹è¾“å…¥ Tensor
        # Video Buffer: list of [2, 3, H, W] (len=16) -> Stack -> [16, 2, 3, H, W]
        # Permute to: [Batch, Views, Channels, Time, Height, Width]
        # -> [1, 2, 3, 16, 224, 224]
        vid_t = torch.stack(list(self.video_buffer)).to(self.device)
        vid_t = vid_t.permute(1, 2, 0, 3, 4).unsqueeze(0)
        
        # State Buffer: [1, 16, 8]
        state_t = torch.tensor(np.array(list(self.state_buffer)), dtype=torch.float32).unsqueeze(0).to(self.device)
        
        # First Frame (Task Anchor)
        # æ³¨æ„ï¼šreset_session æ—¶ä¹Ÿåº”è¯¥æ„é€ ç±»ä¼¼çš„åŒæ‘„ First Frame
        # è¿™é‡Œå‡è®¾ self.first_frame_tensor å·²ç»æ˜¯ [1, 2, 3, H, W] æ ¼å¼
        # å¦‚æœ reset_session è¿˜æ²¡æ”¹ï¼Œè¯·ç¡®ä¿é‚£é‡Œä¹ŸåŠ ä¸Š main_fake é€»è¾‘
        ff_t = self.first_frame_tensor 

        # 8. æ¨¡å‹æ¨ç† (Inference)
        self.scheduler.set_timesteps(self.inference_steps)
        
        with autocast('cuda', dtype=torch.bfloat16):
            # A. ç¼–ç ç‰¹å¾ (VideoMAE + Fusion)
            # Encoder å†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç†åŒæ‘„ç‰¹å¾èåˆ
            features = self.encoder(vid_t, self.text_tokens, state_t, ff_t)
            
            # B. æ‰©æ•£å»å™ª (Diffusion Sampling)
            # Latents åˆå§‹åŒ–: [Batch, Horizon, Action_Dim] -> [1, 16, 8]
            # è¿™å°±æ˜¯ Action Chunking çš„æ ¸å¿ƒï¼šä¸€æ¬¡ç”Ÿæˆ 16 æ­¥
            latents = torch.randn(1, self.pred_horizon, 8, device=self.device) 
            
            for t in self.scheduler.timesteps:
                model_input = self.scheduler.scale_model_input(latents, t)
                t_tensor = torch.tensor([t], device=self.device)
                
                # RDT é¢„æµ‹å™ªå£°
                noise_pred = self.policy(model_input, t_tensor, features)
                
                # ç§»é™¤å™ªå£°
                latents = self.scheduler.step(noise_pred, t, latents).prev_sample
            
        # 9. åå¤„ç†ä¸åå½’ä¸€åŒ–
        # [1, 16, 8] -> [16, 8]
        normalized_actions = latents[0].float()
        
        # åå½’ä¸€åŒ–: (Norm * Std) + Mean
        denormalized_actions = (normalized_actions * self.action_std + self.action_mean).cpu().numpy()
        
        # 10. è¿”å›åŠ¨ä½œåºåˆ— Chunk
        return denormalized_actions.tolist()

    # @torch.no_grad()
    # def step(self, current_frame, current_qpos):
    #     frame_resized = cv2.resize(current_frame, (224, 224))
    #     frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
        
    #     if len(current_qpos) == 7:
    #         current_qpos = list(current_qpos) + [0.0]
        
    #     raw_qpos = np.array(current_qpos, dtype=np.float32)
    #     norm_qpos = (raw_qpos - self.action_mean) / self.action_std
        
    #     self.video_buffer.append(frame_rgb)
    #     self.state_buffer.append(norm_qpos) 
        
    #     if len(self.video_buffer) < self.window_size:
    #         return current_qpos 
        
    #     vid_t = torch.tensor(np.array(list(self.video_buffer)), dtype=torch.float32).permute(0, 3, 1, 2).unsqueeze(0).to(self.device) / 255.0
    #     state_t = torch.tensor(np.array(list(self.state_buffer)), dtype=torch.float32).unsqueeze(0).to(self.device)
        
    #     self.scheduler.set_timesteps(self.inference_steps)
        
    #     with autocast('cuda', dtype=torch.bfloat16):
    #         features = self.encoder(vid_t, self.text_tokens, state_t, self.first_frame_tensor)
    #         latents = torch.randn(1, 1, 8, device=self.device) 
    #         for t in self.scheduler.timesteps:
    #             model_input = self.scheduler.scale_model_input(latents, t)
    #             t_tensor = torch.tensor([t], device=self.device)
    #             noise_pred = self.policy(model_input, t_tensor, features)
    #             latents = self.scheduler.step(noise_pred, t, latents).prev_sample
            
    #     normalized_action = latents[0, 0].float().cpu().numpy()
    #     model_target = normalized_action * self.action_std + self.action_mean
        
    #     # === æ ¸å¿ƒä¿®æ­£é€»è¾‘ ===
        
    #     # 1. æ¢å¤æ­£å¸¸å¢ç›Š (1.0)ï¼Œæ¶ˆé™¤éœ‡è¡
    #     final_target_qpos = raw_qpos + (model_target - raw_qpos) * EXECUTION_GAIN
        
    #     # 2. ã€Gravity Biasã€‘å¦‚æœå¤¹çˆªæ˜¯å¼€çš„ (è¿˜æ²¡æŠ“åˆ°)ï¼Œå¼ºåˆ¶å‘ä¸‹å‹
    #     gripper_is_open = current_qpos[7] > 0.04 
    #     if gripper_is_open:

    #         pass
            
    #     # ä¿®æ­£ï¼šæ—¢ç„¶æ˜¯ Joint Spaceï¼Œæˆ‘ä»¬ç¨å¾®æ”¾å¤§ä¸€ç‚¹ç‚¹ Gainï¼Œä½†ç»ä¸èƒ½ 2.5
    #     final_target_qpos = raw_qpos + (model_target - raw_qpos) * 1.2
        
    #     safe_action = self.safety.apply_safety(target_qpos=final_target_qpos, current_qpos=current_qpos)
    #     return safe_action

# =============================================================================
# æœåŠ¡å™¨ä¸»å¾ªç¯
# =============================================================================
def run_safe_server(host='0.0.0.0', port=6000):
    # è¿™ä¸ªå‡½æ•°ä¸»è¦æ˜¯ç»™ server_gpu_image.py è°ƒç”¨çš„
    # æˆ–è€…å¦‚æœå•ç‹¬è¿è¡Œæ­¤è„šæœ¬è¿›è¡Œæµ‹è¯•ï¼Œå¯ä»¥ç”¨ä¸‹é¢çš„é€»è¾‘
    pass 

if __name__ == '__main__':
    # ä¸ºäº†æ–¹ä¾¿æµ‹è¯•ï¼Œä½ å¯ä»¥åœ¨è¿™é‡ŒæŠŠ server_gpu_image.py çš„é€»è¾‘ç²˜è¿‡æ¥
    # ä½†é€šå¸¸æ˜¯ç›´æ¥è¿è¡Œ python inference/server_gpu_image.py
    pass