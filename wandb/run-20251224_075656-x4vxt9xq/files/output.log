üöÄ WandB logging enabled.
=== Stage C Joint Training ===
Loading Models...
[FusionEncoder] Loading VideoMAE from: /yanghaochuan/models/VideoMAEv2-Large
INFO:transformers_modules.VideoMAEv2_hyphen_Large.modeling_videomaev2:Model config: {'img_size': 224, 'patch_size': 16, 'in_chans': 3, 'num_classes': 0, 'embed_dim': 1024, 'depth': 24, 'num_heads': 16, 'mlp_ratio': 4, 'qkv_bias': True, 'qk_scale': None, 'drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'norm_layer': 'nn.LayerNorm', 'layer_norm_eps': 1e-06, 'init_values': 0.0, 'use_learnable_pos_emb': False, 'tubelet_size': 2, 'use_mean_pooling': False, 'with_cp': False, 'num_frames': 16, 'cos_attn': False}
Some weights of VideoMAEv2 were not initialized from the model checkpoint at /yanghaochuan/models/VideoMAEv2-Large and are newly initialized: ['model.norm.bias', 'model.norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[FusionEncoder] Loading T5 Encoder from: /yanghaochuan/models/flan-t5-large
[FusionEncoder] Aligning heads to teacher dimension: 1152
Loading Stage B: /yanghaochuan/checkpoints/1223stageB_papercup.pt
/yanghaochuan/projects/train/stageC_joint.py:319: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ckpt = torch.load(args.stage_b_ckpt, map_location='cpu')
INFO:[RDTWrapper]:üõ†Ô∏è Init RDT: hidden_size=2048, horizon=65
INFO:[RDTWrapper]:üîç Actual Initialized Dimension: 2048
INFO:[RDTWrapper]:Loading weights from /yanghaochuan/models/rdt-1b/pytorch_model.bin...
/yanghaochuan/projects/model/rdt_model.py:762: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(weights_path, map_location="cpu")
INFO:[RDTWrapper]:Weights loaded. Missing keys: 3
INFO:[RDTWrapper]:‚úÖ x_pos_embed successfully loaded.
Loading RDT weights with auto-slicing...
/yanghaochuan/projects/train/stageC_joint.py:338: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(rdt_file, map_location='cpu')
Applying LoRA...
Loading Dataset from /yanghaochuan/data/1223pick_up_the_paper_cup.hdf5
[Dataset] Loading Tokenizer from /yanghaochuan/models/flan-t5-large...
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[Dataset] Loaded normalization stats.
[Dataset] Loaded 28404 samples from 1223pick_up_the_paper_cup.hdf5
>>> Training Started <<<
Epoch 0 [0/1775] Loss: 1.0352 (Diff: 0.9890 Cons: 0.4617)
m[2K[1mwandb[0m: [38;5;178m‚£æ[0m Encoding video...
üé• Video sample uploaded to WandB.
Epoch 0 [10/1775] Loss: 1.0671 (Diff: 1.0212 Cons: 0.4591)
Epoch 0 [20/1775] Loss: 1.0264 (Diff: 0.9821 Cons: 0.4437)
Epoch 0 [30/1775] Loss: 1.0450 (Diff: 1.0027 Cons: 0.4226)
Epoch 0 [40/1775] Loss: 1.0490 (Diff: 1.0000 Cons: 0.4898)
Epoch 0 [50/1775] Loss: 1.0558 (Diff: 1.0102 Cons: 0.4565)
Epoch 0 [60/1775] Loss: 1.0107 (Diff: 0.9743 Cons: 0.3645)
Epoch 0 [70/1775] Loss: 0.9666 (Diff: 0.9347 Cons: 0.3185)
Epoch 0 [80/1775] Loss: 0.8993 (Diff: 0.8869 Cons: 0.1237)
Epoch 0 [90/1775] Loss: 0.9064 (Diff: 0.8980 Cons: 0.0838)
Epoch 0 [100/1775] Loss: 0.8776 (Diff: 0.8722 Cons: 0.0538)
Epoch 0 [110/1775] Loss: 0.8537 (Diff: 0.8494 Cons: 0.0429)
Epoch 0 [120/1775] Loss: 0.8210 (Diff: 0.8175 Cons: 0.0346)
Epoch 0 [130/1775] Loss: 0.8051 (Diff: 0.8018 Cons: 0.0326)
Epoch 0 [140/1775] Loss: 0.7744 (Diff: 0.7712 Cons: 0.0318)
Epoch 0 [150/1775] Loss: 0.7786 (Diff: 0.7766 Cons: 0.0198)
Epoch 0 [160/1775] Loss: 0.7765 (Diff: 0.7745 Cons: 0.0207)
Epoch 0 [170/1775] Loss: 0.7659 (Diff: 0.7636 Cons: 0.0226)
Epoch 0 [180/1775] Loss: 0.7819 (Diff: 0.7799 Cons: 0.0197)
Epoch 0 [190/1775] Loss: 0.7937 (Diff: 0.7922 Cons: 0.0155)
Epoch 0 [200/1775] Loss: 0.7726 (Diff: 0.7711 Cons: 0.0145)
Epoch 0 [210/1775] Loss: 0.7767 (Diff: 0.7749 Cons: 0.0177)
Epoch 0 [220/1775] Loss: 0.7991 (Diff: 0.7980 Cons: 0.0111)
Epoch 0 [230/1775] Loss: 0.8119 (Diff: 0.8109 Cons: 0.0097)
Epoch 0 [240/1775] Loss: 0.7835 (Diff: 0.7823 Cons: 0.0121)
Epoch 0 [250/1775] Loss: 0.7848 (Diff: 0.7839 Cons: 0.0093)
Epoch 0 [260/1775] Loss: 0.7647 (Diff: 0.7638 Cons: 0.0092)
Epoch 0 [270/1775] Loss: 0.7747 (Diff: 0.7739 Cons: 0.0081)
Epoch 0 [280/1775] Loss: 0.7790 (Diff: 0.7779 Cons: 0.0114)
Epoch 0 [290/1775] Loss: 0.7615 (Diff: 0.7606 Cons: 0.0085)
Epoch 0 [300/1775] Loss: 0.7939 (Diff: 0.7930 Cons: 0.0084)
Epoch 0 [310/1775] Loss: 0.7742 (Diff: 0.7734 Cons: 0.0072)
Epoch 0 [320/1775] Loss: 0.7655 (Diff: 0.7647 Cons: 0.0082)
Epoch 0 [330/1775] Loss: 0.7394 (Diff: 0.7385 Cons: 0.0087)
Epoch 0 [340/1775] Loss: 0.7584 (Diff: 0.7577 Cons: 0.0071)
Epoch 0 [350/1775] Loss: 0.7482 (Diff: 0.7475 Cons: 0.0069)
Epoch 0 [360/1775] Loss: 0.7858 (Diff: 0.7851 Cons: 0.0070)
Epoch 0 [370/1775] Loss: 0.7617 (Diff: 0.7609 Cons: 0.0075)
Epoch 0 [380/1775] Loss: 0.7535 (Diff: 0.7527 Cons: 0.0084)
Epoch 0 [390/1775] Loss: 0.7723 (Diff: 0.7717 Cons: 0.0054)
Epoch 0 [400/1775] Loss: 0.7613 (Diff: 0.7607 Cons: 0.0063)
Epoch 0 [410/1775] Loss: 0.7427 (Diff: 0.7422 Cons: 0.0050)
Epoch 0 [420/1775] Loss: 0.7549 (Diff: 0.7541 Cons: 0.0074)
Epoch 0 [430/1775] Loss: 0.7694 (Diff: 0.7687 Cons: 0.0074)
Epoch 0 [440/1775] Loss: 0.7714 (Diff: 0.7706 Cons: 0.0078)
Epoch 0 [450/1775] Loss: 0.7335 (Diff: 0.7328 Cons: 0.0062)
Epoch 0 [460/1775] Loss: 0.7383 (Diff: 0.7377 Cons: 0.0060)
Epoch 0 [470/1775] Loss: 0.7271 (Diff: 0.7264 Cons: 0.0069)
Epoch 0 [480/1775] Loss: 0.7233 (Diff: 0.7226 Cons: 0.0068)
Epoch 0 [490/1775] Loss: 0.6743 (Diff: 0.6736 Cons: 0.0077)
Epoch 0 [500/1775] Loss: 0.6796 (Diff: 0.6790 Cons: 0.0060)
üé• Video sample uploaded to WandB.
Epoch 0 [510/1775] Loss: 0.6579 (Diff: 0.6571 Cons: 0.0080)
Epoch 0 [520/1775] Loss: 0.6780 (Diff: 0.6773 Cons: 0.0075)
Epoch 0 [530/1775] Loss: 0.6679 (Diff: 0.6673 Cons: 0.0061)
Epoch 0 [540/1775] Loss: 0.6449 (Diff: 0.6442 Cons: 0.0073)
Epoch 0 [550/1775] Loss: 0.6916 (Diff: 0.6908 Cons: 0.0081)
Epoch 0 [560/1775] Loss: 0.6479 (Diff: 0.6474 Cons: 0.0054)
Epoch 0 [570/1775] Loss: 0.6391 (Diff: 0.6385 Cons: 0.0069)
Epoch 0 [580/1775] Loss: 0.6758 (Diff: 0.6752 Cons: 0.0063)
Epoch 0 [590/1775] Loss: 0.6643 (Diff: 0.6637 Cons: 0.0058)
Epoch 0 [600/1775] Loss: 0.6431 (Diff: 0.6425 Cons: 0.0059)
Epoch 0 [610/1775] Loss: 0.6360 (Diff: 0.6354 Cons: 0.0060)
Epoch 0 [620/1775] Loss: 0.6376 (Diff: 0.6369 Cons: 0.0070)
Epoch 0 [630/1775] Loss: 0.6364 (Diff: 0.6356 Cons: 0.0076)
Epoch 0 [640/1775] Loss: 0.6346 (Diff: 0.6338 Cons: 0.0077)
Epoch 0 [650/1775] Loss: 0.6352 (Diff: 0.6345 Cons: 0.0075)
Epoch 0 [660/1775] Loss: 0.5990 (Diff: 0.5982 Cons: 0.0074)
Epoch 0 [670/1775] Loss: 0.5651 (Diff: 0.5643 Cons: 0.0080)
Epoch 0 [680/1775] Loss: 0.6106 (Diff: 0.6097 Cons: 0.0088)
Epoch 0 [690/1775] Loss: 0.5414 (Diff: 0.5407 Cons: 0.0075)
Epoch 0 [700/1775] Loss: 0.5422 (Diff: 0.5415 Cons: 0.0075)
Epoch 0 [710/1775] Loss: 0.5348 (Diff: 0.5340 Cons: 0.0083)
Epoch 0 [720/1775] Loss: 0.5095 (Diff: 0.5087 Cons: 0.0072)
Epoch 0 [730/1775] Loss: 0.5461 (Diff: 0.5454 Cons: 0.0067)
Epoch 0 [740/1775] Loss: 0.5239 (Diff: 0.5231 Cons: 0.0083)
Epoch 0 [750/1775] Loss: 0.4732 (Diff: 0.4725 Cons: 0.0077)
Epoch 0 [760/1775] Loss: 0.4591 (Diff: 0.4583 Cons: 0.0081)
Epoch 0 [770/1775] Loss: 0.4741 (Diff: 0.4733 Cons: 0.0075)
Epoch 0 [780/1775] Loss: 0.4140 (Diff: 0.4132 Cons: 0.0077)
Epoch 0 [790/1775] Loss: 0.5069 (Diff: 0.5063 Cons: 0.0065)
Epoch 0 [800/1775] Loss: 0.4179 (Diff: 0.4171 Cons: 0.0082)
Epoch 0 [810/1775] Loss: 0.4767 (Diff: 0.4760 Cons: 0.0065)
Epoch 0 [820/1775] Loss: 0.4262 (Diff: 0.4255 Cons: 0.0068)
Epoch 0 [830/1775] Loss: 0.4369 (Diff: 0.4362 Cons: 0.0071)
Epoch 0 [840/1775] Loss: 0.4798 (Diff: 0.4789 Cons: 0.0085)
Epoch 0 [850/1775] Loss: 0.4226 (Diff: 0.4219 Cons: 0.0068)
Epoch 0 [860/1775] Loss: 0.4330 (Diff: 0.4324 Cons: 0.0067)
Epoch 0 [870/1775] Loss: 0.4334 (Diff: 0.4327 Cons: 0.0069)
Epoch 0 [880/1775] Loss: 0.4183 (Diff: 0.4175 Cons: 0.0073)
Epoch 0 [890/1775] Loss: 0.4303 (Diff: 0.4297 Cons: 0.0056)
Epoch 0 [900/1775] Loss: 0.4540 (Diff: 0.4534 Cons: 0.0062)
Epoch 0 [910/1775] Loss: 0.4237 (Diff: 0.4230 Cons: 0.0067)
Epoch 0 [920/1775] Loss: 0.4345 (Diff: 0.4338 Cons: 0.0062)
Epoch 0 [930/1775] Loss: 0.4224 (Diff: 0.4218 Cons: 0.0059)
Epoch 0 [940/1775] Loss: 0.4419 (Diff: 0.4413 Cons: 0.0059)
Epoch 0 [950/1775] Loss: 0.4030 (Diff: 0.4024 Cons: 0.0059)
Epoch 0 [960/1775] Loss: 0.3922 (Diff: 0.3916 Cons: 0.0066)
Epoch 0 [970/1775] Loss: 0.3752 (Diff: 0.3746 Cons: 0.0060)
Epoch 0 [980/1775] Loss: 0.3673 (Diff: 0.3667 Cons: 0.0063)
Epoch 0 [990/1775] Loss: 0.3821 (Diff: 0.3815 Cons: 0.0061)
Epoch 0 [1000/1775] Loss: 0.3630 (Diff: 0.3623 Cons: 0.0065)
üé• Video sample uploaded to WandB.
Epoch 0 [1010/1775] Loss: 0.3280 (Diff: 0.3274 Cons: 0.0063)
Epoch 0 [1020/1775] Loss: 0.3251 (Diff: 0.3245 Cons: 0.0060)
Epoch 0 [1030/1775] Loss: 0.3052 (Diff: 0.3046 Cons: 0.0058)
Epoch 0 [1040/1775] Loss: 0.3373 (Diff: 0.3368 Cons: 0.0054)
Epoch 0 [1050/1775] Loss: 0.2985 (Diff: 0.2979 Cons: 0.0062)
Epoch 0 [1060/1775] Loss: 0.3003 (Diff: 0.2997 Cons: 0.0064)
Epoch 0 [1070/1775] Loss: 0.3034 (Diff: 0.3028 Cons: 0.0055)
Epoch 0 [1080/1775] Loss: 0.3216 (Diff: 0.3210 Cons: 0.0059)
Epoch 0 [1090/1775] Loss: 0.3161 (Diff: 0.3156 Cons: 0.0056)
Epoch 0 [1100/1775] Loss: 0.2922 (Diff: 0.2917 Cons: 0.0052)
Epoch 0 [1110/1775] Loss: 0.3271 (Diff: 0.3265 Cons: 0.0054)
Epoch 0 [1120/1775] Loss: 0.2938 (Diff: 0.2932 Cons: 0.0058)
Epoch 0 [1130/1775] Loss: 0.3025 (Diff: 0.3019 Cons: 0.0060)
Epoch 0 [1140/1775] Loss: 0.2854 (Diff: 0.2849 Cons: 0.0056)
Epoch 0 [1150/1775] Loss: 0.3026 (Diff: 0.3020 Cons: 0.0059)
Epoch 0 [1160/1775] Loss: 0.2748 (Diff: 0.2742 Cons: 0.0061)
Epoch 0 [1170/1775] Loss: 0.2743 (Diff: 0.2737 Cons: 0.0056)
Epoch 0 [1180/1775] Loss: 0.2881 (Diff: 0.2876 Cons: 0.0058)
Epoch 0 [1190/1775] Loss: 0.2991 (Diff: 0.2986 Cons: 0.0052)
Epoch 0 [1200/1775] Loss: 0.2537 (Diff: 0.2532 Cons: 0.0055)
Epoch 0 [1210/1775] Loss: 0.2305 (Diff: 0.2299 Cons: 0.0057)
Epoch 0 [1220/1775] Loss: 0.2238 (Diff: 0.2232 Cons: 0.0057)
Epoch 0 [1230/1775] Loss: 0.1810 (Diff: 0.1805 Cons: 0.0056)
Epoch 0 [1240/1775] Loss: 0.2410 (Diff: 0.2405 Cons: 0.0051)
Epoch 0 [1250/1775] Loss: 0.2071 (Diff: 0.2064 Cons: 0.0064)
Epoch 0 [1260/1775] Loss: 0.1965 (Diff: 0.1958 Cons: 0.0065)
Epoch 0 [1270/1775] Loss: 0.1788 (Diff: 0.1783 Cons: 0.0052)
Epoch 0 [1280/1775] Loss: 0.2238 (Diff: 0.2232 Cons: 0.0062)
Epoch 0 [1290/1775] Loss: 0.1635 (Diff: 0.1629 Cons: 0.0054)
Epoch 0 [1300/1775] Loss: 0.1586 (Diff: 0.1581 Cons: 0.0048)
Epoch 0 [1310/1775] Loss: 0.2009 (Diff: 0.2003 Cons: 0.0055)
Epoch 0 [1320/1775] Loss: 0.1648 (Diff: 0.1642 Cons: 0.0053)
Epoch 0 [1330/1775] Loss: 0.1795 (Diff: 0.1790 Cons: 0.0051)
Epoch 0 [1340/1775] Loss: 0.1652 (Diff: 0.1647 Cons: 0.0056)
Epoch 0 [1350/1775] Loss: 0.1659 (Diff: 0.1654 Cons: 0.0056)
Epoch 0 [1360/1775] Loss: 0.1978 (Diff: 0.1972 Cons: 0.0056)
Epoch 0 [1370/1775] Loss: 0.1720 (Diff: 0.1715 Cons: 0.0053)
Epoch 0 [1380/1775] Loss: 0.1737 (Diff: 0.1732 Cons: 0.0052)
Epoch 0 [1390/1775] Loss: 0.1687 (Diff: 0.1682 Cons: 0.0051)
Epoch 0 [1400/1775] Loss: 0.1394 (Diff: 0.1388 Cons: 0.0059)
Epoch 0 [1410/1775] Loss: 0.1740 (Diff: 0.1734 Cons: 0.0056)
Epoch 0 [1420/1775] Loss: 0.1560 (Diff: 0.1555 Cons: 0.0055)
Epoch 0 [1430/1775] Loss: 0.1389 (Diff: 0.1384 Cons: 0.0048)
Epoch 0 [1440/1775] Loss: 0.1019 (Diff: 0.1013 Cons: 0.0060)
Epoch 0 [1450/1775] Loss: 0.1453 (Diff: 0.1448 Cons: 0.0058)
Epoch 0 [1460/1775] Loss: 0.0570 (Diff: 0.0565 Cons: 0.0048)
Epoch 0 [1470/1775] Loss: 0.0654 (Diff: 0.0648 Cons: 0.0051)
Epoch 0 [1480/1775] Loss: 0.0825 (Diff: 0.0819 Cons: 0.0058)
Epoch 0 [1490/1775] Loss: 0.0611 (Diff: 0.0606 Cons: 0.0051)
Epoch 0 [1500/1775] Loss: 0.1165 (Diff: 0.1160 Cons: 0.0057)
üé• Video sample uploaded to WandB.
Epoch 0 [1510/1775] Loss: 0.0764 (Diff: 0.0759 Cons: 0.0053)
Epoch 0 [1520/1775] Loss: 0.0796 (Diff: 0.0790 Cons: 0.0057)
Epoch 0 [1530/1775] Loss: 0.0871 (Diff: 0.0866 Cons: 0.0052)
Epoch 0 [1540/1775] Loss: 0.0684 (Diff: 0.0679 Cons: 0.0051)
Epoch 0 [1550/1775] Loss: 0.0612 (Diff: 0.0607 Cons: 0.0048)
Epoch 0 [1560/1775] Loss: 0.1095 (Diff: 0.1090 Cons: 0.0049)
Epoch 0 [1570/1775] Loss: 0.0625 (Diff: 0.0619 Cons: 0.0053)
Epoch 0 [1580/1775] Loss: 0.0745 (Diff: 0.0740 Cons: 0.0056)
Epoch 0 [1590/1775] Loss: 0.0928 (Diff: 0.0923 Cons: 0.0049)
Epoch 0 [1600/1775] Loss: 0.0610 (Diff: 0.0605 Cons: 0.0047)
Epoch 0 [1610/1775] Loss: 0.0889 (Diff: 0.0885 Cons: 0.0046)
Epoch 0 [1620/1775] Loss: 0.0607 (Diff: 0.0602 Cons: 0.0054)
Epoch 0 [1630/1775] Loss: 0.0678 (Diff: 0.0672 Cons: 0.0055)
Epoch 0 [1640/1775] Loss: 0.0752 (Diff: 0.0747 Cons: 0.0052)
Epoch 0 [1650/1775] Loss: 0.0613 (Diff: 0.0608 Cons: 0.0056)
Epoch 0 [1660/1775] Loss: 0.1130 (Diff: 0.1126 Cons: 0.0049)
Epoch 0 [1670/1775] Loss: 0.0632 (Diff: 0.0627 Cons: 0.0048)
Epoch 0 [1680/1775] Loss: 0.0536 (Diff: 0.0531 Cons: 0.0052)
Epoch 0 [1690/1775] Loss: 0.1148 (Diff: 0.1143 Cons: 0.0050)
Epoch 0 [1700/1775] Loss: 0.0857 (Diff: 0.0853 Cons: 0.0041)
Epoch 0 [1710/1775] Loss: 0.0485 (Diff: 0.0480 Cons: 0.0049)
Epoch 0 [1720/1775] Loss: 0.0725 (Diff: 0.0720 Cons: 0.0054)
Epoch 0 [1730/1775] Loss: 0.0448 (Diff: 0.0443 Cons: 0.0045)
Epoch 0 [1740/1775] Loss: 0.0677 (Diff: 0.0672 Cons: 0.0055)
Epoch 0 [1750/1775] Loss: 0.0782 (Diff: 0.0777 Cons: 0.0044)
Epoch 0 [1760/1775] Loss: 0.0509 (Diff: 0.0504 Cons: 0.0044)
Epoch 0 [1770/1775] Loss: 0.0421 (Diff: 0.0416 Cons: 0.0051)
‚úÖ Saved to /yanghaochuan/checkpoints/epoch_0.pt
Epoch 1 [0/1775] Loss: 0.0845 (Diff: 0.0840 Cons: 0.0050)
Epoch 1 [10/1775] Loss: 0.0923 (Diff: 0.0918 Cons: 0.0048)
Epoch 1 [20/1775] Loss: 0.0461 (Diff: 0.0456 Cons: 0.0052)
Epoch 1 [30/1775] Loss: 0.0625 (Diff: 0.0620 Cons: 0.0053)
Epoch 1 [40/1775] Loss: 0.0673 (Diff: 0.0668 Cons: 0.0047)
Epoch 1 [50/1775] Loss: 0.0516 (Diff: 0.0510 Cons: 0.0054)
Epoch 1 [60/1775] Loss: 0.0608 (Diff: 0.0603 Cons: 0.0049)
Epoch 1 [70/1775] Loss: 0.1533 (Diff: 0.1528 Cons: 0.0048)
Epoch 1 [80/1775] Loss: 0.0379 (Diff: 0.0375 Cons: 0.0044)
Epoch 1 [90/1775] Loss: 0.0449 (Diff: 0.0444 Cons: 0.0047)
Epoch 1 [100/1775] Loss: 0.0432 (Diff: 0.0427 Cons: 0.0050)
Epoch 1 [110/1775] Loss: 0.0554 (Diff: 0.0549 Cons: 0.0049)
Epoch 1 [120/1775] Loss: 0.0544 (Diff: 0.0539 Cons: 0.0051)
Epoch 1 [130/1775] Loss: 0.0959 (Diff: 0.0954 Cons: 0.0048)
Epoch 1 [140/1775] Loss: 0.0420 (Diff: 0.0415 Cons: 0.0046)
Epoch 1 [150/1775] Loss: 0.0666 (Diff: 0.0661 Cons: 0.0051)
Epoch 1 [160/1775] Loss: 0.0652 (Diff: 0.0647 Cons: 0.0046)
Epoch 1 [170/1775] Loss: 0.0571 (Diff: 0.0566 Cons: 0.0053)
Epoch 1 [180/1775] Loss: 0.0686 (Diff: 0.0680 Cons: 0.0056)
Epoch 1 [190/1775] Loss: 0.0472 (Diff: 0.0467 Cons: 0.0049)
Epoch 1 [200/1775] Loss: 0.0512 (Diff: 0.0507 Cons: 0.0050)
Epoch 1 [210/1775] Loss: 0.0601 (Diff: 0.0596 Cons: 0.0049)
Epoch 1 [220/1775] Loss: 0.0453 (Diff: 0.0448 Cons: 0.0043)
üé• Video sample uploaded to WandB.
Epoch 1 [230/1775] Loss: 0.0785 (Diff: 0.0780 Cons: 0.0049)
Epoch 1 [240/1775] Loss: 0.0680 (Diff: 0.0675 Cons: 0.0052)
Epoch 1 [250/1775] Loss: 0.0754 (Diff: 0.0749 Cons: 0.0045)
Epoch 1 [260/1775] Loss: 0.0705 (Diff: 0.0700 Cons: 0.0050)
Epoch 1 [270/1775] Loss: 0.0438 (Diff: 0.0433 Cons: 0.0049)
Epoch 1 [280/1775] Loss: 0.0571 (Diff: 0.0567 Cons: 0.0045)
Epoch 1 [290/1775] Loss: 0.0676 (Diff: 0.0672 Cons: 0.0042)
Epoch 1 [300/1775] Loss: 0.0469 (Diff: 0.0464 Cons: 0.0046)
Epoch 1 [310/1775] Loss: 0.0564 (Diff: 0.0559 Cons: 0.0051)
Epoch 1 [320/1775] Loss: 0.0544 (Diff: 0.0539 Cons: 0.0048)
Epoch 1 [330/1775] Loss: 0.0804 (Diff: 0.0800 Cons: 0.0043)
Epoch 1 [340/1775] Loss: 0.0722 (Diff: 0.0717 Cons: 0.0055)
Epoch 1 [350/1775] Loss: 0.0657 (Diff: 0.0652 Cons: 0.0056)
Epoch 1 [360/1775] Loss: 0.0594 (Diff: 0.0589 Cons: 0.0043)
Epoch 1 [370/1775] Loss: 0.0461 (Diff: 0.0457 Cons: 0.0044)
Epoch 1 [380/1775] Loss: 0.1064 (Diff: 0.1060 Cons: 0.0045)
Epoch 1 [390/1775] Loss: 0.0484 (Diff: 0.0480 Cons: 0.0040)
Epoch 1 [400/1775] Loss: 0.0616 (Diff: 0.0611 Cons: 0.0053)
Epoch 1 [410/1775] Loss: 0.0476 (Diff: 0.0470 Cons: 0.0054)
Epoch 1 [420/1775] Loss: 0.0544 (Diff: 0.0539 Cons: 0.0051)
Epoch 1 [430/1775] Loss: 0.0777 (Diff: 0.0773 Cons: 0.0043)
Epoch 1 [440/1775] Loss: 0.0572 (Diff: 0.0567 Cons: 0.0047)
Epoch 1 [450/1775] Loss: 0.0375 (Diff: 0.0370 Cons: 0.0049)
Epoch 1 [460/1775] Loss: 0.0724 (Diff: 0.0719 Cons: 0.0048)
Epoch 1 [470/1775] Loss: 0.0452 (Diff: 0.0447 Cons: 0.0050)
Epoch 1 [480/1775] Loss: 0.0445 (Diff: 0.0441 Cons: 0.0036)
Epoch 1 [490/1775] Loss: 0.0339 (Diff: 0.0334 Cons: 0.0050)
Epoch 1 [500/1775] Loss: 0.0372 (Diff: 0.0366 Cons: 0.0052)
Epoch 1 [510/1775] Loss: 0.1026 (Diff: 0.1021 Cons: 0.0051)
Epoch 1 [520/1775] Loss: 0.0600 (Diff: 0.0596 Cons: 0.0048)
Epoch 1 [530/1775] Loss: 0.0485 (Diff: 0.0481 Cons: 0.0041)
Epoch 1 [540/1775] Loss: 0.0995 (Diff: 0.0990 Cons: 0.0046)
Epoch 1 [550/1775] Loss: 0.0438 (Diff: 0.0433 Cons: 0.0047)
Epoch 1 [560/1775] Loss: 0.0572 (Diff: 0.0568 Cons: 0.0044)
Epoch 1 [570/1775] Loss: 0.1068 (Diff: 0.1064 Cons: 0.0047)
Epoch 1 [580/1775] Loss: 0.0759 (Diff: 0.0755 Cons: 0.0049)
Epoch 1 [590/1775] Loss: 0.0407 (Diff: 0.0403 Cons: 0.0036)
Epoch 1 [600/1775] Loss: 0.0584 (Diff: 0.0579 Cons: 0.0050)
Epoch 1 [610/1775] Loss: 0.0970 (Diff: 0.0966 Cons: 0.0049)
Epoch 1 [620/1775] Loss: 0.0805 (Diff: 0.0801 Cons: 0.0041)
Epoch 1 [630/1775] Loss: 0.0350 (Diff: 0.0344 Cons: 0.0055)
Epoch 1 [640/1775] Loss: 0.0461 (Diff: 0.0456 Cons: 0.0053)
Epoch 1 [650/1775] Loss: 0.0418 (Diff: 0.0412 Cons: 0.0057)
Epoch 1 [660/1775] Loss: 0.0501 (Diff: 0.0497 Cons: 0.0040)
Epoch 1 [670/1775] Loss: 0.0534 (Diff: 0.0529 Cons: 0.0052)
Epoch 1 [680/1775] Loss: 0.0863 (Diff: 0.0858 Cons: 0.0051)
Epoch 1 [690/1775] Loss: 0.0353 (Diff: 0.0349 Cons: 0.0046)
Epoch 1 [700/1775] Loss: 0.0414 (Diff: 0.0409 Cons: 0.0051)
Epoch 1 [710/1775] Loss: 0.1058 (Diff: 0.1053 Cons: 0.0054)
Epoch 1 [720/1775] Loss: 0.0471 (Diff: 0.0465 Cons: 0.0055)
üé• Video sample uploaded to WandB.
Epoch 1 [730/1775] Loss: 0.0890 (Diff: 0.0885 Cons: 0.0054)
Epoch 1 [740/1775] Loss: 0.0496 (Diff: 0.0491 Cons: 0.0051)
Epoch 1 [750/1775] Loss: 0.0507 (Diff: 0.0501 Cons: 0.0056)
Epoch 1 [760/1775] Loss: 0.0473 (Diff: 0.0469 Cons: 0.0040)
Epoch 1 [770/1775] Loss: 0.0556 (Diff: 0.0551 Cons: 0.0048)
Epoch 1 [780/1775] Loss: 0.0524 (Diff: 0.0520 Cons: 0.0048)
Epoch 1 [790/1775] Loss: 0.0605 (Diff: 0.0599 Cons: 0.0051)
Epoch 1 [800/1775] Loss: 0.0516 (Diff: 0.0510 Cons: 0.0053)
Epoch 1 [810/1775] Loss: 0.0871 (Diff: 0.0866 Cons: 0.0056)
Epoch 1 [820/1775] Loss: 0.0453 (Diff: 0.0448 Cons: 0.0046)
Epoch 1 [830/1775] Loss: 0.0765 (Diff: 0.0759 Cons: 0.0054)
Epoch 1 [840/1775] Loss: 0.0312 (Diff: 0.0308 Cons: 0.0048)
Epoch 1 [850/1775] Loss: 0.0403 (Diff: 0.0399 Cons: 0.0046)
Epoch 1 [860/1775] Loss: 0.0360 (Diff: 0.0355 Cons: 0.0046)
Epoch 1 [870/1775] Loss: 0.1133 (Diff: 0.1128 Cons: 0.0049)
Epoch 1 [880/1775] Loss: 0.0359 (Diff: 0.0354 Cons: 0.0050)
Epoch 1 [890/1775] Loss: 0.0898 (Diff: 0.0894 Cons: 0.0044)
Epoch 1 [900/1775] Loss: 0.0474 (Diff: 0.0468 Cons: 0.0060)
Epoch 1 [910/1775] Loss: 0.0511 (Diff: 0.0506 Cons: 0.0051)
Epoch 1 [920/1775] Loss: 0.0653 (Diff: 0.0648 Cons: 0.0050)
Epoch 1 [930/1775] Loss: 0.0552 (Diff: 0.0547 Cons: 0.0046)
Epoch 1 [940/1775] Loss: 0.0432 (Diff: 0.0427 Cons: 0.0048)
Epoch 1 [950/1775] Loss: 0.0400 (Diff: 0.0395 Cons: 0.0051)
Epoch 1 [960/1775] Loss: 0.0879 (Diff: 0.0874 Cons: 0.0050)
Epoch 1 [970/1775] Loss: 0.0823 (Diff: 0.0818 Cons: 0.0052)
Epoch 1 [980/1775] Loss: 0.0696 (Diff: 0.0690 Cons: 0.0058)
Epoch 1 [990/1775] Loss: 0.0765 (Diff: 0.0761 Cons: 0.0042)
Epoch 1 [1000/1775] Loss: 0.0365 (Diff: 0.0360 Cons: 0.0051)
Epoch 1 [1010/1775] Loss: 0.0362 (Diff: 0.0357 Cons: 0.0054)
Epoch 1 [1020/1775] Loss: 0.0420 (Diff: 0.0415 Cons: 0.0051)
Epoch 1 [1030/1775] Loss: 0.0912 (Diff: 0.0907 Cons: 0.0050)
Epoch 1 [1040/1775] Loss: 0.0459 (Diff: 0.0454 Cons: 0.0058)
Epoch 1 [1050/1775] Loss: 0.0546 (Diff: 0.0541 Cons: 0.0048)
Epoch 1 [1060/1775] Loss: 0.0463 (Diff: 0.0457 Cons: 0.0057)
Epoch 1 [1070/1775] Loss: 0.0462 (Diff: 0.0456 Cons: 0.0055)
Epoch 1 [1080/1775] Loss: 0.0673 (Diff: 0.0667 Cons: 0.0062)
Epoch 1 [1090/1775] Loss: 0.0531 (Diff: 0.0524 Cons: 0.0066)
Epoch 1 [1100/1775] Loss: 0.0503 (Diff: 0.0498 Cons: 0.0055)
Epoch 1 [1110/1775] Loss: 0.1183 (Diff: 0.1178 Cons: 0.0053)
Epoch 1 [1120/1775] Loss: 0.0501 (Diff: 0.0496 Cons: 0.0049)
Epoch 1 [1130/1775] Loss: 0.0432 (Diff: 0.0427 Cons: 0.0045)
Epoch 1 [1140/1775] Loss: 0.0526 (Diff: 0.0520 Cons: 0.0058)
Epoch 1 [1150/1775] Loss: 0.0304 (Diff: 0.0299 Cons: 0.0047)
Epoch 1 [1160/1775] Loss: 0.0541 (Diff: 0.0536 Cons: 0.0050)
Epoch 1 [1170/1775] Loss: 0.0675 (Diff: 0.0669 Cons: 0.0065)
Epoch 1 [1180/1775] Loss: 0.0347 (Diff: 0.0342 Cons: 0.0047)
Epoch 1 [1190/1775] Loss: 0.0727 (Diff: 0.0723 Cons: 0.0048)
Epoch 1 [1200/1775] Loss: 0.0387 (Diff: 0.0381 Cons: 0.0056)
Epoch 1 [1210/1775] Loss: 0.0578 (Diff: 0.0573 Cons: 0.0048)
Epoch 1 [1220/1775] Loss: 0.0951 (Diff: 0.0946 Cons: 0.0043)
üé• Video sample uploaded to WandB.
Epoch 1 [1230/1775] Loss: 0.0377 (Diff: 0.0370 Cons: 0.0072)
Epoch 1 [1240/1775] Loss: 0.0856 (Diff: 0.0850 Cons: 0.0059)
Epoch 1 [1250/1775] Loss: 0.0473 (Diff: 0.0468 Cons: 0.0058)
Epoch 1 [1260/1775] Loss: 0.0598 (Diff: 0.0593 Cons: 0.0057)
Epoch 1 [1270/1775] Loss: 0.0375 (Diff: 0.0369 Cons: 0.0064)
Epoch 1 [1280/1775] Loss: 0.0880 (Diff: 0.0874 Cons: 0.0052)
Epoch 1 [1290/1775] Loss: 0.0348 (Diff: 0.0344 Cons: 0.0049)
Epoch 1 [1300/1775] Loss: 0.0563 (Diff: 0.0558 Cons: 0.0052)
Epoch 1 [1310/1775] Loss: 0.0911 (Diff: 0.0907 Cons: 0.0041)
Epoch 1 [1320/1775] Loss: 0.0475 (Diff: 0.0470 Cons: 0.0055)
Epoch 1 [1330/1775] Loss: 0.0543 (Diff: 0.0537 Cons: 0.0057)
Epoch 1 [1340/1775] Loss: 0.0498 (Diff: 0.0493 Cons: 0.0042)
Epoch 1 [1350/1775] Loss: 0.0360 (Diff: 0.0354 Cons: 0.0062)
Epoch 1 [1360/1775] Loss: 0.0511 (Diff: 0.0506 Cons: 0.0053)
Epoch 1 [1370/1775] Loss: 0.0338 (Diff: 0.0333 Cons: 0.0048)
Epoch 1 [1380/1775] Loss: 0.0406 (Diff: 0.0401 Cons: 0.0049)
Epoch 1 [1390/1775] Loss: 0.0326 (Diff: 0.0320 Cons: 0.0059)
Epoch 1 [1400/1775] Loss: 0.0389 (Diff: 0.0384 Cons: 0.0053)
Epoch 1 [1410/1775] Loss: 0.0503 (Diff: 0.0497 Cons: 0.0061)
Epoch 1 [1420/1775] Loss: 0.0510 (Diff: 0.0506 Cons: 0.0047)
Epoch 1 [1430/1775] Loss: 0.0373 (Diff: 0.0368 Cons: 0.0049)
Epoch 1 [1440/1775] Loss: 0.0414 (Diff: 0.0408 Cons: 0.0059)
  File "/yanghaochuan/projects/train/stageC_joint.py", line 521, in <module>
    train_stage_c(args)
  File "/yanghaochuan/projects/train/stageC_joint.py", line 449, in train_stage_c
    loss_cons = compute_consistency_loss(fusion_encoder, batch, device)
  File "/yanghaochuan/projects/losses/consistency_loss.py", line 49, in compute_consistency_loss
    out_dual = fusion_encoder(video_dual, text, state, ff)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/yanghaochuan/projects/model/fusion_encoder.py", line 232, in forward
    text_outputs = self.text_encoder(input_ids=text_tokens)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1932, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 1100, in forward
    layer_outputs = layer_module(
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 687, in forward
    self_attention_outputs = self.layer[0](
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 602, in forward
    normed_hidden_states = self.layer_norm(hidden_states)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/ego/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 256, in forward
    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
KeyboardInterrupt
