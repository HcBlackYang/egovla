# import torch
# import torch.nn.functional as F

# def compute_consistency_loss(fusion_encoder, batch, device):
#     """
#     è®¡ç®—å•åŒæ‘„ç‰¹å¾ä¸€è‡´æ€§æŸå¤± (Invariance/Consistency Loss)ã€‚
    
#     åŸç†:
#         1. Input A (Dual): å®Œæ•´çš„ [Main, Wrist] è§†é¢‘æµã€‚
#         2. Input B (Single): å°† Main è§†è§’å…¨é»‘ masked æ‰çš„ [Black, Wrist] è§†é¢‘æµã€‚
#         3. ç›®æ ‡: è®© Input B çš„ç‰¹å¾ (Student) é€¼è¿‘ Input A çš„ç‰¹å¾ (Teacher/Target)ã€‚
        
#     Args:
#         fusion_encoder: æ‚¨çš„ç‰¹å¾æå–å™¨æ¨¡å‹ (FusionEncoder)
#         batch: DataLoader å‡ºæ¥çš„ä¸€ä¸ª batch å­—å…¸
#         device: torch device
        
#     Returns:
#         loss: æ ‡é‡ Tensor (MSE)
#     """
    
#     # 1. è·å–åŸå§‹æ•°æ® (Expect [B, 2, 3, 16, 224, 224])
#     # æ³¨æ„ï¼šä¸ºäº†ä½¿ç”¨æ­¤ Lossï¼Œæ‚¨çš„ Dataset å¿…é¡»è¿”å›åŒæ‘„æ•°æ®ã€‚
#     # å¦‚æœ Dataset è¿”å›çš„æ˜¯ [B, 3, 16, 224, 224] (å•æ‘„)ï¼Œåˆ™éœ€è¦å…ˆä¿®æ”¹ Dataset Loaderã€‚
#     video = batch['video'].to(device, non_blocking=True)
#     text = batch['text_tokens'].to(device, non_blocking=True)
#     state = batch['state'].to(device, non_blocking=True)
#     ff = batch['first_frame'].to(device, non_blocking=True) 

#     # æ£€æŸ¥ç»´åº¦: å¿…é¡»åŒ…å« View ç»´åº¦ (é€šå¸¸æ˜¯ç¬¬1ç»´)
#     # [Batch, View, Channel, Time, Height, Width]
#     if video.dim() == 6 and video.shape[1] == 2:
#         # === A. æ„é€ åŒæ‘„è¾“å…¥ (Dual View - Teacher) ===
#         # è€å¸ˆçœ‹åˆ°æ‰€æœ‰ä¿¡æ¯
#         video_dual = video 
        
#         # === B. æ„é€ å•æ‘„è¾“å…¥ (Single View - Student) ===
#         # å­¦ç”Ÿçš„ä¸»æ‘„è¢«é®æŒ¡ (æ¨¡æ‹Ÿæ¨ç†æƒ…å†µ)
#         video_single = video.clone()
#         video_single[:, 0] = 0.0 # å°† View 0 (Main Camera) è®¾ä¸ºå…¨é»‘
        
#         # === C. å‰å‘ä¼ æ’­ ===
        
#         # 1. æå– Teacher ç‰¹å¾ (ä¸éœ€è¦æ¢¯åº¦)
#         with torch.no_grad():
#             # æ³¨æ„ï¼šfusion_encoder å†…éƒ¨éœ€è¦å¤„ç† [B, 2, ...] çš„è¾“å…¥
#             # å¦‚æœ encoder åªæ¥å— [B, C, ...], å®ƒå†…éƒ¨åº”è¯¥æœ‰ flatten B*V çš„é€»è¾‘
#             # æˆ–è€…æˆ‘ä»¬åœ¨å¤–é¢ flatten ä¹Ÿå¯ä»¥ï¼Œä½†é€šå¸¸å»ºè®®æ”¾åœ¨ Encoder forward é‡Œç»Ÿä¸€å¤„ç†
#             out_dual = fusion_encoder(video_dual, text, state, ff)
#             feat_dual = out_dual['e_t'].detach() # [B, 64, 768] (Stop Gradient)
            
#         # 2. æå– Student ç‰¹å¾ (éœ€è¦æ¢¯åº¦)
#         out_single = fusion_encoder(video_single, text, state, ff)
#         feat_single = out_single['e_t'] # [B, 64, 768]
        
#         # === D. è®¡ç®—æŸå¤± ===
#         # å¼ºåˆ¶ Student æ¨¡ä»¿ Teacher
#         loss = F.mse_loss(feat_single, feat_dual)
        
#         return loss

#     else:
#         # å¦‚æœæ•°æ®ä¸æ˜¯åŒæ‘„æ ¼å¼ï¼Œæ— æ³•è®¡ç®—æ­¤ Loss
#         # è¿™ç§æƒ…å†µä¸‹è¿”å› 0ï¼Œé¿å…æŠ¥é”™ï¼Œä½†è¯·åŠ¡å¿…æ£€æŸ¥ Dataset
#         # print("[Warning] Consistency Loss skipped: Input is not dual-view.")
#         return torch.tensor(0.0, device=device, requires_grad=True)

import torch
import torch.nn.functional as F

def compute_consistency_loss(fusion_encoder, batch, device):
    """
    è®¡ç®—å•åŒæ‘„ç‰¹å¾ä¸€è‡´æ€§æŸå¤± (Invariance/Consistency Loss)ã€‚
    
    åŸç†:
        1. Input A (Dual): å®Œæ•´çš„ [Main, Wrist] è§†é¢‘æµ + å®Œæ•´çš„é¦–å¸§ã€‚
        2. Input B (Single): [Black, Wrist] è§†é¢‘æµ + [Black, Wrist] é¦–å¸§ã€‚
        3. ç›®æ ‡: è®© Input B çš„ç‰¹å¾ (Student) é€¼è¿‘ Input A çš„ç‰¹å¾ (Teacher/Target)ã€‚
    """
    
    # 1. è·å–åŸå§‹æ•°æ® (Expect [B, 2, 3, 16, 224, 224])
    video = batch['video'].to(device, non_blocking=True)
    text = batch['text_tokens'].to(device, non_blocking=True)
    state = batch['state'].to(device, non_blocking=True)
    ff = batch['first_frame'].to(device, non_blocking=True) 

    # æ£€æŸ¥ç»´åº¦: å¿…é¡»åŒ…å« View ç»´åº¦
    if video.dim() == 6 and video.shape[1] == 2:
        # === A. æ„é€ åŒæ‘„è¾“å…¥ (Dual View - Teacher) ===
        video_dual = video 
        ff_dual = ff
        
        # === B. æ„é€ å•æ‘„è¾“å…¥ (Single View - Student) ===
        # å­¦ç”Ÿçš„ä¸»æ‘„è¢«é®æŒ¡ (æ¨¡æ‹Ÿæ¨ç†æƒ…å†µ)
        video_single = video.clone()
        video_single[:, 0] = 0.0 # å°† View 0 (Main Camera) è§†é¢‘æµè®¾ä¸ºå…¨é»‘
        
        # ğŸš¨ [å…³é”®ä¿®æ”¹] åŒæ­¥ Mask é¦–å¸§ï¼
        # å¦‚æœä¸Maskè¿™é‡Œï¼Œå­¦ç”Ÿä¼šâ€œä½œå¼Šâ€ï¼Œé€šè¿‡çœ‹é«˜æ¸…é¦–å¸§æ¥æ¨æµ‹å…¨å±€ä¿¡æ¯ï¼Œ
        # å¯¼è‡´å®ƒå­¦ä¸ä¼šä»æ‰‹è…•è§†é¢‘æµä¸­æ¨æ–­ä½ç½®ã€‚
        ff_single = ff.clone()
        ff_single[:, 0] = 0.0 # å°† View 0 (Main Camera) é¦–å¸§è®¾ä¸ºå…¨é»‘
        
        # === C. å‰å‘ä¼ æ’­ ===
        
        # 1. æå– Teacher ç‰¹å¾ (ä¸éœ€è¦æ¢¯åº¦)
        with torch.no_grad():
            out_dual = fusion_encoder(video_dual, text, state, ff_dual)
            feat_dual = out_dual['e_t'].detach() # [B, 64, 768] (Stop Gradient)
            
        # 2. æå– Student ç‰¹å¾ (éœ€è¦æ¢¯åº¦)
        # ğŸš¨ è¿™é‡Œå¿…é¡»ä¼ å…¥ masked åçš„ ff_single
        out_single = fusion_encoder(video_single, text, state, ff_single)
        feat_single = out_single['e_t'] # [B, 64, 768]
        
        # === D. è®¡ç®—æŸå¤± ===
        # å¼ºåˆ¶ Student æ¨¡ä»¿ Teacher
        loss = F.mse_loss(feat_single, feat_dual)
        
        return loss

    else:
        return torch.tensor(0.0, device=device, requires_grad=True)